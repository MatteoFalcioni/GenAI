{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Hugging Face Projects**\n",
        "\n",
        "This notebook will be dedicated to using Hugging Face in order to code some interesting projects:\n",
        "\n",
        "1. Text Summarization (**seq2seq**)\n",
        "2. Text To Image Generation (**diffusion**)\n",
        "3. 3\n",
        "\n",
        "All of these will be done through fine tuning of existing baseline models.\n",
        "\n",
        "We will need a GPU in order to fine tune the models:"
      ],
      "metadata": {
        "id": "f795aa1f"
      },
      "id": "f795aa1f"
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "r86EUqWXxlKD"
      },
      "id": "r86EUqWXxlKD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Text Summarization Project (Seq2Seq)\n",
        "\n",
        "For this task we are going to use a class of models called *Seq2Seq*.\n",
        "\n",
        "Seq2Seq models map an input sequence to an output sequence — useful for tasks like translation, summarization, dialogue.\n",
        "Transformer-based Seq2Seq models (like T5 and BART) replaced older RNN-based ones, achieving much better performance."
      ],
      "metadata": {
        "id": "1XJev9W-xYYG"
      },
      "id": "1XJev9W-xYYG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Install Dependencies\n",
        "\n",
        "We need some packages in order to start with our project:"
      ],
      "metadata": {
        "id": "o8r4T04s3hWq"
      },
      "id": "o8r4T04s3hWq"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q\n",
        "!pip install --upgrade datasets -q"
      ],
      "metadata": {
        "id": "aozXk6pOyMa2"
      },
      "id": "aozXk6pOyMa2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# disinstall and re-install accelerate for gpu acceleration\n",
        "\n",
        "!pip install --upgrade accelerate\n",
        "!pip uninstall -y transformers accelerate  # sometimes colab uses older versions\n",
        "!pip install transformers accelerate  # now we're sure we're using a new version"
      ],
      "metadata": {
        "id": "BgHvynfyyTi2"
      },
      "id": "BgHvynfyyTi2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import to test that everything is fine\n",
        "\n",
        "from transformers import pipeline, set_seed\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer # For the model we're going to use\n",
        "from datasets import load_dataset, load_from_disk # For the datasets\n",
        "\n",
        "# python libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# tokenization\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "from tqdm import tqdm # just progress bar\n",
        "\n",
        "import torch\n",
        "\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "id": "eFZJS_VVzAIE"
      },
      "id": "eFZJS_VVzAIE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's check the device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "rJ8A09y8zjpl"
      },
      "id": "rJ8A09y8zjpl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose our model \"checkpoint\" (ckpt)\n",
        "model_ckpt = \"google/pegasus-cnn_dailymail\" # https://huggingface.co/google/pegasus-cnn_dailymail\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
      ],
      "metadata": {
        "id": "YJPEBLE1zzHi"
      },
      "id": "YJPEBLE1zzHi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model and send it to device\n",
        "model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
      ],
      "metadata": {
        "id": "71JSzWHm0SQM"
      },
      "id": "71JSzWHm0SQM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Get the Data"
      ],
      "metadata": {
        "id": "_JxFFwSF3mvd"
      },
      "id": "_JxFFwSF3mvd"
    },
    {
      "cell_type": "code",
      "source": [
        "# sometimes i have problems loading if i dont update datasets first...\n",
        "!pip install --upgrade datasets fsspec"
      ],
      "metadata": {
        "id": "MsgvObtd2pyV"
      },
      "id": "MsgvObtd2pyV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dataset\n",
        "dataset_samsum = load_dataset(\"knkarthick/samsum\") # https://huggingface.co/datasets/knkarthick/samsum"
      ],
      "metadata": {
        "id": "US5a9mwS036f"
      },
      "id": "US5a9mwS036f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_samsum  # it's composed of dialogue and summary couples"
      ],
      "metadata": {
        "id": "VwCU4bna1OVY"
      },
      "id": "VwCU4bna1OVY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_samsum[\"train\"][\"dialogue\"][1]"
      ],
      "metadata": {
        "id": "RHmyqOVc28U2"
      },
      "id": "RHmyqOVc28U2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_samsum[\"train\"][\"summary\"][1]"
      ],
      "metadata": {
        "id": "avBx7p-r3FQm"
      },
      "id": "avBx7p-r3FQm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samsum_train_df = pd.DataFrame(dataset_samsum['train'])\n",
        "samsum_test_df = pd.DataFrame(dataset_samsum['test'])"
      ],
      "metadata": {
        "id": "S99EUIm-Ay6Y"
      },
      "id": "S99EUIm-Ay6Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.1: Always inspect Your Data Thoroughly..."
      ],
      "metadata": {
        "id": "jV4GviYsFAoJ"
      },
      "id": "jV4GviYsFAoJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# I was getting an error when mapping my dataset, went back and checked the data for NaN values...\n",
        "\n",
        "print(samsum_train_df.isnull().sum())\n",
        "print(samsum_test_df.isnull().sum())"
      ],
      "metadata": {
        "id": "rax41cuvA7qh"
      },
      "id": "rax41cuvA7qh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samsum_train_df[samsum_train_df.isnull().any(axis=1)] # bad data here"
      ],
      "metadata": {
        "id": "BMiGTLeJBRt2"
      },
      "id": "BMiGTLeJBRt2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter the dataset to remove it\n",
        "# Define a filter function\n",
        "def clean_example(example):\n",
        "    return (example['dialogue'] is not None and\n",
        "            example['summary'] is not None)\n",
        "\n",
        "# Apply the filter to each split\n",
        "dataset_samsum_clean = dataset_samsum.map(lambda x: x, remove_columns=[])  # make a copy\n",
        "\n",
        "# Clean\n",
        "dataset_samsum_clean['train'] = dataset_samsum['train'].filter(clean_example)\n",
        "dataset_samsum_clean['validation'] = dataset_samsum['validation'].filter(clean_example)\n",
        "dataset_samsum_clean['test'] = dataset_samsum['test'].filter(clean_example)"
      ],
      "metadata": {
        "id": "4oKsuFwLBaSH"
      },
      "id": "4oKsuFwLBaSH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note:** Hugging Face DatasetDict objects are immutable by default.\n",
        ">\n",
        "> When you apply `.filter()`, it returns a new object — it doesn't modify the original\n",
        "dataset in-place.\n",
        ">\n",
        ">If you want to keep your original `dataset_samsum` untouched, you can make a copy before applying filters.\n",
        ">```python\n",
        "dataset_samsum_clean = dataset_samsum.map(lambda x: x, remove_columns=[])\n",
        "```\n",
        ">This trick is used to make a shallow copy of the dataset before you start modifying (filtering) it, to avoid messing up the original.\n",
        ">\n",
        "> In this case we didn't really need to keep the original with NaN values, but just for safety I made a copy first."
      ],
      "metadata": {
        "id": "E3w_cPRkD2aM"
      },
      "id": "E3w_cPRkD2aM"
    },
    {
      "cell_type": "code",
      "source": [
        "samsum_train_df = pd.DataFrame(dataset_samsum_clean['train'])\n",
        "print(samsum_train_df.isnull().sum())\n",
        "print(samsum_test_df.isnull().sum())"
      ],
      "metadata": {
        "id": "BACpPXQwDiEw"
      },
      "id": "BACpPXQwDiEw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Preprocess data (embedding)"
      ],
      "metadata": {
        "id": "sdqndh5j3HU5"
      },
      "id": "sdqndh5j3HU5"
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_examples_to_features(example_batch):\n",
        "  \"\"\"\n",
        "  Encodes the dataset in batches\n",
        "  \"\"\"\n",
        "\n",
        "  input_encodings = tokenizer(example_batch['dialogue'],\n",
        "                              padding='max_length',\n",
        "                              max_length=1024,\n",
        "                              truncation=True)\n",
        "\n",
        "  with tokenizer.as_target_tokenizer(): # target tokenizer context manager (see below)\n",
        "    target_encodings = tokenizer(example_batch['summary'],\n",
        "                                 padding='max_length',\n",
        "                                 max_length=128,\n",
        "                                 truncation=True)\n",
        "\n",
        "  return {  # tutti i tokenizer ritornano input_ids attention_mask etc.? o Hanno strutture diverse\n",
        "            'input_ids' : input_encodings['input_ids'],\n",
        "            'attention_mask' : input_encodings['attention_mask'],\n",
        "            'labels' : target_encodings['input_ids']\n",
        "  }"
      ],
      "metadata": {
        "id": "NCaaHHJv3uqP"
      },
      "id": "NCaaHHJv3uqP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note:**\n",
        ">\n",
        "> In sequence-to-sequence (seq2seq) models like Pegasus, it is essential to differentiate between input tokens and target tokens during tokenization. Although the tokenizer might appear the same for both, using `tokenizer.as_target_tokenizer()` ensures that tokenization parameters and settings are properly adjusted for the target side (decoder). This is crucial because the model processes the source text through the encoder and generates the target text through the decoder. Properly tokenizing targets guarantees that the model receives the correct input format for loss computation and sequence generation. Without this distinction, the model could misinterpret the labels, leading to incorrect training and poor performance.\n"
      ],
      "metadata": {
        "id": "up0Kdnju9OmY"
      },
      "id": "up0Kdnju9OmY"
    },
    {
      "cell_type": "code",
      "source": [
        "# apply tokenization with map\n",
        "dataset_samsum_pt = dataset_samsum_clean.map(convert_examples_to_features,\n",
        "                                             batched=True)\n"
      ],
      "metadata": {
        "id": "DN8Ry5GS8XTT"
      },
      "id": "DN8Ry5GS8XTT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 Training"
      ],
      "metadata": {
        "id": "L9ftOotr_lyA"
      },
      "id": "L9ftOotr_lyA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.4.1 Data Collator\n",
        "\n",
        "When we have a huge amount of data, it's easy for our machine to run out of memory while training if we load all the data at once. That's the main reason of why we train in batches.\n",
        "\n",
        "To correctly form batches for our training, we can use the [`DataCollator`](https://huggingface.co/docs/transformers/main_classes/data_collator#data-collator) class. It helps us construct batches in the given correct shape of choice.\n",
        "\n",
        "There are some default data collators for different classes of models. In this case we'll use the [`DataCollatorForSeq2Seq` class](https://huggingface.co/docs/transformers/main_classes/data_collator#transformers.DataCollatorForSeq2Seq)."
      ],
      "metadata": {
        "id": "NGqezZo_F6kM"
      },
      "id": "NGqezZo_F6kM"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "seq2seq_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)"
      ],
      "metadata": {
        "id": "0sOnnN-JJUzc"
      },
      "id": "0sOnnN-JJUzc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.4.2 Training Arguments\n"
      ],
      "metadata": {
        "id": "auPAlTmlJeyA"
      },
      "id": "auPAlTmlJeyA"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "trainer_args = TrainingArguments(\n",
        "    output_dir='pegasus-samsum',               # Where to save model checkpoints and logs\n",
        "    num_train_epochs=1,                        # Number of full passes over the training dataset\n",
        "    warmup_steps=500,                          # Number of warmup steps for learning rate scheduler\n",
        "    per_device_train_batch_size=1,             # Batch size per GPU/TPU core/CPU during training\n",
        "    per_device_eval_batch_size=1,              # Batch size per GPU/TPU core/CPU during evaluation\n",
        "    weight_decay=0.01,                         # Strength of L2 weight regularization to prevent overfitting\n",
        "    logging_steps=10,                          # Log training metrics every 10 steps\n",
        "    eval_strategy='steps',               # Evaluate the model every `eval_steps`\n",
        "    eval_steps=500,                            # Number of steps between evaluations\n",
        "    save_steps=1e6,                            # Save model every 1,000,000 steps (effectively disables frequent saving)\n",
        "    gradient_accumulation_steps=16             # Accumulate gradients over 16 steps before performing a backward/update pass\n",
        ")\n"
      ],
      "metadata": {
        "id": "I9fk53wZJlN9"
      },
      "id": "I9fk53wZJlN9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(model=model_pegasus,\n",
        "                  args=trainer_args,\n",
        "                  tokenizer=tokenizer,\n",
        "                  data_collator=seq2seq_collator,\n",
        "                  train_dataset=dataset_samsum_clean['test'],   # Using 'test' for a quick example, otherwise for a real training we should use 'train'\n",
        "                  eval_dataset=dataset_samsum_clean['validation']\n",
        "                  )"
      ],
      "metadata": {
        "id": "oMq3wSnk4vhb"
      },
      "id": "oMq3wSnk4vhb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "hYqH_k6G5hyi"
      },
      "id": "hYqH_k6G5hyi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5 Evaluation Metrics: ROUGE and Beyond\n",
        "\n",
        "Evaluating the performance of text generation tasks like summarization requires special metrics that capture the **semantic and lexical similarity** between a model’s output and a reference text. One of the most widely used metrics for summarization is **ROUGE** (Recall-Oriented Understudy for Gisting Evaluation). ROUGE measures the overlap of n-grams (word sequences), word sequences, and longest common subsequences between the generated text and the ground truth summary.\n",
        "\n",
        "The most common ROUGE variants are:\n",
        "\n",
        "- `ROUGE-1`: Overlap of unigrams (single words)\n",
        "- `ROUGE-2`: Overlap of bigrams (two-word sequences)\n",
        "- `ROUGE-L`: Longest common subsequence (captures sentence-level similarity)\n",
        "\n",
        "ROUGE emphasizes *recall*, meaning it rewards summaries that successfully include important pieces of the reference text. The closest it is to $1$, the best our model is performing.\n",
        "\n",
        "Each NLP task tends to have its own set of suitable metrics. Here's a quick overview:\n",
        "\n",
        "| Task                      | Common Metrics                          | What It Measures                                   |\n",
        "|--------------------------|-----------------------------------------|----------------------------------------------------|\n",
        "| **Summarization**        | `ROUGE`, `BLEU`                         | Content overlap, fluency                          |\n",
        "| **Translation**          | `BLEU`, `METEOR`, `CHRF`                | N-gram matches, semantic similarity               |\n",
        "| **Text Generation**      | `BLEU`, `ROUGE`, `BERTScore`, `Perplexity` | Fluency, diversity, semantic similarity         |\n",
        "| **Question Answering**   | `Exact Match`, `F1 Score`               | Span correctness and token-level overlap          |\n",
        "| **Classification**       | `Accuracy`, `F1`, `Precision`, `Recall` | Correctness of predicted labels                   |\n",
        "| **Named Entity Recognition (NER)** | `F1 Score`, `Precision`, `Recall` | Entity extraction span correctness            |\n",
        "\n",
        "In summarization tasks, ROUGE-F1 score is often the most reported metric, as it balances precision and recall. For semantic understanding, metrics like BERTScore may also be used."
      ],
      "metadata": {
        "id": "mBnROL5A6z9X"
      },
      "id": "mBnROL5A6z9X"
    },
    {
      "cell_type": "code",
      "source": [
        "# Splits a list into batches of a given size for easier processing\n",
        "def generate_batch_sized_chunks(list_of_elements, batch_size):\n",
        "    \"\"\"Yield successive batch-sized chunks from list_of_elements.\"\"\"\n",
        "    for i in range(0, len(list_of_elements), batch_size):\n",
        "        yield list_of_elements[i : i + batch_size]    # yield is a memory-efficient alternative to return.\n",
        "\n",
        "\n",
        "# Calculates evaluation metric (like ROUGE) on a dataset using a model\n",
        "def calculate_metric_on_test_ds(dataset, metric, model, tokenizer,\n",
        "                               batch_size=16, device=device,\n",
        "                               column_text=\"article\",\n",
        "                               column_summary=\"highlights\"):\n",
        "    # Split the dataset into batches of input articles and target summaries\n",
        "    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n",
        "    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
        "\n",
        "    # Loop over batches and generate summaries\n",
        "    for article_batch, target_batch in tqdm(\n",
        "        zip(article_batches, target_batches), total=len(article_batches)):\n",
        "\n",
        "        # Tokenize the input batch of articles\n",
        "        inputs = tokenizer(article_batch, max_length=1024, truncation=True,\n",
        "                           padding=\"max_length\", return_tensors=\"pt\")\n",
        "\n",
        "        # Generate summaries with beam search and length penalty to avoid long output\n",
        "        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
        "                                   attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "                                   length_penalty=0.8, num_beams=8, max_length=128)\n",
        "\n",
        "        # Decode token IDs to strings, clean special tokens\n",
        "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n",
        "                                              clean_up_tokenization_spaces=True)\n",
        "                             for s in summaries]\n",
        "\n",
        "        # Replace empty tokens if any slipped in\n",
        "        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n",
        "\n",
        "        # Add generated vs reference summaries to the metric for scoring\n",
        "        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
        "\n",
        "    # Compute final ROUGE score across the dataset\n",
        "    score = metric.compute()\n",
        "    return score\n"
      ],
      "metadata": {
        "id": "QjIcjqXu688D"
      },
      "id": "QjIcjqXu688D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n",
        "\n",
        "from evaluate import load\n",
        "\n",
        "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
        "\n",
        "rouge_metric = load('rouge')"
      ],
      "metadata": {
        "id": "_RoK6huM8uZv"
      },
      "id": "_RoK6huM8uZv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = calculate_metric_on_test_ds(\n",
        "    dataset_samsum['test'][0:10], rouge_metric, trainer.model, tokenizer, batch_size = 2, column_text = 'dialogue', column_summary= 'summary'\n",
        ")\n",
        "\n",
        "rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n",
        "\n",
        "pd.DataFrame(rouge_dict, index = [f'pegasus'] )"
      ],
      "metadata": {
        "id": "ithvb7Mb8wb4"
      },
      "id": "ithvb7Mb8wb4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.6 Save and Load the Model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L73P5LhW-YhV"
      },
      "id": "L73P5LhW-YhV"
    },
    {
      "cell_type": "code",
      "source": [
        "## Save model\n",
        "model_pegasus.save_pretrained(\"pegasus-samsum-model\")"
      ],
      "metadata": {
        "id": "wLrFUz_y-fZz"
      },
      "id": "wLrFUz_y-fZz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Save tokenizer\n",
        "tokenizer.save_pretrained(\"tokenizer\")"
      ],
      "metadata": {
        "id": "qR4n8CbS-hSf"
      },
      "id": "qR4n8CbS-hSf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/tokenizer\")"
      ],
      "metadata": {
        "id": "P8UJ6tHG-j0d"
      },
      "id": "P8UJ6tHG-j0d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.7 Perform Inference with Our Model\n",
        "\n",
        "We can perform inference with our model."
      ],
      "metadata": {
        "id": "xvbKUuI6-mw5"
      },
      "id": "xvbKUuI6-mw5"
    },
    {
      "cell_type": "code",
      "source": [
        "#Prediction\n",
        "\n",
        "gen_kwargs = {\"length_penalty\": 0.8, # Controls how much the model penalizes long sequences during generation. < 1.0: Encourages longer outputs, > 1.0: Encourages shorter outputs.\n",
        "              \"num_beams\":8, # Enables beam search with 8 beams.\n",
        "              \"max_length\": 128}\n",
        "\n",
        "sample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\n",
        "\n",
        "reference = dataset_samsum[\"test\"][0][\"summary\"]\n",
        "\n",
        "pipe = pipeline(\"summarization\", model=\"pegasus-samsum-model\",tokenizer=tokenizer)\n",
        "\n",
        "print(\"Dialogue:\")\n",
        "print(sample_text)\n",
        "\n",
        "print(\"\\nReference Summary:\")\n",
        "print(reference)\n",
        "\n",
        "print(\"\\nModel Summary:\")\n",
        "print(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])"
      ],
      "metadata": {
        "id": "YMeoMbTO-xtE"
      },
      "id": "YMeoMbTO-xtE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model is not performing perfectly because we trained for only one epoch. If we trained it for more (and on the actual training set) we would get to a better performance."
      ],
      "metadata": {
        "id": "YrjzBmzQ_bRE"
      },
      "id": "YrjzBmzQ_bRE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Text To Image Generation (Diffusion)\n",
        "\n",
        "We will see how we can use a diffusion model for text to image generation from HF.\n",
        "\n",
        "We will use the [`diffusers`](https://huggingface.co/docs/diffusers/index#diffusers) library from Hugging Face."
      ],
      "metadata": {
        "id": "jdrkB_dRCLuR"
      },
      "id": "jdrkB_dRCLuR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Get Dipendencies"
      ],
      "metadata": {
        "id": "vyX8Q07DEVBB"
      },
      "id": "vyX8Q07DEVBB"
    },
    {
      "cell_type": "code",
      "source": [
        "# diffusers is a hugging face page for using diffusion models from huggingface hub\n",
        "!pip install diffusers transformers accelerate"
      ],
      "metadata": {
        "id": "FizqNpl_CR5x"
      },
      "id": "FizqNpl_CR5x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import torch"
      ],
      "metadata": {
        "id": "WQWWaIzFC3AD"
      },
      "id": "WQWWaIzFC3AD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show torch"
      ],
      "metadata": {
        "id": "zZW7lHLuC7lX"
      },
      "id": "zZW7lHLuC7lX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Choose Model"
      ],
      "metadata": {
        "id": "69ey-oshE5SS"
      },
      "id": "69ey-oshE5SS"
    },
    {
      "cell_type": "code",
      "source": [
        "model_id1 = \"dreamlike-art/dreamlike-diffusion-1.0\"   # https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0\n",
        "model_id2 = \"stabilityai/stable-diffusion-xl-base-1.0\"    # https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0\n",
        "\n",
        "model = StableDiffusionPipeline.from_pretrained(model_id1, torch_dtype=torch.float16, use_safetensors=True)\n",
        "model = model.to(\"cuda\")"
      ],
      "metadata": {
        "id": "eechNkc-Dvtp"
      },
      "id": "eechNkc-Dvtp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Use the Loaded Model"
      ],
      "metadata": {
        "id": "GZeQMxkUHAXY"
      },
      "id": "GZeQMxkUHAXY"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"dreamlikeart, a grungy woman with rainbow hair, travelling between dimensions, dynamic pose, happy, soft eyes and narrow chin,\n",
        "extreme bokeh, dainty figure, long hair straight down, torn kawaii shirt and baggy jeans\"\"\""
      ],
      "metadata": {
        "id": "KbZnHggiHCco"
      },
      "id": "KbZnHggiHCco",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = model(prompt).images[0]"
      ],
      "metadata": {
        "id": "nzVPgYlFHF8R"
      },
      "id": "nzVPgYlFHF8R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image"
      ],
      "metadata": {
        "id": "fS4JoMNxHKEc"
      },
      "id": "fS4JoMNxHKEc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Playing with Parameters\n"
      ],
      "metadata": {
        "id": "Z0pYA5yGHQaW"
      },
      "id": "Z0pYA5yGHQaW"
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_image(pipe, prompt, params):\n",
        "  img = pipe(prompt, **params).images\n",
        "\n",
        "  num_images = len(img)\n",
        "  if num_images>1:\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=num_images)\n",
        "    for i in range(num_images):\n",
        "      ax[i].imshow(img[i]);\n",
        "      ax[i].axis('off');\n",
        "\n",
        "  else:\n",
        "    fig = plt.figure()\n",
        "    plt.imshow(img[0]);\n",
        "    plt.axis('off');\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "U25hE8IOHVeb"
      },
      "id": "U25hE8IOHVeb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"dreamlike, beautiful girl playing the festival of colors, draped in traditional Indian attire, throwing colors\"\n",
        "\n",
        "params = {}"
      ],
      "metadata": {
        "id": "zJHQZW3EHYMg"
      },
      "id": "zJHQZW3EHYMg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_image(pipe, prompt, params)"
      ],
      "metadata": {
        "id": "0cw888ZGHYw9"
      },
      "id": "0cw888ZGHYw9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#num inference steps\n",
        "params = {'num_inference_steps': 100}\n",
        "\n",
        "generate_image(pipe, prompt, params)"
      ],
      "metadata": {
        "id": "vOqzMxbhHb8N"
      },
      "id": "vOqzMxbhHb8N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#height width\n",
        "params = {'num_inference_steps': 100, 'width': 512, 'height': int(1.5*640)}\n",
        "\n",
        "generate_image(pipe, prompt, params)"
      ],
      "metadata": {
        "id": "hXm1LDqeHeS6"
      },
      "id": "hXm1LDqeHeS6",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}