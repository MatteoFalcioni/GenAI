{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging Face Projects\n",
        "\n",
        "This notebook will be dedicated to using Hugging Face in order to code some interesting projects:\n",
        "\n",
        "1. Text Summarization  \n",
        "2. 2\n",
        "3. 3\n",
        "\n",
        "All of these will be done through fine tuning of existing baseline models.\n",
        "\n",
        "We will need a GPU in order to fine tune the models:"
      ],
      "metadata": {
        "id": "f795aa1f"
      },
      "id": "f795aa1f"
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "r86EUqWXxlKD"
      },
      "id": "r86EUqWXxlKD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Text Summarization Project (Seq2Seq)\n",
        "\n",
        "For this task we are going to use a class of models called *Seq2Seq*.\n",
        "\n",
        "Seq2Seq models map an input sequence to an output sequence — useful for tasks like translation, summarization, dialogue.\n",
        "Transformer-based Seq2Seq models (like T5 and BART) replaced older RNN-based ones, achieving much better performance."
      ],
      "metadata": {
        "id": "1XJev9W-xYYG"
      },
      "id": "1XJev9W-xYYG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Install Dependencies\n",
        "\n",
        "We need some packages in order to start with our project:"
      ],
      "metadata": {
        "id": "o8r4T04s3hWq"
      },
      "id": "o8r4T04s3hWq"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q\n",
        "!pip install --upgrade datasets -q"
      ],
      "metadata": {
        "id": "aozXk6pOyMa2"
      },
      "id": "aozXk6pOyMa2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# disinstall and re-install accelerate for gpu acceleration\n",
        "\n",
        "!pip install --upgrade accelerate\n",
        "!pip uninstall -y transformers accelerate  # sometimes colab uses older versions\n",
        "!pip install transformers accelerate  # now we're sure we're using a new version"
      ],
      "metadata": {
        "id": "BgHvynfyyTi2"
      },
      "id": "BgHvynfyyTi2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import to test that everything is fine\n",
        "\n",
        "from transformers import pipeline, set_seed\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer # For the model we're going to use\n",
        "from datasets import load_dataset, load_from_disk # For the datasets\n",
        "\n",
        "# python libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# tokenization\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "from tqdm import tqdm # just progress bar\n",
        "\n",
        "import torch\n",
        "\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "id": "eFZJS_VVzAIE"
      },
      "id": "eFZJS_VVzAIE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's check the device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "rJ8A09y8zjpl"
      },
      "id": "rJ8A09y8zjpl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose our model \"checkpoint\" (ckpt)\n",
        "model_ckpt = \"google/pegasus-cnn_dailymail\" # https://huggingface.co/google/pegasus-cnn_dailymail\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
      ],
      "metadata": {
        "id": "YJPEBLE1zzHi"
      },
      "id": "YJPEBLE1zzHi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model and send it to device\n",
        "model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
      ],
      "metadata": {
        "id": "71JSzWHm0SQM"
      },
      "id": "71JSzWHm0SQM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Get the Data"
      ],
      "metadata": {
        "id": "_JxFFwSF3mvd"
      },
      "id": "_JxFFwSF3mvd"
    },
    {
      "cell_type": "code",
      "source": [
        "# sometimes i have problems loading if i dont update datasets first...\n",
        "!pip install --upgrade datasets fsspec"
      ],
      "metadata": {
        "id": "MsgvObtd2pyV"
      },
      "id": "MsgvObtd2pyV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dataset\n",
        "dataset_samsum = load_dataset(\"knkarthick/samsum\") # https://huggingface.co/datasets/knkarthick/samsum"
      ],
      "metadata": {
        "id": "US5a9mwS036f"
      },
      "id": "US5a9mwS036f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_samsum  # it's composed of dialogue and summary couples"
      ],
      "metadata": {
        "id": "VwCU4bna1OVY"
      },
      "id": "VwCU4bna1OVY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_samsum[\"train\"][\"dialogue\"][1]"
      ],
      "metadata": {
        "id": "RHmyqOVc28U2"
      },
      "id": "RHmyqOVc28U2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_samsum[\"train\"][\"summary\"][1]"
      ],
      "metadata": {
        "id": "avBx7p-r3FQm"
      },
      "id": "avBx7p-r3FQm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samsum_train_df = pd.DataFrame(dataset_samsum['train'])\n",
        "samsum_test_df = pd.DataFrame(dataset_samsum['test'])"
      ],
      "metadata": {
        "id": "S99EUIm-Ay6Y"
      },
      "id": "S99EUIm-Ay6Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.1: Always inspect Your Data Thoroughly..."
      ],
      "metadata": {
        "id": "jV4GviYsFAoJ"
      },
      "id": "jV4GviYsFAoJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# I was getting an error when mapping my dataset, went back and checked the data for NaN values...\n",
        "\n",
        "print(samsum_train_df.isnull().sum())\n",
        "print(samsum_test_df.isnull().sum())"
      ],
      "metadata": {
        "id": "rax41cuvA7qh"
      },
      "id": "rax41cuvA7qh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samsum_train_df[samsum_train_df.isnull().any(axis=1)] # bad data here"
      ],
      "metadata": {
        "id": "BMiGTLeJBRt2"
      },
      "id": "BMiGTLeJBRt2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter the dataset to remove it\n",
        "# Define a filter function\n",
        "def clean_example(example):\n",
        "    return (example['dialogue'] is not None and\n",
        "            example['summary'] is not None)\n",
        "\n",
        "# Apply the filter to each split\n",
        "dataset_samsum_clean = dataset_samsum.map(lambda x: x, remove_columns=[])  # make a copy\n",
        "dataset_samsum_clean['train'] = dataset_samsum['train'].filter(clean_example)\n",
        "dataset_samsum_clean['validation'] = dataset_samsum['validation'].filter(clean_example)\n",
        "dataset_samsum_clean['test'] = dataset_samsum['test'].filter(clean_example)"
      ],
      "metadata": {
        "id": "4oKsuFwLBaSH"
      },
      "id": "4oKsuFwLBaSH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note:** Hugging Face DatasetDict objects are immutable by default.\n",
        ">\n",
        "> When you apply `.filter()`, it returns a new object — it doesn't modify the original\n",
        "dataset in-place.\n",
        ">\n",
        ">If you want to keep your original `dataset_samsum` untouched, you can make a copy before applying filters.\n",
        ">```python\n",
        "dataset_samsum_clean = dataset_samsum.map(lambda x: x, remove_columns=[])\n",
        "```\n",
        ">This trick is used to make a shallow copy of the dataset before you start modifying (filtering) it, to avoid messing up the original.\n",
        ">\n",
        "> In this case we didn't really need to keep the original with NaN values, but just for safety I made a copy first."
      ],
      "metadata": {
        "id": "E3w_cPRkD2aM"
      },
      "id": "E3w_cPRkD2aM"
    },
    {
      "cell_type": "code",
      "source": [
        "samsum_train_df = pd.DataFrame(dataset_samsum_clean['train'])\n",
        "print(samsum_train_df.isnull().sum())\n",
        "print(samsum_test_df.isnull().sum())"
      ],
      "metadata": {
        "id": "BACpPXQwDiEw"
      },
      "id": "BACpPXQwDiEw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Preprocess data (embedding)"
      ],
      "metadata": {
        "id": "sdqndh5j3HU5"
      },
      "id": "sdqndh5j3HU5"
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_examples_to_features(example_batch):\n",
        "  \"\"\"\n",
        "  Encodes the dataset in batches\n",
        "  \"\"\"\n",
        "\n",
        "  input_encodings = tokenizer(example_batch['dialogue'],\n",
        "                              padding='max_length',\n",
        "                              max_length=1024,\n",
        "                              truncation=True)\n",
        "\n",
        "  with tokenizer.as_target_tokenizer(): # target tokenizer context manager (see below)\n",
        "    target_encodings = tokenizer(example_batch['summary'],\n",
        "                                 padding='max_length',\n",
        "                                 max_length=128,\n",
        "                                 truncation=True)\n",
        "\n",
        "  return {  # tutti i tokenizer ritornano input_ids attention_mask etc.? o Hanno strutture diverse\n",
        "            'input_ids' : input_encodings['input_ids'],\n",
        "            'attention_mask' : input_encodings['attention_mask'],\n",
        "            'labels' : target_encodings['input_ids']\n",
        "  }"
      ],
      "metadata": {
        "id": "NCaaHHJv3uqP"
      },
      "id": "NCaaHHJv3uqP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note:**\n",
        ">\n",
        "> In sequence-to-sequence (seq2seq) models like Pegasus, it is essential to differentiate between input tokens and target tokens during tokenization. Although the tokenizer might appear the same for both, using `tokenizer.as_target_tokenizer()` ensures that tokenization parameters and settings are properly adjusted for the target side (decoder). This is crucial because the model processes the source text through the encoder and generates the target text through the decoder. Properly tokenizing targets guarantees that the model receives the correct input format for loss computation and sequence generation. Without this distinction, the model could misinterpret the labels, leading to incorrect training and poor performance.\n"
      ],
      "metadata": {
        "id": "up0Kdnju9OmY"
      },
      "id": "up0Kdnju9OmY"
    },
    {
      "cell_type": "code",
      "source": [
        "# apply tokenization with map\n",
        "dataset_samsum_pt = dataset_samsum_clean.map(convert_examples_to_features,\n",
        "                                             batched=True)\n"
      ],
      "metadata": {
        "id": "DN8Ry5GS8XTT"
      },
      "id": "DN8Ry5GS8XTT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Training"
      ],
      "metadata": {
        "id": "L9ftOotr_lyA"
      },
      "id": "L9ftOotr_lyA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4.1 Data Collator\n",
        "\n",
        "When we have a huge amount of data, it's easy for our machine to run out of memory while training if we load all the data at once. That's the main reason of why we train in batches.\n",
        "\n",
        "To correctly form batches for our training, we can use the [`DataCollator`](https://huggingface.co/docs/transformers/main_classes/data_collator#data-collator) class. It helps us construct batches in the given correct shape of choice.\n",
        "\n",
        "There are some default data collators for different classes of models. In this case we'll use the [`DataCollatorForSeq2Seq` class](https://huggingface.co/docs/transformers/main_classes/data_collator#transformers.DataCollatorForSeq2Seq)."
      ],
      "metadata": {
        "id": "NGqezZo_F6kM"
      },
      "id": "NGqezZo_F6kM"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "seq2seq_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)"
      ],
      "metadata": {
        "id": "0sOnnN-JJUzc"
      },
      "id": "0sOnnN-JJUzc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.4.2 Training Arguments\n"
      ],
      "metadata": {
        "id": "auPAlTmlJeyA"
      },
      "id": "auPAlTmlJeyA"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "trainer_args = TrainingArguments(\n",
        "    output_dir =\n",
        ")"
      ],
      "metadata": {
        "id": "I9fk53wZJlN9"
      },
      "id": "I9fk53wZJlN9",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}