{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "338a8299",
   "metadata": {},
   "source": [
    "# Injected State\n",
    "\n",
    "While working with [RAGV4](https://github.com/MatteoFalcioni/experiments/blob/main/RAG_V4.0_.ipynb) I noticed a huge problem. \n",
    "\n",
    "I was building an agent supervisor system that managed two worker agents - a data analyst and a visualizer. Problem was, when the data analyst had completed its analysis on the datasets, the visualizer wasn't able to access the analysed data, and he would try to run the analysis again. \n",
    "\n",
    "Why did this happen? Well, the datasets were loaded in memory and stored as global variables in a dictionary. But this made it difficult for different agents to retrieve the same data, as the global needed to be shared and the possibility to access it needed to be enforced through prompting. Isn't there a standard way of letting all agents access the same data at runtime?\n",
    "\n",
    "Of course there is, and it's actually a basic concept in LangGraph: it's the graph's **State**. If we want some object to be accessible and visible to all agents at any time, than that object should be in the Graph state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a174a646",
   "metadata": {},
   "source": [
    "So we'd just need to subclass the basic `MessagesState` class from LangGraph and add our relevant object to state to define our *\"state schema\"*, like this: \n",
    "\n",
    "```python\n",
    "class MyState(MessagesState):   # already contains a structure like Annotated[Sequence[BaseMessage], operator.add]\n",
    "    dataframes : dict   # add your needed fields \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ffd644",
   "metadata": {},
   "source": [
    "Perfect, right...? No! If we only did this and tried to access the `dataframes` dict inside our tools, we wouldn't manage to do so. \n",
    "\n",
    "We have to follow a specific syntax in order to access state data in our tools and modify it. We need to use the [`InjectedState`](https://langchain-ai.github.io/langgraph/reference/agents/#langgraph.prebuilt.tool_node.InjectedState) annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377338f7",
   "metadata": {},
   "source": [
    "### Using `InjectedState` in tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6fd210",
   "metadata": {},
   "source": [
    "The following is an example from the [`InjectedState`](https://langchain-ai.github.io/langgraph/reference/agents/#langgraph.prebuilt.tool_node.InjectedState) documentation.\n",
    "\n",
    "Here they don't subclass `MessagesState` but the principle is the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae077438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [ToolMessage(content='not enough messages', name='state_tool', tool_call_id='1'),\n",
       "  ToolMessage(content='bar2', name='foo_tool', tool_call_id='2')]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage, AIMessage\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "from langgraph.prebuilt import InjectedState, ToolNode\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):    # create your state schema\n",
    "    messages: List[BaseMessage]\n",
    "    foo: str\n",
    "\n",
    "@tool\n",
    "def state_tool(x: int, state: Annotated[dict, InjectedState]) -> str:   # use Annotated[dict, InjectedState]\n",
    "    '''Do something with state.'''\n",
    "    if len(state[\"messages\"]) > 2:      # here we use the whole state\n",
    "        return state[\"foo\"] + str(x)\n",
    "    else:\n",
    "        return \"not enough messages\"\n",
    "\n",
    "@tool\n",
    "def foo_tool(x: int, foo: Annotated[str, InjectedState(\"foo\")]) -> str: # we can select a specific field to pass with InjectedState(\"<field_name>\")\n",
    "    '''Do something else with state.'''\n",
    "    return foo + str(x + 1)\n",
    "\n",
    "node = ToolNode([state_tool, foo_tool])\n",
    "\n",
    "tool_call1 = {\"name\": \"state_tool\", \"args\": {\"x\": 1}, \"id\": \"1\", \"type\": \"tool_call\"}\n",
    "tool_call2 = {\"name\": \"foo_tool\", \"args\": {\"x\": 1}, \"id\": \"2\", \"type\": \"tool_call\"}\n",
    "state = {\n",
    "    \"messages\": [AIMessage(\"\", tool_calls=[tool_call1, tool_call2])],\n",
    "    \"foo\": \"bar\",\n",
    "}\n",
    "node.invoke(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d786f50f",
   "metadata": {},
   "source": [
    "### Integrating `InjectedState` with agents\n",
    "\n",
    "The simplest way to integrate `InjectedState` with agentic framework is to use the [`create_react_agent()`](https://langchain-ai.github.io/langgraph/reference/agents/#langgraph.prebuilt.chat_agent_executor.create_react_agent) function from LangGraph. \n",
    "\n",
    "We need to pass our custom state as the `state_schema` parameter, like this:\n",
    "\n",
    "```python\n",
    "agent = create_react_agent(\n",
    "    model=..., \n",
    "    tools=[state_tool, foo_tool],\n",
    "    state_schema=MyState\n",
    ")\n",
    "```\n",
    "\n",
    "In this way the model knows what states it's working with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9ba594",
   "metadata": {},
   "source": [
    "### Other Context Management practises \n",
    "\n",
    "Before moving to a practical example, allow us to cite [other common context management practises in LangGraph](https://langchain-ai.github.io/langgraph/how-tos/tool-calling/#context-management).\n",
    "\n",
    "As a matter of fact, `InjectedState` is not the only way to allow our graph state to persist as context data. It is the most flexible and \"lightweight\" standard, but we can use: \n",
    "\n",
    "* [`Configuration`](https://langchain-ai.github.io/langgraph/how-tos/tool-calling/#configuration) : \"*Use configuration when you have static, immutable runtime data that tools require, such as user identifiers\"*. \n",
    "\n",
    "* [Long term memory](https://langchain-ai.github.io/langgraph/how-tos/tool-calling/#long-term-memory) : *\"Use long-term memory to store user-specific or application-specific data across different sessions\"*. The main difference here is that the data persists to other sources (like disk) even after the current session ends. This is useful for working with heavy datasets, or to leverage memory across different runs of a chatbot. See [Stores](https://langchain-ai.github.io/langgraph/reference/store/#storage) for further references."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1250c1b4",
   "metadata": {},
   "source": [
    "## Example #1 of `InjectedState` workflow\n",
    "\n",
    "Let's make a simple example to recap the actual workflow with `InjectedState`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3359cec",
   "metadata": {},
   "source": [
    "### 1. Create your custom \"`state_schema`\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b97a0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "\n",
    "class CustomState(MessagesState): \n",
    "    username : str \n",
    "    remaining_steps : int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b86d3b",
   "metadata": {},
   "source": [
    ">**Note:** from the [create_react_agent() doc](https://langchain-ai.github.io/langgraph/reference/agents/#langgraph.prebuilt.chat_agent_executor.create_react_agent):  \n",
    ">\n",
    ">*`state_schema` : An optional state schema that defines graph state. Must have `messages` and `remaining_steps` keys. Defaults to AgentState that defines those two keys.*\n",
    ">\n",
    "> `messages` is implemented by `MessagesState`, but we need to implement `reamining_steps` otherwise it will error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc6c77a",
   "metadata": {},
   "source": [
    "### 2. Write your tools using `InjectedState`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adb4b52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import InjectedState\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import tool, InjectedToolCallId\n",
    "from langgraph.types import Command\n",
    "\n",
    "@tool \n",
    "def get_internal_value(state : Annotated[CustomState, InjectedState]) -> str:\n",
    "    \"\"\"tool to retrieve the username\"\"\"\n",
    "    return state.get('username')\n",
    "\n",
    "@tool \n",
    "def update_username(new_name : str, tool_call_id : Annotated[str, InjectedToolCallId]\n",
    ") -> Command:\n",
    "    \"\"\"Update username in short-term memory.\"\"\"\n",
    "    \n",
    "    return Command(update={\n",
    "        \"username\" : new_name,\n",
    "        \"messages\" : [\n",
    "            ToolMessage(f\"Updated username to {new_name}\", tool_call_id=tool_call_id)\n",
    "        ]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b909be",
   "metadata": {},
   "source": [
    ">**Note:** Notice how we used: \n",
    ">   - `state.get()` to read the value\n",
    ">   - a `Command` return in order to update the state : here we also need to append to messages a `ToolMessage`, otherwise it will error. In order to do so, we constructed it with `Annotated[str, InjectedToolCallId]` to follow the correct approach - but we could have done it in a simpler way like `ToolMessage(\"Success\", tool_call_id=...)` as the error suggests:\n",
    ">\n",
    ">   *Expected to have a matching ToolMessage in Command.update for tool 'update_username', got: []. Every tool call (LLM requesting to call a tool) in the message history MUST have a corresponding ToolMessage. You can fix it by modifying the tool to return `Command(update=[ToolMessage(\"Success\", tool_call_id=tool_call_id), ...], ...)`*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be688d3a",
   "metadata": {},
   "source": [
    "### 3. Create the agent passing the custom `state_schema` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f84897c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=ChatOpenAI(model=\"gpt-4o\"),\n",
    "    tools=[update_username, get_internal_value],\n",
    "    state_schema=CustomState\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c6050f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your previous username was \"Matteo\". I have updated it to \"Mario\". The username now is \"Mario\".\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "initial_state = {\n",
    "    \"messages\": [HumanMessage(content=\"Whats my username? Update it to Mario. What's the username now?\")],\n",
    "    \"username\": \"Matteo\",\n",
    "    \"remaining_steps\": 15\n",
    "}\n",
    "\n",
    "print(agent.invoke(initial_state)[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ebf364",
   "metadata": {},
   "source": [
    "## RAGV4 Example\n",
    "\n",
    "How about a practical application? \n",
    "\n",
    "Let's correctly build RAGV4 using `InjectedState`. We will only build the data analyst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c30b2f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup keys\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4b9b038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_if_undefined(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"Please provide your {var}\")\n",
    "\n",
    "\n",
    "_set_if_undefined(\"OPENAI_API_KEY\")\n",
    "_set_if_undefined(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5947e107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool, InjectedToolCallId\n",
    "from langchain_core.messages import ToolMessage\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from langgraph.prebuilt import InjectedState\n",
    "from typing_extensions import Annotated\n",
    "from typing import Dict, Union\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langgraph.types import Command\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "\n",
    "DATASET_FOLDER = \"./LLM_data\"\n",
    "\n",
    "# ----------------------\n",
    "# Define state schema\n",
    "# ----------------------\n",
    "\n",
    "class DatasetState(MessagesState):\n",
    "    loaded: Dict[str, Union[pd.DataFrame, gpd.GeoDataFrame]]  # will store either pd.DataFrame or gpd.GeoDataFrame\n",
    "    descriptions: Dict[str, str]    # will store datasets descriptions\n",
    "    remaining_steps: int\n",
    "\n",
    "# ----------------------\n",
    "# Tool: list datasets\n",
    "# ----------------------\n",
    "@tool\n",
    "def list_loadable_datasets() -> str:\n",
    "    \"\"\"Lists all available parquet datasets in the dataset folder.\"\"\"\n",
    "    files = [f for f in os.listdir(DATASET_FOLDER) if f.endswith(\".parquet\")]\n",
    "    return \"\\n\".join(files) if files else \"No parquet datasets found.\"\n",
    "\n",
    "@tool\n",
    "def list_inmemory_datasets(state: Annotated[DatasetState, InjectedState]) -> str:\n",
    "    \"\"\"Lists all loaded datasets and their type (DataFrame or GeoDataFrame).\"\"\"\n",
    "    if not state[\"loaded\"]:\n",
    "        return \"No loaded datasets in memory. Use list_loadable_datasets() to see available files.\"\n",
    "    \n",
    "    lines = []\n",
    "    for name, df in state[\"loaded\"].items():\n",
    "        dtype = \"GeoDataFrame\" if isinstance(df, gpd.GeoDataFrame) else \"DataFrame\"\n",
    "        lines.append(f\"- {name}: {dtype} (shape={df.shape})\")\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# ----------------------\n",
    "# Tool: load dataset\n",
    "# ----------------------\n",
    "@tool\n",
    "def load_dataset_named(file_name: str, state: Annotated[DatasetState, InjectedState], tool_call_id: Annotated[str, InjectedToolCallId]) -> Command:\n",
    "    \"\"\"\n",
    "    Loads a Parquet dataset and its description.\n",
    "    If geometry is present, loads as GeoDataFrame.\n",
    "    Updates state['loaded'][name] and state['descriptions'][name].\n",
    "    \"\"\"\n",
    "    loaded = state.get('loaded')\n",
    "    descriptions = state.get('descriptions')\n",
    "\n",
    "    file_stem = Path(file_name).stem\n",
    "    file_name = f\"{file_stem}.parquet\"\n",
    "    path = Path(DATASET_FOLDER) / file_name\n",
    "\n",
    "    if not path.exists():\n",
    "        available_files = os.listdir(DATASET_FOLDER)\n",
    "        return f\"File '{file_name}' not found. Available files: {available_files}\"\n",
    "\n",
    "    # Load DataFrame\n",
    "    try:\n",
    "        df = pd.read_parquet(path)\n",
    "        if \"geometry\" in df.columns:\n",
    "            try:\n",
    "                df = gpd.read_parquet(path)\n",
    "            except Exception as geo_err:\n",
    "                return f\"Geometry column found but failed to load as GeoDataFrame: {geo_err}\"\n",
    "        loaded[file_stem] = df\n",
    "    except Exception as e:\n",
    "        return f\"Error loading dataset '{file_name}': {e}\"\n",
    "\n",
    "    # Load description\n",
    "    desc_path = Path(DATASET_FOLDER) / f\"{file_stem}.txt\"\n",
    "    try:\n",
    "        with open(desc_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_desc = f.read()\n",
    "    except Exception as e:\n",
    "        raw_desc = \"Description file missing or unreadable.\"\n",
    "\n",
    "    # Enrich and save\n",
    "    dtype_str = type(df).__name__   # DataFrame or GeoDataFrame\n",
    "    head_str = df.head().to_string(index=False)\n",
    "    cols_str = \", \".join(df.columns)\n",
    "    enriched_desc = f\"{dtype_str}\\n{raw_desc}\\n\\n---\\nPreview (first rows):\\n{head_str}\\n\\nColumns: {cols_str}\"\n",
    "\n",
    "    descriptions[file_stem] = enriched_desc\n",
    "\n",
    "    return Command(update={\n",
    "        \"loaded\" : loaded,\n",
    "        \"descriptions\" : descriptions,\n",
    "        \"messages\" : [\n",
    "            ToolMessage(f\"Updated state dictionaries with loaded[{file_stem}] and descriptions[{file_stem}]\", tool_call_id=tool_call_id)\n",
    "        ]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20876304",
   "metadata": {},
   "source": [
    ">**Incredibly important:** As we saw above, **all state updates from tool must be made through `Command`**. If you try to update states just by reassignment, this will fail silently! \n",
    ">\n",
    ">The correct way is to return a `Command` with the updates. What I also did, since I wanted to add entries to my existing dictionaries, was get them with the `.get()` function, then add a given dictionary, and then update with the new dictionaries in `Command`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86c35987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# Tool: python repl\n",
    "# ----------------------\n",
    "repl = PythonREPL()\n",
    "# Now use the tool with your injected REPL\n",
    "@tool\n",
    "def python_repl_tool(\n",
    "    code: Annotated[str, \"The python code to execute\"], state: Annotated[DatasetState, InjectedState]\n",
    "):\n",
    "    \"\"\"\n",
    "    Use this to execute python code. If you want to see the output of a value,\n",
    "    you should print it out with `print(...)`. This is visible to the user.\n",
    "    Datasets are available as variables like `quartieri`, and descriptions as a dict `descriptions`['quartieri']\n",
    "    \"\"\"\n",
    "\n",
    "    for name, df in state[\"loaded\"].items():\n",
    "        repl.globals[name] = df\n",
    "\n",
    "    # Inject descriptions as a dictionary\n",
    "    repl.globals[\"descriptions\"] = state[\"descriptions\"]\n",
    "    \n",
    "    try:\n",
    "        result = repl.run(code)\n",
    "    except BaseException as e:\n",
    "        return f\"Failed to execute. Error: {repr(e)}\"\n",
    "    return f\"Successfully executed:\\n```python\\n{code}\\n```\\nStdout: {result}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18c6e0a",
   "metadata": {},
   "source": [
    ">**Note for `python_repl()`:** a subtle mistake is that if not specified in the docstring, the llm will not print results from the repl and therefore it will not understand the code it's writing. *So always specify it!*\n",
    ">\n",
    ">Also, Claude 4 understands it better than gpt-4o. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54f66e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\"You are a data analyst. Use your tools to explore and load datasets relevant to the task.\\n\"\n",
    "        \"The files you need to load are in the subdirectory at ./LLM_data\\n\\n\"\n",
    "        \"Always check the description before working with a dataframe, in order to see what columns and values you are working with.\\n\"\n",
    "        \"You can also check the datasets loaded in memory with your list_inmemory_datasets() tool, \\\n",
    "        and check available datasets to load with the list_loadable_datasets() tool.\"\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22a866da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "analyst_agent = create_react_agent(\n",
    "    model=\"openai:gpt-4o\",\n",
    "    tools=[list_loadable_datasets, list_inmemory_datasets, load_dataset_named, python_repl_tool],\n",
    "    prompt=prompt,\n",
    "    name=\"data_analyst\",\n",
    "    state_schema=DatasetState\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bdf055",
   "metadata": {},
   "source": [
    ">**Note:** Don't forget `state_schema`, otherwise the model won't know what its state is supposed to look like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e5e0cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "\n",
    "def pretty_print_message(message, indent=False):\n",
    "    pretty_message = message.pretty_repr(html=True)\n",
    "    if not indent:\n",
    "        print(pretty_message)\n",
    "        return\n",
    "\n",
    "    indented = \"\\n\".join(\"\\t\" + c for c in pretty_message.split(\"\\n\"))\n",
    "    print(indented)\n",
    "\n",
    "\n",
    "def pretty_print_messages(update, last_message=False):\n",
    "    is_subgraph = False\n",
    "    if isinstance(update, tuple):\n",
    "        ns, update = update\n",
    "        # skip parent graph updates in the printouts\n",
    "        if len(ns) == 0:\n",
    "            return\n",
    "\n",
    "        graph_id = ns[-1].split(\":\")[0]\n",
    "        print(f\"Update from subgraph {graph_id}:\")\n",
    "        print(\"\\n\")\n",
    "        is_subgraph = True\n",
    "\n",
    "    for node_name, node_update in update.items():\n",
    "        update_label = f\"Update from node {node_name}:\"\n",
    "        if is_subgraph:\n",
    "            update_label = \"\\t\" + update_label\n",
    "\n",
    "        print(update_label)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        messages = convert_to_messages(node_update[\"messages\"])\n",
    "        if last_message:\n",
    "            messages = messages[-1:]\n",
    "\n",
    "        for m in messages:\n",
    "            pretty_print_message(m, indent=is_subgraph)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d8cebe",
   "metadata": {},
   "source": [
    ">**Final note:** passing initial input without initialized values will error. This should probably be done in a better way - like initializing values to a default, or making them Optional (this could make sense if we have some agents that don't need datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7a8f8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node agent:\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: data_analyst\n",
      "Tool Calls:\n",
      "  list_loadable_datasets (call_59o3GIDynOIRFWMX8OuaUxE7)\n",
      " Call ID: call_59o3GIDynOIRFWMX8OuaUxE7\n",
      "  Args:\n",
      "  list_inmemory_datasets (call_E6XjxCCL8Pz4dpYpxcToR2P9)\n",
      " Call ID: call_E6XjxCCL8Pz4dpYpxcToR2P9\n",
      "  Args:\n",
      "\n",
      "\n",
      "Update from node tools:\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: list_loadable_datasets\n",
      "\n",
      "neighborhoods.parquet\n",
      "public_bathrooms.parquet\n",
      "median_income_by_statistical_area.parquet\n",
      "neighborhood_residents_data_1986to2024.parquet\n",
      "neighborhood_socio_demographic_data_lastupdated2019.parquet\n",
      "statistical_zones.parquet\n",
      "pharmacies.parquet\n",
      "points_of_interest.parquet\n",
      "\n",
      "\n",
      "Update from node tools:\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: list_inmemory_datasets\n",
      "\n",
      "No loaded datasets in memory. Use list_loadable_datasets() to see available files.\n",
      "\n",
      "\n",
      "Update from node agent:\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: data_analyst\n",
      "Tool Calls:\n",
      "  load_dataset_named (call_dBFl381YyEbjKKXTzcZM5kdz)\n",
      " Call ID: call_dBFl381YyEbjKKXTzcZM5kdz\n",
      "  Args:\n",
      "    file_name: neighborhoods.parquet\n",
      "  load_dataset_named (call_sFYxEDLPv8RZPXUUn2aCLhbA)\n",
      " Call ID: call_sFYxEDLPv8RZPXUUn2aCLhbA\n",
      "  Args:\n",
      "    file_name: public_bathrooms.parquet\n",
      "\n",
      "\n",
      "Update from node tools:\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: load_dataset_named\n",
      "\n",
      "Updated state dictionaries with loaded[public_bathrooms] and descriptions[public_bathrooms]\n",
      "\n",
      "\n",
      "Update from node tools:\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: load_dataset_named\n",
      "\n",
      "Updated state dictionaries with loaded[neighborhoods] and descriptions[neighborhoods]\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "InvalidUpdateError",
     "evalue": "At key 'loaded': Can receive only one value per step. Use an Annotated key to handle multiple values.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_CONCURRENT_GRAPH_UPDATE",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidUpdateError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 11\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      4\u001b[0m initial_state_2 \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoad a dataset of your choice, read its description, then load another. Then list the datasets you have in memory.\u001b[39m\u001b[38;5;124m\"\u001b[39m)],\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mremaining_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m22\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded\u001b[39m\u001b[38;5;124m\"\u001b[39m: {},\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescriptions\u001b[39m\u001b[38;5;124m\"\u001b[39m: {}\n\u001b[1;32m      9\u001b[0m }\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m analyst_agent\u001b[38;5;241m.\u001b[39mstream(initial_state_2):\n\u001b[1;32m     12\u001b[0m     pretty_print_messages(chunk)\n",
      "File \u001b[0;32m~/miniconda3/envs/openAI/lib/python3.10/site-packages/langgraph/pregel/__init__.py:2544\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[0m\n\u001b[1;32m   2534\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   2535\u001b[0m             [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mwrites],\n\u001b[1;32m   2536\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2539\u001b[0m         ):\n\u001b[1;32m   2540\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   2541\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m _output(\n\u001b[1;32m   2542\u001b[0m                 stream_mode, print_mode, subgraphs, stream\u001b[38;5;241m.\u001b[39mget, queue\u001b[38;5;241m.\u001b[39mEmpty\n\u001b[1;32m   2543\u001b[0m             )\n\u001b[0;32m-> 2544\u001b[0m         \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_tick\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2545\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m _output(\n\u001b[1;32m   2547\u001b[0m     stream_mode, print_mode, subgraphs, stream\u001b[38;5;241m.\u001b[39mget, queue\u001b[38;5;241m.\u001b[39mEmpty\n\u001b[1;32m   2548\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/openAI/lib/python3.10/site-packages/langgraph/pregel/loop.py:526\u001b[0m, in \u001b[0;36mPregelLoop.after_tick\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    524\u001b[0m writes \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mwrites]\n\u001b[1;32m    525\u001b[0m \u001b[38;5;66;03m# all tasks have finished\u001b[39;00m\n\u001b[0;32m--> 526\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdated_channels \u001b[38;5;241m=\u001b[39m \u001b[43mapply_writes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpointer_get_next_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrigger_to_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;66;03m# produce values output\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdated_channels\u001b[38;5;241m.\u001b[39misdisjoint(\n\u001b[1;32m    535\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys,)\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys\n\u001b[1;32m    538\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/openAI/lib/python3.10/site-packages/langgraph/pregel/algo.py:299\u001b[0m, in \u001b[0;36mapply_writes\u001b[0;34m(checkpoint, channels, tasks, get_next_version, trigger_to_nodes)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chan, vals \u001b[38;5;129;01min\u001b[39;00m pending_writes_by_channel\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chan \u001b[38;5;129;01min\u001b[39;00m channels:\n\u001b[0;32m--> 299\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mchannels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchan\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvals\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m next_version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    300\u001b[0m             checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchannel_versions\u001b[39m\u001b[38;5;124m\"\u001b[39m][chan] \u001b[38;5;241m=\u001b[39m next_version\n\u001b[1;32m    301\u001b[0m             \u001b[38;5;66;03m# unavailable channels can't trigger tasks, so don't add them\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/openAI/lib/python3.10/site-packages/langgraph/channels/last_value.py:58\u001b[0m, in \u001b[0;36mLastValue.update\u001b[0;34m(self, values)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(values) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     54\u001b[0m     msg \u001b[38;5;241m=\u001b[39m create_error_message(\n\u001b[1;32m     55\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt key \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: Can receive only one value per step. Use an Annotated key to handle multiple values.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     56\u001b[0m         error_code\u001b[38;5;241m=\u001b[39mErrorCode\u001b[38;5;241m.\u001b[39mINVALID_CONCURRENT_GRAPH_UPDATE,\n\u001b[1;32m     57\u001b[0m     )\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidUpdateError(msg)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mInvalidUpdateError\u001b[0m: At key 'loaded': Can receive only one value per step. Use an Annotated key to handle multiple values.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_CONCURRENT_GRAPH_UPDATE"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "import time\n",
    "\n",
    "initial_state_2 = {\n",
    "    \"messages\": [HumanMessage(content=\"Load a dataset of your choice, read its description, then load another. Then list the datasets you have in memory.\")],\n",
    "    \"remaining_steps\": 22,\n",
    "    \"loaded\": {},\n",
    "    \"descriptions\": {}\n",
    "}\n",
    "\n",
    "for chunk in analyst_agent.stream(initial_state_2):\n",
    "    pretty_print_messages(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aef9bb",
   "metadata": {},
   "source": [
    "^^ for this problem see https://github.com/langchain-ai/langgraph/discussions/1787 : basically the system is updating the state keys in parallel. But the solution should be simple. Also: https://www.reddit.com/r/LangChain/comments/1hxt5t7/help_me_understand_state_reducers_in_langgraph/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
