{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "338a8299",
   "metadata": {},
   "source": [
    "# Injected State\n",
    "\n",
    "While working with [RAGV4](https://github.com/MatteoFalcioni/experiments/blob/main/RAG_V4.0_.ipynb) I noticed a huge problem. \n",
    "\n",
    "I was building an agent supervisor system that managed two worker agents - a data analyst and a visualizer. Problem was, when the data analyst had completed its analysis on the datasets, the visualizer wasn't able to access the analysed data, and he would try to run the analysis again. \n",
    "\n",
    "Why did this happen? Well, the datasets were loaded in memory and stored as global variables in a dictionary. But this made it difficult for different agents to retrieve the same data, as the global needed to be shared and the possibility to access it needed to be enforced through prompting. Isn't there a standard way of letting all agents access the same data at runtime?\n",
    "\n",
    "Of course there is, and it's actually a basic concept in LangGraph: it's the graph's **State**. If we want some object to be accessible and visible to all agents at any time, than that object should be in the Graph state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a174a646",
   "metadata": {},
   "source": [
    "So we'd just need to subclass the basic `MessagesState` class from LangGraph and add our relevant object to state to define our *\"state schema\"*, like this: \n",
    "\n",
    "```python\n",
    "class MyState(MessagesState):   # already contains a structure like Annotated[Sequence[BaseMessage], operator.add]\n",
    "    dataframes : dict   # add your needed fields \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ffd644",
   "metadata": {},
   "source": [
    "Perfect, right...? No! If we only did this and tried to access the `dataframes` dict inside our tools, we wouldn't manage to do so. \n",
    "\n",
    "We have to follow a specific syntax in order to access state data in our tools and modify it. We need to use the [`InjectedState`](https://langchain-ai.github.io/langgraph/reference/agents/#langgraph.prebuilt.tool_node.InjectedState) annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377338f7",
   "metadata": {},
   "source": [
    "### Using `InjectedState` in tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6fd210",
   "metadata": {},
   "source": [
    "The following is an example from the [`InjectedState`](https://langchain-ai.github.io/langgraph/reference/agents/#langgraph.prebuilt.tool_node.InjectedState) documentation.\n",
    "\n",
    "Here they don't subclass `MessagesState` but the principle is the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae077438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [ToolMessage(content='not enough messages', name='state_tool', tool_call_id='1'),\n",
       "  ToolMessage(content='bar2', name='foo_tool', tool_call_id='2')]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage, AIMessage\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "from langgraph.prebuilt import InjectedState, ToolNode\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):    # create your state schema\n",
    "    messages: List[BaseMessage]\n",
    "    foo: str\n",
    "\n",
    "@tool\n",
    "def state_tool(x: int, state: Annotated[dict, InjectedState]) -> str:   # use Annotated[dict, InjectedState]\n",
    "    '''Do something with state.'''\n",
    "    if len(state[\"messages\"]) > 2:      # here we use the whole state\n",
    "        return state[\"foo\"] + str(x)\n",
    "    else:\n",
    "        return \"not enough messages\"\n",
    "\n",
    "@tool\n",
    "def foo_tool(x: int, foo: Annotated[str, InjectedState(\"foo\")]) -> str: # we can select a specific field to pass with InjectedState(\"<field_name>\")\n",
    "    '''Do something else with state.'''\n",
    "    return foo + str(x + 1)\n",
    "\n",
    "node = ToolNode([state_tool, foo_tool])\n",
    "\n",
    "tool_call1 = {\"name\": \"state_tool\", \"args\": {\"x\": 1}, \"id\": \"1\", \"type\": \"tool_call\"}\n",
    "tool_call2 = {\"name\": \"foo_tool\", \"args\": {\"x\": 1}, \"id\": \"2\", \"type\": \"tool_call\"}\n",
    "state = {\n",
    "    \"messages\": [AIMessage(\"\", tool_calls=[tool_call1, tool_call2])],\n",
    "    \"foo\": \"bar\",\n",
    "}\n",
    "node.invoke(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d786f50f",
   "metadata": {},
   "source": [
    "### Integrating `InjectedState` with agents\n",
    "\n",
    "The simplest way to integrate `InjectedState` with agentic framework is to use the [`create_react_agent()`](https://langchain-ai.github.io/langgraph/reference/agents/#langgraph.prebuilt.chat_agent_executor.create_react_agent) function from LangGraph. \n",
    "\n",
    "We need to pass our custom state as the `state_schema` parameter, like this:\n",
    "\n",
    "```python\n",
    "agent = create_react_agent(\n",
    "    model=..., \n",
    "    tools=[state_tool, foo_tool],\n",
    "    state_schema=MyState\n",
    ")\n",
    "```\n",
    "\n",
    "In this way the model knows what states it's working with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9ba594",
   "metadata": {},
   "source": [
    "### Other Context Management practises \n",
    "\n",
    "Before moving to a practical example, allow us to cite [other common context management practises in LangGraph](https://langchain-ai.github.io/langgraph/how-tos/tool-calling/#context-management).\n",
    "\n",
    "As a matter of fact, `InjectedState` is not the only way to allow our graph state to persist as context data. It is the most flexible and \"lightweight\" standard, but we can use: \n",
    "\n",
    "* [`Configuration`](https://langchain-ai.github.io/langgraph/how-tos/tool-calling/#configuration) : \"*Use configuration when you have static, immutable runtime data that tools require, such as user identifiers\"*. \n",
    "\n",
    "* [Long term memory](https://langchain-ai.github.io/langgraph/how-tos/tool-calling/#long-term-memory) : *\"Use long-term memory to store user-specific or application-specific data across different sessions\"*. The main difference here is that the data persists to other sources (like disk) even after the current session ends. This is useful for working with heavy datasets, or to leverage memory across different runs of a chatbot. See [Stores](https://langchain-ai.github.io/langgraph/reference/store/#storage) for further references."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1250c1b4",
   "metadata": {},
   "source": [
    "## Example #1 of `InjectedState` workflow\n",
    "\n",
    "Let's make a simple example to recap the actual workflow with `InjectedState`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3359cec",
   "metadata": {},
   "source": [
    "### 1. Create your custom \"`state_schema`\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b97a0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "\n",
    "class CustomState(MessagesState): \n",
    "    username : str \n",
    "    remaining_steps : int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b86d3b",
   "metadata": {},
   "source": [
    ">**Note:** from the [create_react_agent() doc](https://langchain-ai.github.io/langgraph/reference/agents/#langgraph.prebuilt.chat_agent_executor.create_react_agent):  \n",
    ">\n",
    ">*`state_schema` : An optional state schema that defines graph state. Must have `messages` and `remaining_steps` keys. Defaults to AgentState that defines those two keys.*\n",
    ">\n",
    "> `messages` is implemented by `MessagesState`, but we need to implement `reamining_steps` otherwise it will error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc6c77a",
   "metadata": {},
   "source": [
    "### 2. Write your tools using `InjectedState`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adb4b52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import InjectedState\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import tool, InjectedToolCallId\n",
    "from langgraph.types import Command\n",
    "\n",
    "@tool \n",
    "def get_internal_value(state : Annotated[CustomState, InjectedState]) -> str:\n",
    "    \"\"\"tool to retrieve the username\"\"\"\n",
    "    return state.get('username')\n",
    "\n",
    "@tool \n",
    "def update_username(new_name : str, tool_call_id : Annotated[str, InjectedToolCallId]\n",
    ") -> Command:\n",
    "    \"\"\"Update username in short-term memory.\"\"\"\n",
    "    \n",
    "    return Command(update={\n",
    "        \"username\" : new_name,\n",
    "        \"messages\" : [\n",
    "            ToolMessage(f\"Updated username to {new_name}\", tool_call_id=tool_call_id)\n",
    "        ]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b909be",
   "metadata": {},
   "source": [
    ">**Note:** Notice how we used: \n",
    ">   - `state.get()` to read the value\n",
    ">   - a `Command` return in order to update the state : here we also need to append to messages a `ToolMessage`, otherwise it will error. In order to do so, we constructed it with `Annotated[str, InjectedToolCallId]` to follow the correct approach - but we could have done it in a simpler way like `ToolMessage(\"Success\", tool_call_id=...)` as the error suggests:\n",
    ">\n",
    ">   *Expected to have a matching ToolMessage in Command.update for tool 'update_username', got: []. Every tool call (LLM requesting to call a tool) in the message history MUST have a corresponding ToolMessage. You can fix it by modifying the tool to return `Command(update=[ToolMessage(\"Success\", tool_call_id=tool_call_id), ...], ...)`*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be688d3a",
   "metadata": {},
   "source": [
    "### 3. Create the agent passing the custom `state_schema` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f84897c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=ChatOpenAI(model=\"gpt-4o\"),\n",
    "    tools=[update_username, get_internal_value],\n",
    "    state_schema=CustomState\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c6050f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your username has been updated to Mario.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "initial_state = {\n",
    "    \"messages\": [HumanMessage(content=\"Whats my username? Update it to Mario. What's the username now?\")],\n",
    "    \"username\": \"Matteo\",\n",
    "    \"remaining_steps\": 15\n",
    "}\n",
    "\n",
    "print(agent.invoke(initial_state)[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ebf364",
   "metadata": {},
   "source": [
    "## RAGV4 Example\n",
    "\n",
    "How about a practical application? \n",
    "\n",
    "Let's correctly build RAGV4 using `InjectedState`. We will only build the data analyst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c30b2f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup keys\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "def _set_if_undefined(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"Please provide your {var}\")\n",
    "\n",
    "\n",
    "_set_if_undefined(\"OPENAI_API_KEY\")\n",
    "_set_if_undefined(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5947e107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from langgraph.prebuilt import InjectedState\n",
    "from typing_extensions import Annotated\n",
    "from typing import Dict, Union\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "DATASET_FOLDER = \"./LLM_data\"\n",
    "\n",
    "# ----------------------\n",
    "# Define state schema\n",
    "# ----------------------\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "class DatasetState(MessagesState):\n",
    "    loaded: Dict[str, Union[pd.DataFrame, gpd.GeoDataFrame]]  # will store either pd.DataFrame or gpd.GeoDataFrame\n",
    "    descriptions: Dict[str, str]    # will store datasets descriptions\n",
    "    remaining_steps: int\n",
    "\n",
    "# ----------------------\n",
    "# Tool: list datasets\n",
    "# ----------------------\n",
    "@tool\n",
    "def list_loadable_datasets() -> str:\n",
    "    \"\"\"Lists all available parquet datasets in the dataset folder.\"\"\"\n",
    "    files = [f for f in os.listdir(DATASET_FOLDER) if f.endswith(\".parquet\")]\n",
    "    return \"\\n\".join(files) if files else \"No parquet datasets found.\"\n",
    "\n",
    "@tool\n",
    "def list_inmemory_datasets(state: Annotated[DatasetState, InjectedState]) -> str:\n",
    "    \"\"\"Lists all loaded datasets and their type (DataFrame or GeoDataFrame).\"\"\"\n",
    "    if not state[\"loaded\"]:\n",
    "        return \"No loaded datasets in memory. Use list_loadable_datasets() to see available files.\"\n",
    "    \n",
    "    lines = []\n",
    "    for name, df in state[\"loaded\"].items():\n",
    "        dtype = \"GeoDataFrame\" if isinstance(df, gpd.GeoDataFrame) else \"DataFrame\"\n",
    "        lines.append(f\"- {name}: {dtype} (shape={df.shape})\")\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# ----------------------\n",
    "# Tool: load dataset\n",
    "# ----------------------\n",
    "@tool\n",
    "def load_dataset_named(file_name: str, state: Annotated[DatasetState, InjectedState]) -> str:\n",
    "    \"\"\"\n",
    "    Loads a Parquet dataset and its description.\n",
    "    If geometry is present, loads as GeoDataFrame.\n",
    "    Populates state['loaded'][name] and state['descriptions'][name].\n",
    "    \"\"\"\n",
    "    file_stem = Path(file_name).stem\n",
    "    file_name = f\"{file_stem}.parquet\"\n",
    "    path = Path(DATASET_FOLDER) / file_name\n",
    "\n",
    "    if not path.exists():\n",
    "        available_files = os.listdir(DATASET_FOLDER)\n",
    "        return f\"File '{file_name}' not found. Available files: {available_files}\"\n",
    "\n",
    "    # Load DataFrame\n",
    "    try:\n",
    "        df = pd.read_parquet(path)\n",
    "        if \"geometry\" in df.columns:\n",
    "            try:\n",
    "                df = gpd.read_parquet(path)\n",
    "            except Exception as geo_err:\n",
    "                return f\"Geometry column found but failed to load as GeoDataFrame: {geo_err}\"\n",
    "        state[\"loaded\"][file_stem] = df\n",
    "    except Exception as e:\n",
    "        return f\"Error loading dataset '{file_name}': {e}\"\n",
    "\n",
    "    # Load description\n",
    "    desc_path = Path(DATASET_FOLDER) / f\"{file_stem}.txt\"\n",
    "    try:\n",
    "        with open(desc_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_desc = f.read()\n",
    "    except Exception as e:\n",
    "        raw_desc = \"Description file missing or unreadable.\"\n",
    "\n",
    "    # Enrich and save\n",
    "    dtype_str = type(df).__name__   # DataFrame or GeoDataFrame\n",
    "    head_str = df.head().to_string(index=False)\n",
    "    cols_str = \", \".join(df.columns)\n",
    "    enriched_desc = f\"{dtype_str}\\n{raw_desc}\\n\\n---\\nPreview (first rows):\\n{head_str}\\n\\nColumns: {cols_str}\"\n",
    "\n",
    "    if \"descriptions\" not in state:\n",
    "        state[\"descriptions\"] = {}\n",
    "    state[\"descriptions\"][file_stem] = enriched_desc\n",
    "\n",
    "    return (\n",
    "        f\"Dataset '{file_name}' loaded into state['loaded']['{file_stem}']\\n\"\n",
    "        f\"Description enriched and stored in state['descriptions']['{file_stem}']\"\n",
    "    )\n",
    "    \n",
    "\n",
    "# ----------------------\n",
    "# Tool: python repl\n",
    "# ----------------------\n",
    "repl = PythonREPL()\n",
    "# Now use the tool with your injected REPL\n",
    "@tool\n",
    "def python_repl_tool(\n",
    "    code: Annotated[str, \"The python code to execute\"], state: Annotated[DatasetState, InjectedState]\n",
    "):\n",
    "    \"\"\"\n",
    "    Execute arbitrary Python code with all loaded datasets and descriptions available as dictionaries.\n",
    "    For example, if 'quartieri' is loaded, you can write: loaded[\"quartieri\"].head(), and get\n",
    "    the description with descriptions[\"quartieri]\n",
    "    \"\"\"\n",
    "\n",
    "    repl.globals.update({   # inject custom globals into the tools\n",
    "        \"loaded\" : state['loaded'],\n",
    "        \"descriptions\" : state['descriptions']\n",
    "    })\n",
    "    try:\n",
    "        result = repl.run(code)\n",
    "    except BaseException as e:\n",
    "        return f\"Failed to execute. Error: {repr(e)}\"\n",
    "    return f\"Successfully executed:\\n```python\\n{code}\\n```\\nStdout: {result}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54f66e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\"You are a data analyst. Use your tools to explore and load datasets relevant to the task.\\n\"\n",
    "        \"The files you need to load are in the subdirectory at ../../experiments/LLM_data/\\n\\n\"\n",
    "        \"When writing python code with your tools, you will be able to access the loaded datasets and their descriptions\\\n",
    "        by using loaded['name'] for dataframes and descriptions['name] for descriptions.\\n\"\n",
    "        \"Always read the description before working with a dataset.\\n\"\n",
    "        \"You can also check the datasets loaded in memory with your list_inmemory_datasets() tool, \\\n",
    "        and check available datasets to load with the list_loadable_datasets() tool.\"\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22a866da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "analyst_agent = create_react_agent(\n",
    "    model=\"anthropic:claude-sonnet-4-0\",\n",
    "    tools=[list_loadable_datasets, list_inmemory_datasets, load_dataset_named, python_repl_tool],\n",
    "    prompt=prompt,\n",
    "    name=\"data_analyst\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e5e0cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "\n",
    "def pretty_print_message(message, indent=False):\n",
    "    pretty_message = message.pretty_repr(html=True)\n",
    "    if not indent:\n",
    "        print(pretty_message)\n",
    "        return\n",
    "\n",
    "    indented = \"\\n\".join(\"\\t\" + c for c in pretty_message.split(\"\\n\"))\n",
    "    print(indented)\n",
    "\n",
    "\n",
    "def pretty_print_messages(update, last_message=False):\n",
    "    is_subgraph = False\n",
    "    if isinstance(update, tuple):\n",
    "        ns, update = update\n",
    "        # skip parent graph updates in the printouts\n",
    "        if len(ns) == 0:\n",
    "            return\n",
    "\n",
    "        graph_id = ns[-1].split(\":\")[0]\n",
    "        print(f\"Update from subgraph {graph_id}:\")\n",
    "        print(\"\\n\")\n",
    "        is_subgraph = True\n",
    "\n",
    "    for node_name, node_update in update.items():\n",
    "        update_label = f\"Update from node {node_name}:\"\n",
    "        if is_subgraph:\n",
    "            update_label = \"\\t\" + update_label\n",
    "\n",
    "        print(update_label)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        messages = convert_to_messages(node_update[\"messages\"])\n",
    "        if last_message:\n",
    "            messages = messages[-1:]\n",
    "\n",
    "        for m in messages:\n",
    "            pretty_print_message(m, indent=is_subgraph)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7a8f8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node agent:\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: data_analyst\n",
      "\n",
      "[{'text': \"I'll help you find the number of residents in Murri. Let me start by exploring the available datasets to find relevant data.\", 'type': 'text'}, {'id': 'toolu_01XSgGW57WZRjr2FTnHQ6RRW', 'input': {}, 'name': 'list_loadable_datasets', 'type': 'tool_use'}]\n",
      "Tool Calls:\n",
      "  list_loadable_datasets (toolu_01XSgGW57WZRjr2FTnHQ6RRW)\n",
      " Call ID: toolu_01XSgGW57WZRjr2FTnHQ6RRW\n",
      "  Args:\n",
      "\n",
      "\n",
      "Update from node tools:\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: list_loadable_datasets\n",
      "\n",
      "Error: FileNotFoundError(2, 'No such file or directory')\n",
      " Please fix your mistakes.\n",
      "\n",
      "\n",
      "Update from node agent:\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: data_analyst\n",
      "\n",
      "[{'text': 'Let me check what datasets are currently loaded in memory:', 'type': 'text'}, {'id': 'toolu_01AgzPa4NoPpR5bstLTribXu', 'input': {}, 'name': 'list_inmemory_datasets', 'type': 'tool_use'}]\n",
      "Tool Calls:\n",
      "  list_inmemory_datasets (toolu_01AgzPa4NoPpR5bstLTribXu)\n",
      " Call ID: toolu_01AgzPa4NoPpR5bstLTribXu\n",
      "  Args:\n",
      "\n",
      "\n",
      "Update from node tools:\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: list_inmemory_datasets\n",
      "\n",
      "Error: 2 validation errors for list_inmemory_datasets\n",
      "state.loaded\n",
      "  Field required [type=missing, input_value={'messages': [HumanMessag..., 'remaining_steps': 22}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "state.descriptions\n",
      "  Field required [type=missing, input_value={'messages': [HumanMessag..., 'remaining_steps': 22}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      " Please fix your mistakes.\n",
      "\n",
      "\n",
      "Update from node agent:\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: data_analyst\n",
      "\n",
      "[{'text': 'Let me try to load some datasets that might contain information about residents in Murri. Let me start by exploring what datasets are available:', 'type': 'text'}, {'id': 'toolu_01NLEataWzGDXibsQMTqYUPj', 'input': {'code': 'import os\\nimport pandas as pd\\n\\n# Check what files are available in the data directory\\ndata_dir = \"../../experiments/LLM_data/\"\\nif os.path.exists(data_dir):\\n    files = os.listdir(data_dir)\\n    print(\"Available files:\")\\n    for file in files:\\n        print(f\"- {file}\")\\nelse:\\n    print(f\"Directory {data_dir} does not exist\")'}, 'name': 'python_repl_tool', 'type': 'tool_use'}]\n",
      "Tool Calls:\n",
      "  python_repl_tool (toolu_01NLEataWzGDXibsQMTqYUPj)\n",
      " Call ID: toolu_01NLEataWzGDXibsQMTqYUPj\n",
      "  Args:\n",
      "    code: import os\n",
      "import pandas as pd\n",
      "\n",
      "# Check what files are available in the data directory\n",
      "data_dir = \"../../experiments/LLM_data/\"\n",
      "if os.path.exists(data_dir):\n",
      "    files = os.listdir(data_dir)\n",
      "    print(\"Available files:\")\n",
      "    for file in files:\n",
      "        print(f\"- {file}\")\n",
      "else:\n",
      "    print(f\"Directory {data_dir} does not exist\")\n",
      "\n",
      "\n",
      "Update from node tools:\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: python_repl_tool\n",
      "\n",
      "Error: 2 validation errors for python_repl_tool\n",
      "state.loaded\n",
      "  Field required [type=missing, input_value={'messages': [HumanMessag..., 'remaining_steps': 20}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "state.descriptions\n",
      "  Field required [type=missing, input_value={'messages': [HumanMessag..., 'remaining_steps': 20}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      " Please fix your mistakes.\n",
      "\n",
      "\n",
      "Update from node agent:\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: data_analyst\n",
      "\n",
      "[{'text': 'It seems I need to first load a dataset. Let me try loading a dataset that might contain information about residents. Let me try some common dataset names:', 'type': 'text'}, {'id': 'toolu_01WDajhAvq81oNVMeWLA6mcw', 'input': {'file_name': 'quartieri'}, 'name': 'load_dataset_named', 'type': 'tool_use'}]\n",
      "Tool Calls:\n",
      "  load_dataset_named (toolu_01WDajhAvq81oNVMeWLA6mcw)\n",
      " Call ID: toolu_01WDajhAvq81oNVMeWLA6mcw\n",
      "  Args:\n",
      "    file_name: quartieri\n",
      "\n",
      "\n",
      "Update from node tools:\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: load_dataset_named\n",
      "\n",
      "Error: 2 validation errors for load_dataset_named\n",
      "state.loaded\n",
      "  Field required [type=missing, input_value={'messages': [HumanMessag..., 'remaining_steps': 18}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "state.descriptions\n",
      "  Field required [type=missing, input_value={'messages': [HumanMessag..., 'remaining_steps': 18}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      " Please fix your mistakes.\n",
      "\n",
      "\n",
      "Update from node agent:\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: data_analyst\n",
      "\n",
      "[{'text': \"I'm encountering some initialization issues with the state. Let me try a different approach and check what's available in the data directory:\", 'type': 'text'}, {'id': 'toolu_0194dYjSVCWVyCRpQ94jbtkF', 'input': {'code': '# Initialize empty dictionaries if they don\\'t exist\\nif \\'loaded\\' not in globals():\\n    loaded = {}\\nif \\'descriptions\\' not in globals():\\n    descriptions = {}\\n\\nimport os\\nimport pandas as pd\\n\\n# Check what files are available in the data directory\\ndata_dir = \"../../experiments/LLM_data/\"\\nif os.path.exists(data_dir):\\n    files = os.listdir(data_dir)\\n    print(\"Available files:\")\\n    for file in files:\\n        print(f\"- {file}\")\\nelse:\\n    print(f\"Directory {data_dir} does not exist\")'}, 'name': 'python_repl_tool', 'type': 'tool_use'}]\n",
      "Tool Calls:\n",
      "  python_repl_tool (toolu_0194dYjSVCWVyCRpQ94jbtkF)\n",
      " Call ID: toolu_0194dYjSVCWVyCRpQ94jbtkF\n",
      "  Args:\n",
      "    code: # Initialize empty dictionaries if they don't exist\n",
      "if 'loaded' not in globals():\n",
      "    loaded = {}\n",
      "if 'descriptions' not in globals():\n",
      "    descriptions = {}\n",
      "\n",
      "import os\n",
      "import pandas as pd\n",
      "\n",
      "# Check what files are available in the data directory\n",
      "data_dir = \"../../experiments/LLM_data/\"\n",
      "if os.path.exists(data_dir):\n",
      "    files = os.listdir(data_dir)\n",
      "    print(\"Available files:\")\n",
      "    for file in files:\n",
      "        print(f\"- {file}\")\n",
      "else:\n",
      "    print(f\"Directory {data_dir} does not exist\")\n",
      "\n",
      "\n",
      "Update from node tools:\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: python_repl_tool\n",
      "\n",
      "Error: 2 validation errors for python_repl_tool\n",
      "state.loaded\n",
      "  Field required [type=missing, input_value={'messages': [HumanMessag..., 'remaining_steps': 16}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "state.descriptions\n",
      "  Field required [type=missing, input_value={'messages': [HumanMessag..., 'remaining_steps': 16}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      " Please fix your mistakes.\n",
      "\n",
      "\n",
      "Update from node agent:\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: data_analyst\n",
      "\n",
      "I apologize for the technical issues. It seems there's a problem with the system initialization. Let me try to get around this by using the list_loadable_datasets function in a different way. Could you please help me by providing more context about what datasets might be available, or if you know of any specific dataset names that contain information about residents in Murri?\n",
      "\n",
      "Alternatively, if you have access to the data directory, could you let me know what parquet files are available there? This would help me load the appropriate dataset to find the number of residents in Murri.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in analyst_agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the number of residents in Murri?\"}]}\n",
    "):\n",
    "    pretty_print_messages(chunk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
