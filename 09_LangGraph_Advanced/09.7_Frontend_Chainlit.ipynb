{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2267b53",
   "metadata": {},
   "source": [
    "# Building Frontend with Chainlit\n",
    "\n",
    "[Chainlit](https://docs.chainlit.io/get-started/overview) is an open-source Python package to build production ready Conversational AI. \n",
    "\n",
    "It suits our needs since it has a simple integration with LangChain and LangGraph. It will allow us to build a frontend very quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440cdaeb",
   "metadata": {},
   "source": [
    "## Chainlit tutorial\n",
    "\n",
    "Let's follow together the tutorial from [Chainlit's LangChain integration](https://docs.chainlit.io/integrations/langchain#prerequisites):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a4642f",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "* A working installation of Chainlit (requires `python >= 3.9`)\n",
    "\n",
    "* LangGraph installed (and all needed packages for your specific applications like `langchain_openai` and so on)\n",
    "\n",
    "* An OpenAI API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50301111",
   "metadata": {},
   "source": [
    "### 1) Create a Python file\n",
    "\n",
    "Create a new Python file named `app.py` in your project directory. This file will contain the main logic for your LLM application.\n",
    "\n",
    "We will create it inside `chainlit_apps`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d441489",
   "metadata": {},
   "source": [
    "### 2) Write the Application Logic\n",
    "\n",
    "In `app.py`, import the necessary packages and define one function to handle a new chat session and another function to handle messages incoming from the UI.\n",
    "\n",
    "With LangGraph we can do it in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf64d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ inside app.py ------------------------\n",
    "\n",
    "from typing import Literal\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain.schema.runnable.config import RunnableConfig\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "import chainlit as cl\n",
    "\n",
    "# setup keys\n",
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def _set_if_undefined(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"Please provide your {var}\")\n",
    "\n",
    "_set_if_undefined(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "# example tool\n",
    "@tool\n",
    "def get_weather(city: Literal[\"nyc\", \"sf\"]):\n",
    "    \"\"\"Use this to get weather information.\"\"\"\n",
    "    if city == \"nyc\":\n",
    "        return \"It might be cloudy in nyc\"\n",
    "    elif city == \"sf\":\n",
    "        return \"It's always sunny in sf\"\n",
    "    else:\n",
    "        raise AssertionError(\"Unknown city\")\n",
    "\n",
    "\n",
    "tools = [get_weather]\n",
    "model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "final_model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "model = model.bind_tools(tools)\n",
    "# NOTE: this is where we're adding a tag that we'll can use later to filter the model stream events to only the model called in the final node.\n",
    "# This is not necessary if you call a single LLM but might be important in case you call multiple models within the node and want to filter events\n",
    "# from only one of them.\n",
    "final_model = final_model.with_config(tags=[\"final_node\"])\n",
    "tool_node = ToolNode(tools=tools)\n",
    "\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.graph.message import MessagesState\n",
    "from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage\n",
    "\n",
    "\n",
    "def should_continue(state: MessagesState) -> Literal[\"tools\", \"final\"]:\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM makes a tool call, then we route to the \"tools\" node\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    # Otherwise, we stop (reply to the user)\n",
    "    return \"final\"\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    messages = state[\"messages\"]\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def call_final_model(state: MessagesState):\n",
    "    messages = state[\"messages\"]\n",
    "    last_ai_message = messages[-1]\n",
    "    response = final_model.invoke(\n",
    "        [\n",
    "            SystemMessage(\"Rewrite this in the voice of Al Roker\"),\n",
    "            HumanMessage(last_ai_message.content),\n",
    "        ]\n",
    "    )\n",
    "    # overwrite the last AI message from the agent\n",
    "    response.id = last_ai_message.id\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "builder.add_node(\"agent\", call_model)\n",
    "builder.add_node(\"tools\", tool_node)\n",
    "# add a separate final node\n",
    "builder.add_node(\"final\", call_final_model)\n",
    "\n",
    "builder.add_edge(START, \"agent\")\n",
    "builder.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    ")\n",
    "\n",
    "builder.add_edge(\"tools\", \"agent\")\n",
    "builder.add_edge(\"final\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "@cl.on_message\n",
    "async def on_message(msg: cl.Message):\n",
    "    config = {\"configurable\": {\"thread_id\": cl.context.session.id}}\n",
    "    cb = cl.LangchainCallbackHandler()\n",
    "    final_answer = cl.Message(content=\"\")\n",
    "    \n",
    "    for msg, metadata in graph.stream({\"messages\": [HumanMessage(content=msg.content)]}, stream_mode=\"messages\", config=RunnableConfig(callbacks=[cb], **config)):\n",
    "        if (\n",
    "            msg.content\n",
    "            and not isinstance(msg, HumanMessage)\n",
    "            and metadata[\"langgraph_node\"] == \"final\"\n",
    "        ):\n",
    "            await final_answer.stream_token(msg.content)\n",
    "\n",
    "    await final_answer.send()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d592d86",
   "metadata": {},
   "source": [
    "### 3) Run the Application\n",
    "\n",
    "To start your app, open a terminal and navigate to the directory containing `app.py`. Then run the following command:\n",
    "\n",
    "```bash\n",
    "chainlit run app.py -w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee697ba9",
   "metadata": {},
   "source": [
    "The `-w` flag tells Chainlit to enable auto-reloading, so you donâ€™t need to restart the server every time you make changes to your application. Your chatbot UI should now be accessible at http://localhost:8000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c524dc",
   "metadata": {},
   "source": [
    "and... it works. I have some problems with LangSmith tracing (as always) but it works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94209379",
   "metadata": {},
   "source": [
    "## Basic Chainlit frontend for custom graph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
