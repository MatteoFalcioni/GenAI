{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2267b53",
   "metadata": {},
   "source": [
    "# Building Frontend with Chainlit\n",
    "\n",
    "[Chainlit](https://docs.chainlit.io/get-started/overview) is an open-source Python package to build production ready Conversational AI. \n",
    "\n",
    "It suits our needs since it has a simple integration with LangChain and LangGraph. It will allow us to build a frontend very quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440cdaeb",
   "metadata": {},
   "source": [
    "## Chainlit tutorial\n",
    "\n",
    "Let's follow together the tutorial from [Chainlit's LangChain integration](https://docs.chainlit.io/integrations/langchain#prerequisites):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a4642f",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "* A working installation of Chainlit (requires `python >= 3.9`)\n",
    "\n",
    "* LangGraph installed (and all needed packages for your specific applications like `langchain_openai` and so on)\n",
    "\n",
    "* An OpenAI API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50301111",
   "metadata": {},
   "source": [
    "### 1) Create a Python file\n",
    "\n",
    "Create a new Python file named `app.py` in your project directory. This file will contain the main logic for your LLM application.\n",
    "\n",
    "We will create it inside `chainlit_tutorial/base_tutorial`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d441489",
   "metadata": {},
   "source": [
    "### 2) Write the Application Logic\n",
    "\n",
    "In `app.py`, import the necessary packages and define one function to handle a new chat session and another function to handle messages incoming from the UI.\n",
    "\n",
    "With LangGraph we can do it in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf64d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ inside app.py ------------------------\n",
    "\n",
    "from typing import Literal\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain.schema.runnable.config import RunnableConfig\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "import chainlit as cl\n",
    "\n",
    "# setup keys\n",
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def _set_if_undefined(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"Please provide your {var}\")\n",
    "\n",
    "\n",
    "_set_if_undefined(\"OPENAI_API_KEY\")\n",
    "\n",
    "# example tool\n",
    "@tool\n",
    "def get_weather(city: Literal[\"nyc\", \"sf\"]):\n",
    "    \"\"\"Use this to get weather information.\"\"\"\n",
    "    if city == \"nyc\":\n",
    "        return \"It might be cloudy in nyc\"\n",
    "    elif city == \"sf\":\n",
    "        return \"It's always sunny in sf\"\n",
    "    else:\n",
    "        raise AssertionError(\"Unknown city\")\n",
    "\n",
    "\n",
    "tools = [get_weather]\n",
    "model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "final_model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "model = model.bind_tools(tools)\n",
    "# NOTE: this is where we're adding a tag that we'll can use later to filter the model stream events to only the model called in the final node.\n",
    "# This is not necessary if you call a single LLM but might be important in case you call multiple models within the node and want to filter events\n",
    "# from only one of them.\n",
    "final_model = final_model.with_config(tags=[\"final_node\"])\n",
    "tool_node = ToolNode(tools=tools)\n",
    "\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.graph.message import MessagesState\n",
    "from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage\n",
    "\n",
    "\n",
    "def should_continue(state: MessagesState) -> Literal[\"tools\", \"final\"]:\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM makes a tool call, then we route to the \"tools\" node\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    # Otherwise, we stop (reply to the user)\n",
    "    return \"final\"\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    messages = state[\"messages\"]\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def call_final_model(state: MessagesState):\n",
    "    messages = state[\"messages\"]\n",
    "    last_ai_message = messages[-1]\n",
    "    response = final_model.invoke(\n",
    "        [\n",
    "            SystemMessage(\"Rewrite this in the voice of Al Roker\"),\n",
    "            HumanMessage(last_ai_message.content),\n",
    "        ]\n",
    "    )\n",
    "    # overwrite the last AI message from the agent\n",
    "    response.id = last_ai_message.id\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "builder.add_node(\"agent\", call_model)\n",
    "builder.add_node(\"tools\", tool_node)\n",
    "# add a separate final node\n",
    "builder.add_node(\"final\", call_final_model)\n",
    "\n",
    "builder.add_edge(START, \"agent\")\n",
    "builder.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    ")\n",
    "\n",
    "builder.add_edge(\"tools\", \"agent\")\n",
    "builder.add_edge(\"final\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "@cl.on_message\n",
    "async def on_message(msg: cl.Message):\n",
    "    config = {\"configurable\": {\"thread_id\": cl.context.session.id}}\n",
    "    cb = cl.LangchainCallbackHandler()\n",
    "    final_answer = cl.Message(content=\"\")\n",
    "    \n",
    "    for msg, metadata in graph.stream({\"messages\": [HumanMessage(content=msg.content)]}, stream_mode=\"messages\", config=RunnableConfig(callbacks=[cb], **config)):\n",
    "        if (\n",
    "            msg.content\n",
    "            and not isinstance(msg, HumanMessage)\n",
    "            and metadata[\"langgraph_node\"] == \"final\"\n",
    "        ):\n",
    "            await final_answer.stream_token(msg.content)\n",
    "\n",
    "    await final_answer.send()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d592d86",
   "metadata": {},
   "source": [
    "### 3) Run the Application\n",
    "\n",
    "To start your app, open a terminal and navigate to the directory containing `app.py`. Then run the following command:\n",
    "\n",
    "```bash\n",
    "chainlit run app.py -w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee697ba9",
   "metadata": {},
   "source": [
    "The `-w` flag tells Chainlit to enable auto-reloading, so you donâ€™t need to restart the server every time you make changes to your application. Your chatbot UI should now be accessible at http://localhost:8000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c524dc",
   "metadata": {},
   "source": [
    "and... it works. I have some problems with LangSmith tracing (as always) but it works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94209379",
   "metadata": {},
   "source": [
    "## Basic Chainlit frontend for custom graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c310eba",
   "metadata": {},
   "source": [
    "Now of course I want to try to make it work with my custom graph.\n",
    "\n",
    "For the first example I will only use a supervisor and the data analyst I have built for my agentic system. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcfc13f",
   "metadata": {},
   "source": [
    "### The Agent definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aa417a",
   "metadata": {},
   "source": [
    "We will define our analyst in an `analyst.py` file in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7bbbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing_extensions import Annotated\n",
    "from typing import Union, Dict\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langgraph.types import Command\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "\n",
    "DATASET_FOLDER = \"./LLM_data\"\n",
    "\n",
    "\n",
    "def merge_dictionary_entries(existing_dict: Union[dict, None] = None, new_dict: Union[dict, None] = None) -> dict:\n",
    "    \"\"\"\n",
    "    Custom reducer to merge dictionary updates:\n",
    "    adds keys from new_dict only if they are not already in existing_dict.\n",
    "    \"\"\"\n",
    "    if not existing_dict:\n",
    "        existing_dict = {}\n",
    "    if not new_dict:\n",
    "        new_dict = {}\n",
    "\n",
    "    for key, data in new_dict.items():\n",
    "        if key not in existing_dict:\n",
    "            existing_dict[key] = data\n",
    "    \n",
    "    return existing_dict\n",
    "\n",
    "# custom state schema \n",
    "from langgraph.managed.is_last_step import RemainingSteps\n",
    "\n",
    "class AgentState(MessagesState):\n",
    "    loaded: Annotated[Dict[str, Union[pd.DataFrame, gpd.GeoDataFrame]], merge_dictionary_entries]\n",
    "    remaining_steps: RemainingSteps     # key to let LangGraph automatically manage graph's supersteps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42317606",
   "metadata": {},
   "source": [
    "#### tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7575f473",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_core.tools import tool, InjectedToolCallId\n",
    "from langgraph.prebuilt import InjectedState\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Tool: list datasets\n",
    "# ----------------------\n",
    "@tool\n",
    "def list_loadable_datasets() -> str:\n",
    "    \"\"\"Lists all available parquet datasets in the dataset folder.\"\"\"\n",
    "    files = [f for f in os.listdir(DATASET_FOLDER) if f.endswith(\".parquet\")]\n",
    "    return \"\\n\".join(files) if files else \"No parquet datasets found.\"\n",
    "\n",
    "@tool\n",
    "def list_inmemory_datasets(\n",
    "    state: Annotated[AgentState, InjectedState],\n",
    "    tool_call_id: Annotated[str, InjectedToolCallId]\n",
    ") -> Command:\n",
    "    \"\"\"Lists all loaded datasets and their type (DataFrame or GeoDataFrame).\"\"\"\n",
    "    if not state[\"loaded\"]:\n",
    "        output = \"No loaded datasets in memory. Use list_loadable_datasets() to see available files.\"\n",
    "    \n",
    "    else:\n",
    "        lines = [\n",
    "            f\"- {name}: {'GeoDataFrame' if isinstance(df, gpd.GeoDataFrame) else 'DataFrame'} (shape={df.shape})\"\n",
    "            for name, df in state[\"loaded\"].items()\n",
    "        ]\n",
    "        output = \"\\n\".join(lines)\n",
    "\n",
    "    return Command(update={\n",
    "        \"messages\": [ToolMessage(content=output, tool_call_id=tool_call_id)],\n",
    "    })\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Tool: python repl\n",
    "# ----------------------\n",
    "repl = PythonREPL()\n",
    "@tool\n",
    "def python_repl_tool(\n",
    "    code: Annotated[str, \"The python code to execute\"], \n",
    "    state: Annotated[AgentState, InjectedState], \n",
    "    tool_call_id: Annotated[str, InjectedToolCallId]\n",
    ") -> Command:\n",
    "    \"\"\"\n",
    "    Use this to execute python code. If you want to see the output of a value,\n",
    "    print it out with `print(...)`. This is visible to the user. \n",
    "    \"\"\"\n",
    "\n",
    "    for name, df in state[\"loaded\"].items():\n",
    "        repl.globals[name] = df\n",
    "    \n",
    "    try:\n",
    "        result = repl.run(code)\n",
    "    except BaseException as e:\n",
    "        tool_err_1 = f\"Failed to execute. Error: {repr(e)}\"\n",
    "        return Command(update={\"messages\": [ToolMessage(content=tool_err_1, tool_call_id=tool_call_id)]})\n",
    "    \n",
    "    tool_output = f\"Successfully executed:\\n```python\\n{code}\\n```\\nStdout: {result}\"\n",
    "    return Command(update={\"messages\": [ToolMessage(content=tool_output, tool_call_id=tool_call_id)]})\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Tool: load datasets\n",
    "# ----------------------\n",
    "@tool\n",
    "def load_dataset(file_name: str, \n",
    "                 tool_call_id: Annotated[str, InjectedToolCallId]\n",
    ") -> Command:\n",
    "    \"\"\"\n",
    "    Loads a Parquet dataset (optionally as GeoDataFrame) and updates state['loaded'][name].\n",
    "    \"\"\"\n",
    "    update = {}\n",
    "\n",
    "    file_stem = Path(file_name).stem\n",
    "    file_name = f\"{file_stem}.parquet\"\n",
    "    path = Path(DATASET_FOLDER) / file_name\n",
    "\n",
    "    if not path.exists():\n",
    "        available_files = os.listdir(DATASET_FOLDER)\n",
    "        tool_err_result1 = f\"File '{file_name}' not found. Available files: {available_files}\"\n",
    "        return Command(update={\"messages\": [ToolMessage(tool_err_result1, tool_call_id=tool_call_id)]})\n",
    "\n",
    "\n",
    "    try:\n",
    "        df = pd.read_parquet(path)\n",
    "        if \"geometry\" in df.columns:\n",
    "            try:\n",
    "                df = gpd.read_parquet(path)\n",
    "            except Exception as geo_err:\n",
    "                tool_err_result2 = f\"Geometry column found but failed to load as GeoDataFrame: {geo_err}\"\n",
    "                return Command(update={\"messages\": [ToolMessage(tool_err_result2, tool_call_id=tool_call_id)]})\n",
    "            \n",
    "        update[file_stem] = df\n",
    "\n",
    "    except Exception as e:\n",
    "        tool_err_result3 = f\"Error loading dataset '{file_name}': {e}\"\n",
    "        return Command(update={\"messages\": [ToolMessage(tool_err_result3, tool_call_id=tool_call_id)]})\n",
    "\n",
    "\n",
    "    return Command(update={\n",
    "        \"loaded\": update,   \n",
    "        \"messages\": [\n",
    "            ToolMessage(f\"Loaded dataset '{file_stem}' into memory.\", tool_call_id=tool_call_id)\n",
    "        ]\n",
    "    })\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Tool: describe_dataset\n",
    "# ----------------------\n",
    "@tool\n",
    "def describe_dataset(name: str, \n",
    "                     state: Annotated[AgentState, InjectedState], \n",
    "                     tool_call_id: Annotated[str, InjectedToolCallId]\n",
    "                     ) -> Command:\n",
    "    \"\"\"\n",
    "    Generates a detailed description for a loaded dataset.\n",
    "\n",
    "    This function returns a summary including:\n",
    "    - the dataset type (DataFrame or GeoDataFrame),\n",
    "    - a preview of the first few rows,\n",
    "    - and the list of column names.\n",
    "    \"\"\"\n",
    "\n",
    "    loaded = state.get('loaded')\n",
    "\n",
    "    df = loaded.get(name)\n",
    "    if df is None:\n",
    "        loaded_keys = list(loaded.keys())\n",
    "        available_data = [f for f in os.listdir(DATASET_FOLDER) if f.endswith(\".parquet\")]\n",
    "        tool_err = f\"Dataset '{name}' not found. \\nLoaded datasets are: {loaded_keys} \\nAvailable datasets to load are {available_data}\"\n",
    "        return Command(update={\"messages\": [ToolMessage(tool_err, tool_call_id=tool_call_id)]})\n",
    "\n",
    "    dtype_str = type(df).__name__   # DataFrame or GeoDataFrame\n",
    "    shape_info = f\"{df.shape[0]} rows x {df.shape[1]} columns\" # dataset shape\n",
    "    # Show only the first N columns and limit long values\n",
    "    MAX_COLS = 5\n",
    "    MAX_ROWS = 5\n",
    "\n",
    "    try:\n",
    "        # Limit columns\n",
    "        preview_df = df.iloc[:MAX_ROWS, :MAX_COLS].copy()\n",
    "        # Truncate long values\n",
    "        for col in preview_df.columns:\n",
    "            preview_df[col] = preview_df[col].astype(str).str.slice(0, 40)\n",
    "\n",
    "        head_str = preview_df.to_string(index=False)\n",
    "    except Exception as e:\n",
    "        tool_err = f\"[Could not generate preview: {str(e)}]\"\n",
    "        return Command(update={\"messages\": [ToolMessage(content=tool_err, tool_call_id=tool_call_id)]})\n",
    "\n",
    "    # show column types too\n",
    "    cols_str = \"\\n\".join([f\"- {col} ({df[col].dtype})\" for col in df.columns[:MAX_COLS]])\n",
    "    if df.shape[1] > MAX_COLS:\n",
    "        cols_str += f\"\\n...and {df.shape[1] - MAX_COLS} more columns\"\n",
    "\n",
    "    geometry_info = \"\"\n",
    "    if dtype_str.lower() == \"geodataframe\":\n",
    "        geometry_col = df.geometry.name if df.geometry.name in df.columns else None\n",
    "        if geometry_col:\n",
    "            geometry_info = f\"\\nActive geometry column: '{geometry_col}'\"\n",
    "        else:\n",
    "            geometry_info = \"\\n No active geometry column set!\"\n",
    "\n",
    "\n",
    "    tool_output = (\n",
    "        f\"{dtype_str} | {shape_info}\\n\"\n",
    "        f\"{geometry_info}\\n\"\n",
    "        f\"---\\n\"\n",
    "        f\"Preview (first {MAX_ROWS} rows, {MAX_COLS} columns):\\n{head_str}\\n\\n\"\n",
    "        f\"Columns:\\n{cols_str}\"\n",
    "    )\n",
    "\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\" : [ToolMessage(content=tool_output, tool_call_id=tool_call_id)],\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Tool: fuzzy match name\n",
    "# ----------------------\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "@tool\n",
    "def fuzzy_match_name(dataset_name : str, \n",
    "                     dataset_column : str, \n",
    "                     input_str : str, \n",
    "                     state : Annotated[AgentState, InjectedState],\n",
    "                     tool_call_id: Annotated[str, InjectedToolCallId],\n",
    "                     threshold : int = 65,\n",
    ") -> Command:\n",
    "    \"\"\"\n",
    "    Performs fuzzy matching to find the best match for the input string\n",
    "    within a specified column of a dataset.\n",
    "\n",
    "    Returns the best matching string and score if above the threshold,\n",
    "    otherwise a message indicating no match.\n",
    "\n",
    "    The best match can then be used to extract data from that entry in the dataframe. \n",
    "\n",
    "    Example:\n",
    "    - Input: (\"points_of_interest\", \"name\", \"torre del orologio\")\n",
    "    - Output: \"Torre dell'Orologio | score: 92\"\n",
    "    \"\"\"\n",
    "    loaded = state.get('loaded')\n",
    "\n",
    "    try:\n",
    "        known_names = (\n",
    "            loaded[dataset_name][dataset_column]\n",
    "            .dropna()\n",
    "            .astype(str)\n",
    "            .str.strip()\n",
    "            .unique()\n",
    "        )\n",
    "    except KeyError:\n",
    "        tool_err = f\"Dataset '{dataset_name}' or column '{dataset_column}' not found.\"\n",
    "        return Command(update={\"messages\": [ToolMessage(tool_err, tool_call_id=tool_call_id)]})\n",
    "\n",
    "    match_result = process.extractOne(\n",
    "        input_str,\n",
    "        known_names,\n",
    "        scorer=fuzz.token_sort_ratio\n",
    "    )\n",
    "\n",
    "    if match_result is None:\n",
    "        tool_err2 = f\"No match candidates available in {dataset_name}.{dataset_column}.\"\n",
    "        Command(update={\"messages\": [ToolMessage(tool_err2, tool_call_id=tool_call_id)]})\n",
    "\n",
    "    match, score, _ = match_result\n",
    "    if score >= threshold:\n",
    "        tool_output = f\"{match} | score: {score}\"\n",
    "    else:\n",
    "        tool_output = f\"No match found for '{input_str}' in '{dataset_name}.{dataset_column}' (best score: {score})\"\n",
    "\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [ToolMessage(content=tool_output, tool_call_id=tool_call_id)]\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef49cd3",
   "metadata": {},
   "source": [
    "#### create data analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8756f831",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst_suffix = (\n",
    "    \"You are a data analyst. Use your tools to explore and load datasets relevant to the task, analize them and then produce a visualization if requested.\\n\"\n",
    "    \"The files you need to load are in the subdirectory at ./LLM_data as .parquet files\\n\"\n",
    "    \"You can check which datasets are currently loaded with the `list_inmemory_datasets` tool, \\\n",
    "    and which datasets are available to load using the `list_loadable_datasets` tool.\\n\"\n",
    "    \"You can describe datasets with the `describe_dataset` tool.\\n\"\n",
    "    \"You can write custom python code with your `python_repl_tool`\\n\"\n",
    "    \"When asked to analize a law, use your `analize_law` tool. Laws are stored as graph state, so don't try to get them from datasets. Use your `analize_law` tool.\\n\\n\"\n",
    "    \"**VERY IMPORTANT** : **When printing Python code, ALWAYS use `print(...)`**. Do NOT rely on implicit output like `quartieri.head()`. ALWAYS USE `print(...)`\\n\"\n",
    "    \"In your `python_repl_tool`, loaded datasets will appear as variables (e.g., if you load quartieri.parquet, the dataset will be accessible as `quartieri`)\\n\\n\"\n",
    "    \"All spatial datasets use a geometry column (GeoDataFrame) containing shapely Point or Polygon objects.\\n\"\n",
    "    \"Always use the 'geometry' field when doing spatial operations, and avoid computing or reconstructing from latitude/longitude.\\n\"\n",
    "    \"When doing spatial queries (e.g., selecting features within 1 km), ensure you are working in a projected CRS (not WGS84). Use `.to_crs(epsg=32632)` to convert if needed.\\n\\n\"\n",
    "    \"When matching column names in datasets, use the fuzzy_name_match() tool to first inspect what name the item is registered as in the dataframe.\\n\\n\"\n",
    "    \"-------\\n\"\n",
    "    \"**Visualization**\\n\"\n",
    "    \"If visualization is requested, you must:\\n\"\n",
    "    \"   - Use the `python_repl_tool` to create **one clear, interpretable figure**, based on the request.\\n\"\n",
    "    \"   - display the visualization exactly ONCE using .show(), if possible.\\n\"\n",
    "    \"   - save the output figure to the `SAVING_DIRECTORY` folder, i.e. `./visualizer_outputs/`, after displaying it.\\n\"\n",
    "    \"Always aim to produce visually appealing plots. Your visualizations should be easy to interpret and presentation-ready.\"\n",
    "    \"\\nDefault visualization preferences:\\n\"\n",
    "    \"- Use line plots, bar charts, or scatter plots for tabular data.\\n\"\n",
    "    \"- For geospatial data, use `.explore()` or overlay plots via geopandas or folium.\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15631924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "analyst_agent = create_react_agent(\n",
    "    model=\"openai:gpt-4o\",  \n",
    "    tools=[list_loadable_datasets,\n",
    "           list_inmemory_datasets, \n",
    "           load_dataset, \n",
    "           describe_dataset, \n",
    "           python_repl_tool,\n",
    "           fuzzy_match_name,],\n",
    "    prompt=analyst_suffix,\n",
    "    name=\"analyst_agent\",\n",
    "    state_schema=AgentState\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5233da8",
   "metadata": {},
   "source": [
    "### Then in `app.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa51506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_supervisor import create_supervisor\n",
    "from langchain.chat_models import init_chat_model\n",
    "from analyst import analyst_agent, AgentState \n",
    "\n",
    "import chainlit as cl\n",
    "from langchain.schema.runnable.config import RunnableConfig\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "supervisor = create_supervisor(\n",
    "    model=init_chat_model(\"openai:gpt-4.1-2025-04-14\"),   \n",
    "    agents=[analyst_agent],\n",
    "    prompt=(\n",
    "        \"You are a supervisor managing a data analyst agent and a RAG agent spcialized in law retrieval. \\n\"\n",
    "        \"Assign data-analysis-related tasks to the data analyst\\n\"\n",
    "        \"Assign retrieval of laws or articles to the RAG agent.\\n\"\n",
    "        \"If the user asks to analize a law, you should first ask the RAG agent to retrieve it, then ask the data_analyst to analize it.\\n\\n\"\n",
    "        \"Do not call agents in parallel, call one agent at a time.\"\n",
    "    ),\n",
    "    state_schema = AgentState,  \n",
    "    add_handoff_back_messages=True,\n",
    "    output_mode=\"full_history\"\n",
    ").with_config(tags=[\"main_model\"])\n",
    "\n",
    "graph = supervisor.compile()\n",
    "\n",
    "\n",
    "@cl.on_message\n",
    "async def on_message(msg: cl.Message):\n",
    "    config = {\"configurable\": {\"thread_id\": cl.context.session.id}}\n",
    "    cb = cl.LangchainCallbackHandler()\n",
    "    final_answer = cl.Message(content=\"\")\n",
    "    \n",
    "    for msg, metadata in graph.stream({\"messages\": [HumanMessage(content=msg.content)]}, stream_mode=\"messages\", config=RunnableConfig(callbacks=[cb], **config)):\n",
    "        if msg.content:\n",
    "            await final_answer.stream_token(msg.content)\n",
    "\n",
    "    await final_answer.send()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1e497f",
   "metadata": {},
   "source": [
    "https://www.datacamp.com/tutorial/chainlit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
