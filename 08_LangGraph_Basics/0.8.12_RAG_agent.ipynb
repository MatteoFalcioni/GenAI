{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd97d96d",
   "metadata": {},
   "source": [
    "# RAG Agent\n",
    "\n",
    "We are going to create a RAG agent with LangGraph.\n",
    "\n",
    "We will be creating two agents for this task: the llm agent and the retriever agent. the structure we want co construct is this:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"../images/rag_agent.png\" alt=\"Basic Example of a graph in LangGraph\" style=\"width: 30%;\">\n",
    "</div>\n",
    "\n",
    "\n",
    "### Vector Database\n",
    "We will use Chroma wrapped in LangChain with `from langchain_chroma import Chroma` : https://python.langchain.com/docs/integrations/vectorstores/chroma/\n",
    "\n",
    "The `vectorstore.as_retriever` transforms the vector store into a retriever for easier usage in your chains. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3f6353",
   "metadata": {},
   "source": [
    "### `should_continue`\n",
    "\n",
    "Our `should_continue` function this time outputs a boolean value: `True` if a tool was called by the llm, `False` if not. That's how we decide if we are retrieveing data or not.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bf5304",
   "metadata": {},
   "source": [
    "### Why Two Agents? \n",
    "\n",
    "In this LangGraph-based implementation, we use two agents instead of binding the retriever tool directly to the LLM to promote modularity, control, and flexibility.\n",
    "\n",
    "By separating the LLM reasoning step from the tool execution step, we follow a \"separation of concerns\" principle that makes the system easier to debug, extend, and maintain. \n",
    "\n",
    "The LLM agent decides whether to call a tool and with what query, while the retriever agent handles the actual execution of that tool.\n",
    "\n",
    "This structure also supports multi-hop reasoning, allowing the LLM to iteratively query and refine answers based on retrieved information. Additionally, this graph-based setup enables future extensions, such as adding more tools or custom execution logic, without entangling the core LLM flow. Overall, the two-agent approach aligns better with the modular and iterative design philosophy of LangGraph.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
