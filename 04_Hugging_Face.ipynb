{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Hugging Face**"
      ],
      "metadata": {
        "id": "njWkn6lbzh-R"
      },
      "id": "njWkn6lbzh-R"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hugging Face is one of the most influential companies in the field of artificial intelligence, particularly known for democratizing access to state-of-the-art machine learning models. Originally famous for its work in natural language processing (NLP), Hugging Face created the `transformers` library, which provides easy-to-use implementations of powerful models like BERT, GPT, T5, and many others. Over time, it expanded beyond NLP into areas like computer vision, audio processing, and even reinforcement learning.\n",
        "\n",
        "The Hugging Face Hub acts as a central repository where researchers and developers can share, download, and fine-tune pre-trained models and datasets. This ecosystem has become essential for accelerating AI research and application development, reducing the need to train large models from scratch. Today, Hugging Face offers tools for model training, evaluation, deployment, and even optimization, playing a critical role in making cutting-edge AI more accessible to both researchers and industry practitioners."
      ],
      "metadata": {
        "id": "e538a978"
      },
      "id": "e538a978"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. The `transfomer` Library\n",
        "\n",
        "With the `transformers` library, you can perform tasks like text classification, translation, summarization, question answering, and more with just a few lines of code. It supports interoperability with both PyTorch and TensorFlow backends, and its API is designed to be user-friendly and flexible.\n",
        "\n",
        "Here's a simple example of how you can use `transformers` to perform summarization:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "a9SUCXaU0Gbf"
      },
      "id": "a9SUCXaU0Gbf"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "KpNPaLGL1zrX"
      },
      "id": "KpNPaLGL1zrX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load a summarization pipeline\n",
        "summarizer = pipeline(\"summarization\")\n",
        "\n",
        "# Text to summarize\n",
        "text = \"\"\"\n",
        "Hugging Face is a company that specializes in Natural Language Processing technologies.\n",
        "They have created the popular transformers library which allows users to access\n",
        "pre-trained models for a variety of tasks such as text classification, summarization,\n",
        "and translation, with minimal code and configuration.\n",
        "\"\"\"\n",
        "\n",
        "# Generate summary\n",
        "summary = summarizer(text, max_length=40, min_length=5, do_sample=False)\n",
        "\n",
        "print(summary[0]['summary_text'])"
      ],
      "metadata": {
        "id": "TmhaAqpV0-Cx"
      },
      "id": "TmhaAqpV0-Cx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example demonstrates how easily you can harness the power of a pre-trained model without any deep learning expertise. The `transformers` library continues to evolve rapidly, introducing new models and capabilities that are at the forefront of AI research and application."
      ],
      "metadata": {
        "id": "4ElwVGBD07nv"
      },
      "id": "4ElwVGBD07nv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  1.2 Sentiment Analysis Example\n",
        "\n",
        "Let's make another example about sentiment analysis."
      ],
      "metadata": {
        "id": "8zveSGHEzWjw"
      },
      "id": "8zveSGHEzWjw"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "result = classifier(\"I hated the last Star Wars movie\")   # rightfully so, it sucked\n",
        "print(result)"
      ],
      "metadata": {
        "id": "yX_1j7oxzyr6"
      },
      "id": "yX_1j7oxzyr6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is another way we can pass the task  to our model:"
      ],
      "metadata": {
        "id": "_Uo_rS7T0LeN"
      },
      "id": "_Uo_rS7T0LeN"
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline(task=\"sentiment-analysis\")(\"I kind of enjoyed the Barbie Movie\")"
      ],
      "metadata": {
        "id": "34BdKy8w0upo"
      },
      "id": "34BdKy8w0upo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An important hing we should learnis specify a model that we want to use. When we don't select one, Hugging Face defaults one for us.\n",
        "\n",
        "Let's choose [facebook/bart-large-mnli](https://huggingface.co/facebook/bart-large-mnli) and perform sentiment analysis with it."
      ],
      "metadata": {
        "id": "B3M-DSD102fw"
      },
      "id": "B3M-DSD102fw"
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline(task=\"sentiment-analysis\", model=\"facebook/bart-large-mnli\")\\\n",
        "                                    (\"Western european media is censoring Palestine related content.\\\n",
        "                                    They are defending Israel, while it is committing war crimes. \\\n",
        "                                    That's unacceptable.\")"
      ],
      "metadata": {
        "id": "I_U7EhQW1c5u"
      },
      "id": "I_U7EhQW1c5u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can actually perform sentiment analysis in batches, by passing a list of texts to perform the task on."
      ],
      "metadata": {
        "id": "3czxORXt2B3T"
      },
      "id": "3czxORXt2B3T"
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(task=\"sentiment-analysis\", model=\"SamLowe/roberta-base-go_emotions\")  # using a more complex sentiment model\n",
        "\n",
        "task_list= [\"I ove learning about AI\", \\\n",
        "        \"I am not sure using GPT in your everyday life is going to be good long term\", \\\n",
        "        \"I love working out\"]\n",
        "\n",
        "classifier(task_list)"
      ],
      "metadata": {
        "id": "s1rvy31I2V8X"
      },
      "id": "s1rvy31I2V8X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Text Generation\n",
        "\n",
        "Another incredible task available is text generation:"
      ],
      "metadata": {
        "id": "7xLun-g926Od"
      },
      "id": "7xLun-g926Od"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "text_generator = pipeline(\"text-generation\", model=\"distilbert/distilgpt2\")\n",
        "\n",
        "generated_text = text_generator(\"Today is a rainy day in London\",\n",
        "                                truncation=True,\n",
        "                                num_return_sequences = 2)\n",
        "print(\"Generated_text:\\n\", generated_text[0]['generated_text'])"
      ],
      "metadata": {
        "id": "9lqnBX153KEH"
      },
      "id": "9lqnBX153KEH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4. Text Summarization With Hugging Face"
      ],
      "metadata": {
        "id": "UwRtl561prmi"
      },
      "id": "UwRtl561prmi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now try to use a model to summarize papers from ArXiv.\n",
        "\n",
        "First we need to import the [`arxiv`](https://pypi.org/project/arxiv/) library:"
      ],
      "metadata": {
        "id": "mMi6DJpfpv25"
      },
      "id": "mMi6DJpfpv25"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arxiv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzAKeZMZqN88",
        "outputId": "ca7ec9dd-db4c-4722-c37e-4e9c0cbe43b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting arxiv\n",
            "  Downloading arxiv-2.2.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting feedparser~=6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.11/dist-packages (from arxiv) (2.32.3)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (2025.4.26)\n",
            "Downloading arxiv-2.2.0-py3-none-any.whl (11 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=34cc686a121c2366f7a40675d4e42602f2fad5daa25e73ec67c1369d26fa8a31\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
            "Successfully installed arxiv-2.2.0 feedparser-6.0.11 sgmllib3k-1.0.0\n"
          ]
        }
      ],
      "id": "tzAKeZMZqN88"
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "1sXRkb9UqJNg"
      },
      "execution_count": null,
      "outputs": [],
      "id": "1sXRkb9UqJNg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use it to fetch scientific papers regarding a chosen topic:"
      ],
      "metadata": {
        "id": "fTHlIhpWqTbd"
      },
      "id": "fTHlIhpWqTbd"
    },
    {
      "cell_type": "code",
      "source": [
        "# we need to write a query:\n",
        "query = \"ai OR artificial intelligence OR machine learning\"\n",
        "\n",
        "search = arxiv.Search(query=query, max_results=10, sort_by=arxiv.SortCriterion.SubmittedDate)\n",
        "\n",
        "# Fetch papers\n",
        "\n",
        "papers = []\n",
        "\n",
        "for result in search.results():\n",
        "  papers.append({\n",
        "      'published' : result.published,\n",
        "      'title' : result.title,\n",
        "      'abstract' : result.summary,\n",
        "      'categories' : result.categories\n",
        "  })\n",
        "\n",
        "# convert to DF\n",
        "\n",
        "df = pd.DataFrame(papers)\n",
        "df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "id": "PRPtS7dyqiTq",
        "outputId": "b5201f60-b9d6-4b33-c70e-3db019d1ff5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-2324e0f67863>:10: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
            "  for result in search.results():\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  published  \\\n",
              "0 2025-06-06 17:59:50+00:00   \n",
              "1 2025-06-06 17:59:28+00:00   \n",
              "2 2025-06-06 17:58:54+00:00   \n",
              "3 2025-06-06 17:58:36+00:00   \n",
              "4 2025-06-06 17:51:26+00:00   \n",
              "5 2025-06-06 17:48:23+00:00   \n",
              "6 2025-06-06 17:47:27+00:00   \n",
              "7 2025-06-06 17:43:00+00:00   \n",
              "8 2025-06-06 17:40:12+00:00   \n",
              "9 2025-06-06 17:39:32+00:00   \n",
              "\n",
              "                                               title  \\\n",
              "0  TerraFM: A Scalable Foundation Model for Unifi...   \n",
              "1  Eigenspectrum Analysis of Neural Networks with...   \n",
              "2                Distillation Robustifies Unlearning   \n",
              "3  Movie Facts and Fibs (MF$^2$): A Benchmark for...   \n",
              "4  Accurately simulating core-collapse self-inter...   \n",
              "5  Cartridges: Lightweight and general-purpose lo...   \n",
              "6  Integrating Complexity and Biological Realism:...   \n",
              "7  PyGemini: Unified Software Development towards...   \n",
              "8  Reflect-then-Plan: Offline Model-Based Plannin...   \n",
              "9  An Optimized Franz-Parisi Criterion and its Eq...   \n",
              "\n",
              "                                            abstract  \\\n",
              "0  Modern Earth observation (EO) increasingly lev...   \n",
              "1  Diagnosing deep neural networks (DNNs) through...   \n",
              "2  Current LLM unlearning methods are not robust:...   \n",
              "3  Despite recent progress in vision-language mod...   \n",
              "4  The properties of satellite halos provide a pr...   \n",
              "5  Large language models are often used to answer...   \n",
              "6  Spiking Neural Networks (SNNs) event-driven na...   \n",
              "7  Ensuring the safety and certifiability of auto...   \n",
              "8  Offline reinforcement learning (RL) is crucial...   \n",
              "9  Bandeira et al. (2022) introduced the Franz-Pa...   \n",
              "\n",
              "                                          categories  \n",
              "0                                            [cs.CV]  \n",
              "1                                     [cs.LG, cs.AI]  \n",
              "2                                     [cs.LG, cs.AI]  \n",
              "3                              [cs.CV, cs.CL, cs.LG]  \n",
              "4                 [astro-ph.CO, astro-ph.GA, hep-ph]  \n",
              "5                              [cs.CL, cs.AI, cs.LG]  \n",
              "6                         [cs.NE, eess.IV, q-bio.NC]  \n",
              "7  [cs.RO, cs.SE, cs.SY, eess.SY, D.2.11; I.6.2; ...  \n",
              "8                                     [cs.AI, cs.LG]  \n",
              "9  [math.ST, cond-mat.stat-mech, cs.CC, stat.ML, ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-01ef2b8a-0f8a-46c6-bbc1-7ba3aa9c1c7b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>published</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>categories</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2025-06-06 17:59:50+00:00</td>\n",
              "      <td>TerraFM: A Scalable Foundation Model for Unifi...</td>\n",
              "      <td>Modern Earth observation (EO) increasingly lev...</td>\n",
              "      <td>[cs.CV]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2025-06-06 17:59:28+00:00</td>\n",
              "      <td>Eigenspectrum Analysis of Neural Networks with...</td>\n",
              "      <td>Diagnosing deep neural networks (DNNs) through...</td>\n",
              "      <td>[cs.LG, cs.AI]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2025-06-06 17:58:54+00:00</td>\n",
              "      <td>Distillation Robustifies Unlearning</td>\n",
              "      <td>Current LLM unlearning methods are not robust:...</td>\n",
              "      <td>[cs.LG, cs.AI]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2025-06-06 17:58:36+00:00</td>\n",
              "      <td>Movie Facts and Fibs (MF$^2$): A Benchmark for...</td>\n",
              "      <td>Despite recent progress in vision-language mod...</td>\n",
              "      <td>[cs.CV, cs.CL, cs.LG]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2025-06-06 17:51:26+00:00</td>\n",
              "      <td>Accurately simulating core-collapse self-inter...</td>\n",
              "      <td>The properties of satellite halos provide a pr...</td>\n",
              "      <td>[astro-ph.CO, astro-ph.GA, hep-ph]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2025-06-06 17:48:23+00:00</td>\n",
              "      <td>Cartridges: Lightweight and general-purpose lo...</td>\n",
              "      <td>Large language models are often used to answer...</td>\n",
              "      <td>[cs.CL, cs.AI, cs.LG]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2025-06-06 17:47:27+00:00</td>\n",
              "      <td>Integrating Complexity and Biological Realism:...</td>\n",
              "      <td>Spiking Neural Networks (SNNs) event-driven na...</td>\n",
              "      <td>[cs.NE, eess.IV, q-bio.NC]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2025-06-06 17:43:00+00:00</td>\n",
              "      <td>PyGemini: Unified Software Development towards...</td>\n",
              "      <td>Ensuring the safety and certifiability of auto...</td>\n",
              "      <td>[cs.RO, cs.SE, cs.SY, eess.SY, D.2.11; I.6.2; ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2025-06-06 17:40:12+00:00</td>\n",
              "      <td>Reflect-then-Plan: Offline Model-Based Plannin...</td>\n",
              "      <td>Offline reinforcement learning (RL) is crucial...</td>\n",
              "      <td>[cs.AI, cs.LG]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2025-06-06 17:39:32+00:00</td>\n",
              "      <td>An Optimized Franz-Parisi Criterion and its Eq...</td>\n",
              "      <td>Bandeira et al. (2022) introduced the Franz-Pa...</td>\n",
              "      <td>[math.ST, cond-mat.stat-mech, cs.CC, stat.ML, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-01ef2b8a-0f8a-46c6-bbc1-7ba3aa9c1c7b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-01ef2b8a-0f8a-46c6-bbc1-7ba3aa9c1c7b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-01ef2b8a-0f8a-46c6-bbc1-7ba3aa9c1c7b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-aa52f147-c8de-4910-bd05-1a257754a438\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-aa52f147-c8de-4910-bd05-1a257754a438')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-aa52f147-c8de-4910-bd05-1a257754a438 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"published\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2025-06-06 17:39:32+00:00\",\n        \"max\": \"2025-06-06 17:59:50+00:00\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"2025-06-06 17:40:12+00:00\",\n          \"2025-06-06 17:59:28+00:00\",\n          \"2025-06-06 17:48:23+00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Reflect-then-Plan: Offline Model-Based Planning through a Doubly Bayesian Lens\",\n          \"Eigenspectrum Analysis of Neural Networks without Aspect Ratio Bias\",\n          \"Cartridges: Lightweight and general-purpose long context representations via self-study\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Offline reinforcement learning (RL) is crucial when online exploration is\\ncostly or unsafe but often struggles with high epistemic uncertainty due to\\nlimited data. Existing methods rely on fixed conservative policies, restricting\\nadaptivity and generalization. To address this, we propose Reflect-then-Plan\\n(RefPlan), a novel doubly Bayesian offline model-based (MB) planning approach.\\nRefPlan unifies uncertainty modeling and MB planning by recasting planning as\\nBayesian posterior estimation. At deployment, it updates a belief over\\nenvironment dynamics using real-time observations, incorporating uncertainty\\ninto MB planning via marginalization. Empirical results on standard benchmarks\\nshow that RefPlan significantly improves the performance of conservative\\noffline RL policies. In particular, RefPlan maintains robust performance under\\nhigh epistemic uncertainty and limited data, while demonstrating resilience to\\nchanging environment dynamics, improving the flexibility, generalizability, and\\nrobustness of offline-learned policies.\",\n          \"Diagnosing deep neural networks (DNNs) through the eigenspectrum of weight\\nmatrices has been an active area of research in recent years. At a high level,\\neigenspectrum analysis of DNNs involves measuring the heavytailness of the\\nempirical spectral densities (ESD) of weight matrices. It provides insight into\\nhow well a model is trained and can guide decisions on assigning better\\nlayer-wise training hyperparameters. In this paper, we address a challenge\\nassociated with such eigenspectrum methods: the impact of the aspect ratio of\\nweight matrices on estimated heavytailness metrics. We demonstrate that\\nmatrices of varying sizes (and aspect ratios) introduce a non-negligible bias\\nin estimating heavytailness metrics, leading to inaccurate model diagnosis and\\nlayer-wise hyperparameter assignment. To overcome this challenge, we propose\\nFARMS (Fixed-Aspect-Ratio Matrix Subsampling), a method that normalizes the\\nweight matrices by subsampling submatrices with a fixed aspect ratio. Instead\\nof measuring the heavytailness of the original ESD, we measure the average ESD\\nof these subsampled submatrices. We show that measuring the heavytailness of\\nthese submatrices with the fixed aspect ratio can effectively mitigate the\\naspect ratio bias. We validate our approach across various optimization\\ntechniques and application domains that involve eigenspectrum analysis of\\nweights, including image classification in computer vision (CV) models,\\nscientific machine learning (SciML) model training, and large language model\\n(LLM) pruning. Our results show that despite its simplicity, FARMS uniformly\\nimproves the accuracy of eigenspectrum analysis while enabling more effective\\nlayer-wise hyperparameter assignment in these application domains. In one of\\nthe LLM pruning experiments, FARMS reduces the perplexity of the LLaMA-7B model\\nby 17.3% when compared with the state-of-the-art method.\",\n          \"Large language models are often used to answer queries grounded in large text\\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\\nentire corpus in the context window and leveraging in-context learning (ICL).\\nAlthough current models support contexts of 100K-1M tokens, this setup is\\ncostly to serve because the memory consumption of the KV cache scales with\\ninput length. We explore an alternative: training a smaller KV cache offline on\\neach corpus. At inference time, we load this trained KV cache, which we call a\\nCartridge, and decode a response. Critically, the cost of training a Cartridge\\ncan be amortized across all the queries referencing the same corpus. However,\\nwe find that the naive approach of training the Cartridge with next-token\\nprediction on the corpus is not competitive with ICL. Instead, we propose\\nself-study, a training recipe in which we generate synthetic conversations\\nabout the corpus and train the Cartridge with a context-distillation objective.\\nWe find that Cartridges trained with self-study replicate the functionality of\\nICL, while being significantly cheaper to serve. On challenging long-context\\nbenchmarks, Cartridges trained with self-study match ICL performance while\\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\\nextends the model's effective context length (e.g. from 128k to 484k tokens on\\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\\ntime without retraining.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"categories\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "id": "PRPtS7dyqiTq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "How about summarizing these abstracts? Easy:"
      ],
      "metadata": {
        "id": "G5to5ct1rXjM"
      },
      "id": "G5to5ct1rXjM"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "abstract = df['abstract'][0]\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Summarization\n",
        "summarization_result = summarizer(abstract)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2_mAVYVsRr-",
        "outputId": "20cbbd22-94fa-4f5c-af07-ecb6634d4bfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ],
      "id": "a2_mAVYVsRr-"
    },
    {
      "cell_type": "code",
      "source": [
        "summarization_result[0]['summary_text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "R2OqFYyPsiCP",
        "outputId": "a990c57a-7d6e-4598-929d-b9a1436dbd2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'TerraFM is a self-supervised learning model that leverages Sentinel-1 and Sentinel-2 imagery. It uses large spatial tiles and land-cover awaresampling to enrich spatial and semantic coverage. Our training strategy integrates local-global-contrastive learning and introduces a dual-centering mechanism.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "id": "R2OqFYyPsiCP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Transformer-Based Tokenization\n",
        "\n",
        "A key focus for working with Large Language Models (LLMs) is understanding how to properly tokenize text, and Hugging Face makes this process seamless. Before diving into model usage, it's essential to grasp some foundational technicalities.\n",
        "\n",
        "One of the most important is transformer-based tokenization, the standard method for preparing text for LLMs like ChatGPT and BERT. Unlike traditional methods such as Bag of Words, transformer tokenization breaks down text into subword units using techniques like Byte-Pair Encoding (BPE) or WordPiece. This allows models to efficiently handle rare words, typos, and unseen vocabulary while preserving semantic meaning.\n",
        "\n",
        "Transformer tokenizers are dynamic and context-aware, significantly improving performance and generalization in modern NLP tasks compared to older tokenization strategies."
      ],
      "metadata": {
        "id": "YDKwqPZP3j5b"
      },
      "id": "YDKwqPZP3j5b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Understanding Transfomer Tokenization\n",
        "\n",
        "The best way to understand what's happening when we tokenize through a transformer-like arhcitecture is to look at what models do under the hood.\n",
        "\n",
        "In order to do so we can use the [`AutoTokenizer`](https://huggingface.co/docs/transformers/v4.52.3/en/model_doc/auto#transformers.AutoTokenizer) class from the `transformers` library.\n",
        "\n",
        "This is part of [Hugging Face's Auto Classes](https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes).\n",
        "\n",
        "The Auto Classes in Hugging Face, such as `AutoTokenizer` and `AutoModel`, are designed to simplify working with a wide range of pre-trained models. Instead of manually selecting the appropriate tokenizer or model class, Auto Classes automatically infer the correct one based on the model checkpoint you provide. This abstraction makes your code more flexible and model-agnostic, allowing you to swap between different architectures like BERT, RoBERTa, or DistilBERT without changing your pipeline.\n",
        "\n",
        "`AutoTokenizer` in particular ensures that text is tokenized using the exact method the underlying model was trained with, whether it’s WordPiece, Byte-Pair Encoding (BPE), or SentencePiece. This guarantees compatibility and maximizes model performance when processing text data."
      ],
      "metadata": {
        "id": "yS-lTtSM4smI"
      },
      "id": "yS-lTtSM4smI"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load a pre-trained tokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # using a bert-like tokenizer\n",
        "\n",
        "# example text\n",
        "text = \"I really liked the movie No Country for Old Men\"\n",
        "\n",
        "# Tokenize the Text\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(\"Tokens:\\n\", tokens)"
      ],
      "metadata": {
        "id": "L0KgE_O98ChG"
      },
      "id": "L0KgE_O98ChG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These tokens look like usual word tokens from before, where is the transformer architecture?\n",
        "\n",
        "The important step is the transformation from tokens to ids. Here, each token is mapped to a unique integer ID using the model's pre-trained vocabulary. These IDs are then fed into the embedding layer of the transformer architecture, which transforms them into dense vectors that capture semantic information.\n",
        "\n",
        "This step is crucial because transformers operate on *continuous vector representations* rather than discrete words or tokens.\n",
        "\n",
        "The (auto) tokenizer ensures that the input format precisely matches what the transformer model expects, preserving alignment between tokenization and model training.\n"
      ],
      "metadata": {
        "id": "uPkNKLbK4vYo"
      },
      "id": "uPkNKLbK4vYo"
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert tokens to input IDs\n",
        "\n",
        "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(\"Input IDs:\\n\", input_ids)"
      ],
      "metadata": {
        "id": "PX5spDff9sOJ"
      },
      "id": "PX5spDff9sOJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whenever we want to pass data to our transformer model, we should do these encoding operations: tokenization + conversion to IDs.\n",
        "\n",
        "The `tokenizer()` method allows us to do so in one go:"
      ],
      "metadata": {
        "id": "TQer3oDI-G8K"
      },
      "id": "TQer3oDI-G8K"
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the text (tokenization + converting to IDs)\n",
        "\n",
        "encoded_input = tokenizer(text)\n",
        "\n",
        "print(\"Encoded input\", encoded_input)"
      ],
      "metadata": {
        "id": "Rm-dCKmP96kw"
      },
      "id": "Rm-dCKmP96kw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are some interesting things happeping here:\n",
        "\n",
        "* First, we can see that the first token is $101$: this indicates the start of the sentence.\n",
        "\n",
        "* Secondly, we see a `token_type_ids` object: let's skip its analysis for now, we'll come back to it later on.\n",
        "\n",
        "* Third, and most imortantly, we see our `attention_mask`, automatically computed by our tokenizer."
      ],
      "metadata": {
        "id": "kvLnYWUu-byX"
      },
      "id": "kvLnYWUu-byX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we want to decode, we will pass the input ids to the `decode()` method:"
      ],
      "metadata": {
        "id": "4rPEEJrJ-6QB"
      },
      "id": "4rPEEJrJ-6QB"
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode text\n",
        "\n",
        "decoded_output = tokenizer.decode(input_ids)\n",
        "print(\"Decode Output: \", decoded_output)"
      ],
      "metadata": {
        "id": "ktthKN9C_L9G"
      },
      "id": "ktthKN9C_L9G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Fine Tuning Using a Pretrained Model\n",
        "\n",
        "We will now try to fine tune a pretrained model on the `IMDB Dataset`.\n"
      ],
      "metadata": {
        "id": "_0A-jkcY_znJ"
      },
      "id": "_0A-jkcY_znJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Loading an Hugging Face Dataset\n",
        "\n",
        "It's really simple to use Hugging Face's datasets. First we must install the `datasets` library:"
      ],
      "metadata": {
        "id": "lQLWgBJRDVyH"
      },
      "id": "lQLWgBJRDVyH"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade datasets fsspec\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "1TiVUenTALb7"
      },
      "id": "1TiVUenTALb7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can just load the dataset by passing its name to the `load_dataset()` function:"
      ],
      "metadata": {
        "id": "SjA06ZzfAbju"
      },
      "id": "SjA06ZzfAbju"
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"imdb\")"
      ],
      "metadata": {
        "id": "vzkZn9MmAi7B"
      },
      "id": "vzkZn9MmAi7B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "metadata": {
        "id": "CzWQjCFCAPMS"
      },
      "id": "CzWQjCFCAPMS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.DataFrame(ds['train'])\n",
        "\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "CO5SaYGxCE32"
      },
      "id": "CO5SaYGxCE32",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.DataFrame(ds['test'])\n",
        "\n",
        "test_df.head()"
      ],
      "metadata": {
        "id": "Z6ovqboyCkHg"
      },
      "id": "Z6ovqboyCkHg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.info()"
      ],
      "metadata": {
        "id": "VqHhbkn3CPi5"
      },
      "id": "VqHhbkn3CPi5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(np.unique(train_df['label'])) # what labels do we have?"
      ],
      "metadata": {
        "id": "hul7Oi_JC57M"
      },
      "id": "hul7Oi_JC57M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Tokenize The Data\n",
        "\n",
        "Let's tokenize the dataset. We will add padding of size `max_length` (the maximum lenght accpeted by the model) in order for our tokens to be of the same size - padding shorter phrases -  and set `truncation=True` for the same reason (truncating longer words to our max length). See [padding and truncation](https://huggingface.co/docs/transformers/pad_truncation#padding-and-truncation) for more details."
      ],
      "metadata": {
        "id": "ydSNt2eRDKnC"
      },
      "id": "ydSNt2eRDKnC"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the (bert-like) tokenizer (automatically retrieved by AutoTokenizer)\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "  return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_ds = ds.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "zgK_A16tEHEF"
      },
      "id": "zgK_A16tEHEF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above we use the `map` function. From its [documentation](https://huggingface.co/docs/datasets/process#map):\n",
        "\n",
        "\"*Some of the more powerful applications of 🤗 Datasets come from using the `map()` function. The primary purpose of `map()` is to speed up processing functions. It allows you to apply a processing function to each example in a dataset, independently or in batches.*\""
      ],
      "metadata": {
        "id": "qCiQxzBzFt4h"
      },
      "id": "qCiQxzBzFt4h"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_ds"
      ],
      "metadata": {
        "id": "Q1kdCmjRGI0Z"
      },
      "id": "Q1kdCmjRGI0Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Set Up the Training Arguments\n",
        "\n",
        "We can almost start training; we just need to secify the hyperparameters and the training settings.\n",
        "\n",
        "We can do this with the help of Hugging face's [`TrainingArguments`](https://huggingface.co/docs/datasets/process#map):"
      ],
      "metadata": {
        "id": "O4r7b-yZGQ9I"
      },
      "id": "O4r7b-yZGQ9I"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\", # output dir\n",
        "    eval_strategy=\"epoch\",  # evaluate every epoch\n",
        "    learning_rate=2e-5, # lr\n",
        "    per_device_train_batch_size=16, #batch size for training\n",
        "    per_device_eval_batch_size=16,  # batch size for evaluation\n",
        "    num_train_epochs=1, # number of training epochs\n",
        "    weight_decay=0.01 # strength of weight decay\n",
        ")\n",
        "\n",
        "training_args"
      ],
      "metadata": {
        "id": "B3XeOCg2HBzO"
      },
      "id": "B3XeOCg2HBzO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Initialize the Model\n",
        "\n",
        "Now we can initialize our model, using the `Auto` class, and initialize its [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer#trainer)."
      ],
      "metadata": {
        "id": "FhvBlDAQHa_-"
      },
      "id": "FhvBlDAQHa_-"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, Trainer\n",
        "\n",
        "# Load pre-trained model\n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased',\n",
        "                                                           num_labels=2)  # we only have 2 labels (0,1)\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_ds['train'],\n",
        "    eval_dataset=tokenized_ds['test']\n",
        ")"
      ],
      "metadata": {
        "id": "2DjejE_qIL3q"
      },
      "id": "2DjejE_qIL3q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note:**\n",
        ">\n",
        "> In Hugging Face's `transformers` library, choosing the correct Auto Class is crucial depending on your task.\n",
        ">\n",
        "> `AutoModel` loads **only the base pre-trained transformer model** (like BERT or RoBERTa) without any task-specific layers. *It outputs embeddings but does not add the necessary heads for tasks like classification, generation, or question answering*.\n",
        ">\n",
        "> On the other hand, `AutoModelForSequenceClassification` automatically adds a classification head on top of the base model, specifically designed for classification tasks. This head typically consists of a dense layer that outputs logits for each class, enabling the model to compute classification loss functions like cross-entropy.\n",
        ">\n",
        "> Specifying the right Auto Class ensures that your model is not only correctly structured but also fully compatible with high-level APIs like Hugging Face’s `Trainer`, which expects models to output both logits and loss during training.\n",
        ">\n",
        "> In short, use `AutoModel` if you only need raw embeddings or plan to design your own head; use task-specific Auto Classes like `AutoModelForSequenceClassification` to quickly get a model ready for fine-tuning on your downstream task.\n"
      ],
      "metadata": {
        "id": "r845bhI8JsVD"
      },
      "id": "r845bhI8JsVD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5 Train the Model\n",
        "\n",
        "Now training is as simple as it could be:"
      ],
      "metadata": {
        "id": "wlABTaJiJLfz"
      },
      "id": "wlABTaJiJLfz"
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "NMQvAZZzJ_Yl"
      },
      "id": "NMQvAZZzJ_Yl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.6 Evaluate the Trained Model\n",
        "\n",
        "Evaluating the model is very simple as well:"
      ],
      "metadata": {
        "id": "g9wUZYa_K1OZ"
      },
      "id": "g9wUZYa_K1OZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "results = trainer.evaluate()\n",
        "print(results)"
      ],
      "metadata": {
        "id": "toevnd6XK9ot"
      },
      "id": "toevnd6XK9ot",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.7 Save the Fine-Tuned Model\n",
        "\n",
        "We can of course save the trained model for later use:"
      ],
      "metadata": {
        "id": "JdiVFfobLMJW"
      },
      "id": "JdiVFfobLMJW"
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model:\n",
        "model.save_pretrained('./fine-tuned-model')"
      ],
      "metadata": {
        "id": "vj1KFXVePtZd"
      },
      "id": "vj1KFXVePtZd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note:**\n",
        ">\n",
        "> When fine-tuning a transformer model, it’s essential to save the tokenizer along with the model. The tokenizer defines how input text is split into tokens and mapped to IDs, and it must match the model’s pretraining configuration. `AutoTokenizer` relies on files like `tokenizer_config.json` and `vocab.txt` to load the correct tokenizer automatically. Without these files, the model would not know how to correctly interpret input text, leading to mismatches or errors. Saving both ensures that future users can seamlessly reload and use the fine-tuned model without manual adjustments.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iDEXAaGBQKc-"
      },
      "id": "iDEXAaGBQKc-"
    },
    {
      "cell_type": "code",
      "source": [
        "# Save tokenizer\n",
        "tokenizer.save_pretrained('./fine-tuned-model')"
      ],
      "metadata": {
        "id": "8M-gPf0uQbM-"
      },
      "id": "8M-gPf0uQbM-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Hugging Face API Key Generation"
      ],
      "metadata": {
        "id": "BRpwlEAmtH8w"
      },
      "id": "BRpwlEAmtH8w"
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Hugging Face API keys](https://www.nightfall.ai/ai-security-101/hugging-face-api-key) are secure tokens that allow users to authenticate and interact programmatically with the Hugging Face Hub. They serve as a safer alternative to using your account password, enabling access to features that require user authentication. With an API key, you can perform actions such as uploading models and datasets to your profile, accessing private or restricted models, and managing repositories.\n",
        "\n",
        "When you fine-tune a model and wish to share it publicly or privately on Hugging Face, an API key is required to push the model to the Hub. Without authentication, you are limited to accessing public models and datasets. API keys can be generated from your Hugging Face account settings, under the \"Access Tokens\" section, where you can also control their permissions — for instance, restricting them to read-only or granting full write access.\n",
        "\n",
        "Once generated, the key can be used in your Python environment with the `huggingface_hub` library by calling the `login()` function or set as an environment variable for use in scripts and CI/CD pipelines. This approach ensures that workflows involving model sharing, fine-tuning, or private access are both secure and efficient. API keys have become an essential tool for anyone working seriously with the Hugging Face ecosystem.\n"
      ],
      "metadata": {
        "id": "p-4nyHL8tNF_"
      },
      "id": "p-4nyHL8tNF_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Permission | Description                                                  |\n",
        "|------------|--------------------------------------------------------------|\n",
        "| `read`     | Allows downloading public and private models or datasets.    |\n",
        "| `write`    | Allows uploading (pushing) models or datasets and updating them. |\n",
        "| `admin`    | Full control: read, write, and delete repositories or manage settings. |\n",
        "\n",
        "> **Note:** It’s good practice to generate API keys with only the permissions needed for your task. For example, use `read` for inference scripts and `write` or `admin` only when necessary for uploading or managing content.\n"
      ],
      "metadata": {
        "id": "aXewJnVYuwN2"
      },
      "id": "aXewJnVYuwN2"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}