{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Hugging Face**"
      ],
      "metadata": {
        "id": "njWkn6lbzh-R"
      },
      "id": "njWkn6lbzh-R"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hugging Face is one of the most influential companies in the field of artificial intelligence, particularly known for democratizing access to state-of-the-art machine learning models. Originally famous for its work in natural language processing (NLP), Hugging Face created the `transformers` library, which provides easy-to-use implementations of powerful models like BERT, GPT, T5, and many others. Over time, it expanded beyond NLP into areas like computer vision, audio processing, and even reinforcement learning.\n",
        "\n",
        "The Hugging Face Hub acts as a central repository where researchers and developers can share, download, and fine-tune pre-trained models and datasets. This ecosystem has become essential for accelerating AI research and application development, reducing the need to train large models from scratch. Today, Hugging Face offers tools for model training, evaluation, deployment, and even optimization, playing a critical role in making cutting-edge AI more accessible to both researchers and industry practitioners."
      ],
      "metadata": {
        "id": "e538a978"
      },
      "id": "e538a978"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. The `transfomer` Library\n",
        "\n",
        "With the `transformers` library, you can perform tasks like text classification, translation, summarization, question answering, and more with just a few lines of code. It supports interoperability with both PyTorch and TensorFlow backends, and its API is designed to be user-friendly and flexible.\n",
        "\n",
        "Here's a simple example of how you can use `transformers` to perform summarization:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "a9SUCXaU0Gbf"
      },
      "id": "a9SUCXaU0Gbf"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "KpNPaLGL1zrX"
      },
      "id": "KpNPaLGL1zrX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load a summarization pipeline\n",
        "summarizer = pipeline(\"summarization\")\n",
        "\n",
        "# Text to summarize\n",
        "text = \"\"\"\n",
        "Hugging Face is a company that specializes in Natural Language Processing technologies.\n",
        "They have created the popular transformers library which allows users to access\n",
        "pre-trained models for a variety of tasks such as text classification, summarization,\n",
        "and translation, with minimal code and configuration.\n",
        "\"\"\"\n",
        "\n",
        "# Generate summary\n",
        "summary = summarizer(text, max_length=40, min_length=5, do_sample=False)\n",
        "\n",
        "print(summary[0]['summary_text'])"
      ],
      "metadata": {
        "id": "TmhaAqpV0-Cx"
      },
      "id": "TmhaAqpV0-Cx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example demonstrates how easily you can harness the power of a pre-trained model without any deep learning expertise. The `transformers` library continues to evolve rapidly, introducing new models and capabilities that are at the forefront of AI research and application."
      ],
      "metadata": {
        "id": "4ElwVGBD07nv"
      },
      "id": "4ElwVGBD07nv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  1.2 Sentiment Analysis Example\n",
        "\n",
        "Let's make another example about sentiment analysis."
      ],
      "metadata": {
        "id": "8zveSGHEzWjw"
      },
      "id": "8zveSGHEzWjw"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "result = classifier(\"I hated the last Star Wars movie\")   # rightfully so, it sucked\n",
        "print(result)"
      ],
      "metadata": {
        "id": "yX_1j7oxzyr6"
      },
      "id": "yX_1j7oxzyr6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is another way we can pass the task  to our model:"
      ],
      "metadata": {
        "id": "_Uo_rS7T0LeN"
      },
      "id": "_Uo_rS7T0LeN"
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline(task=\"sentiment-analysis\")(\"I kind of enjoyed the Barbie Movie\")"
      ],
      "metadata": {
        "id": "34BdKy8w0upo"
      },
      "id": "34BdKy8w0upo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An important hing we should learnis specify a model that we want to use. When we don't select one, Hugging Face defaults one for us.\n",
        "\n",
        "Let's choose [facebook/bart-large-mnli](https://huggingface.co/facebook/bart-large-mnli) and perform sentiment analysis with it."
      ],
      "metadata": {
        "id": "B3M-DSD102fw"
      },
      "id": "B3M-DSD102fw"
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline(task=\"sentiment-analysis\", model=\"facebook/bart-large-mnli\")\\\n",
        "                                    (\"Western european media is censoring Palestine related content.\\\n",
        "                                    They are defending Israel, while it is committing war crimes. \\\n",
        "                                    That's unacceptable.\")"
      ],
      "metadata": {
        "id": "I_U7EhQW1c5u"
      },
      "id": "I_U7EhQW1c5u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can actually perform sentiment analysis in batches, by passing a list of texts to perform the task on."
      ],
      "metadata": {
        "id": "3czxORXt2B3T"
      },
      "id": "3czxORXt2B3T"
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(task=\"sentiment-analysis\", model=\"SamLowe/roberta-base-go_emotions\")  # using a more complex sentiment model\n",
        "\n",
        "task_list= [\"I ove learning about AI\", \\\n",
        "        \"I am not sure using GPT in your everyday life is going to be good long term\", \\\n",
        "        \"I love working out\"]\n",
        "\n",
        "classifier(task_list)"
      ],
      "metadata": {
        "id": "s1rvy31I2V8X"
      },
      "id": "s1rvy31I2V8X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Text Generation\n",
        "\n",
        "Another incredible task available is text generation:"
      ],
      "metadata": {
        "id": "7xLun-g926Od"
      },
      "id": "7xLun-g926Od"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "text_generator = pipeline(\"text-generation\", model=\"distilbert/distilgpt2\")\n",
        "\n",
        "generated_text = text_generator(\"Today is a rainy day in London\",\n",
        "                                truncation=True,\n",
        "                                num_return_sequences = 2)\n",
        "print(\"Generated_text:\\n\", generated_text[0]['generated_text'])"
      ],
      "metadata": {
        "id": "9lqnBX153KEH"
      },
      "id": "9lqnBX153KEH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Transformer-Based Tokenization\n",
        "\n",
        "A key focus for working with Large Language Models (LLMs) is understanding how to properly tokenize text, and Hugging Face makes this process seamless. Before diving into model usage, it's essential to grasp some foundational technicalities.\n",
        "\n",
        "One of the most important is transformer-based tokenization, the standard method for preparing text for LLMs like ChatGPT and BERT. Unlike traditional methods such as Bag of Words, transformer tokenization breaks down text into subword units using techniques like Byte-Pair Encoding (BPE) or WordPiece. This allows models to efficiently handle rare words, typos, and unseen vocabulary while preserving semantic meaning.\n",
        "\n",
        "Transformer tokenizers are dynamic and context-aware, significantly improving performance and generalization in modern NLP tasks compared to older tokenization strategies."
      ],
      "metadata": {
        "id": "YDKwqPZP3j5b"
      },
      "id": "YDKwqPZP3j5b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Understanding Transfomer Tokenization\n",
        "\n",
        "The best way to understand what's happening when we tokenize through a transformer-like arhcitecture is to look at what models do under the hood.\n",
        "\n",
        "In order to do so we can use the [`AutoTokenizer`](https://huggingface.co/docs/transformers/v4.52.3/en/model_doc/auto#transformers.AutoTokenizer) class from the `transformers` library.\n",
        "\n",
        "This is part of [Hugging Face's Auto Classes](https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes).\n",
        "\n",
        "The Auto Classes in Hugging Face, such as `AutoTokenizer` and `AutoModel`, are designed to simplify working with a wide range of pre-trained models. Instead of manually selecting the appropriate tokenizer or model class, Auto Classes automatically infer the correct one based on the model checkpoint you provide. This abstraction makes your code more flexible and model-agnostic, allowing you to swap between different architectures like BERT, RoBERTa, or DistilBERT without changing your pipeline.\n",
        "\n",
        "`AutoTokenizer` in particular ensures that text is tokenized using the exact method the underlying model was trained with, whether itâ€™s WordPiece, Byte-Pair Encoding (BPE), or SentencePiece. This guarantees compatibility and maximizes model performance when processing text data."
      ],
      "metadata": {
        "id": "yS-lTtSM4smI"
      },
      "id": "yS-lTtSM4smI"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load a pre-trained tokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # using a bert-like tokenizer\n",
        "\n",
        "# example text\n",
        "text = \"I really liked the movie No Country for Old Men\"\n",
        "\n",
        "# Tokenize the Text\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(\"Tokens:\\n\", tokens)"
      ],
      "metadata": {
        "id": "L0KgE_O98ChG"
      },
      "id": "L0KgE_O98ChG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These tokens look like usual word tokens from before, where is the transformer architecture?\n",
        "\n",
        "The important step is the transformation from tokens to ids. Here, each token is mapped to a unique integer ID using the model's pre-trained vocabulary. These IDs are then fed into the embedding layer of the transformer architecture, which transforms them into dense vectors that capture semantic information.\n",
        "\n",
        "This step is crucial because transformers operate on *continuous vector representations* rather than discrete words or tokens.\n",
        "\n",
        "The (auto) tokenizer ensures that the input format precisely matches what the transformer model expects, preserving alignment between tokenization and model training.\n"
      ],
      "metadata": {
        "id": "uPkNKLbK4vYo"
      },
      "id": "uPkNKLbK4vYo"
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert tokens to input IDs\n",
        "\n",
        "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(\"Input IDs:\\n\", input_ids)"
      ],
      "metadata": {
        "id": "PX5spDff9sOJ"
      },
      "id": "PX5spDff9sOJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whenever we want to pass data to our transformer model, we should do these encoding operations: tokenization + conversion to IDs.\n",
        "\n",
        "The `tokenizer()` method allows us to do so in one go:"
      ],
      "metadata": {
        "id": "TQer3oDI-G8K"
      },
      "id": "TQer3oDI-G8K"
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the text (tokenization + converting to IDs)\n",
        "\n",
        "encoded_input = tokenizer(text)\n",
        "\n",
        "print(\"Encoded input\", encoded_input)"
      ],
      "metadata": {
        "id": "Rm-dCKmP96kw"
      },
      "id": "Rm-dCKmP96kw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are some interesting things happeping here:\n",
        "\n",
        "* First, we can see that the first token is $101$: this indicates the start of the sentence.\n",
        "\n",
        "* Secondly, we see a `token_type_ids` object: let's skip its analysis for now, we'll come back to it later on.\n",
        "\n",
        "* Third, and most imortantly, we see our `attention_mask`, automatically computed by our tokenizer."
      ],
      "metadata": {
        "id": "kvLnYWUu-byX"
      },
      "id": "kvLnYWUu-byX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we want to decode, we will pass the input ids to the `decode()` method:"
      ],
      "metadata": {
        "id": "4rPEEJrJ-6QB"
      },
      "id": "4rPEEJrJ-6QB"
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode text\n",
        "\n",
        "decoded_output = tokenizer.decode(input_ids)\n",
        "print(\"Decode Output: \", decoded_output)"
      ],
      "metadata": {
        "id": "ktthKN9C_L9G"
      },
      "id": "ktthKN9C_L9G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Fine Tuning Using a Pretrained Model\n",
        "\n",
        "We will now try to fine tune a pretrained model on the `IMDB Dataset`.\n"
      ],
      "metadata": {
        "id": "_0A-jkcY_znJ"
      },
      "id": "_0A-jkcY_znJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Loading an Hugging Face Dataset\n",
        "\n",
        "It's really simple to use Hugging Face's datasets. First we must install the `datasets` library:"
      ],
      "metadata": {
        "id": "lQLWgBJRDVyH"
      },
      "id": "lQLWgBJRDVyH"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade datasets fsspec\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "1TiVUenTALb7"
      },
      "id": "1TiVUenTALb7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can just load the dataset by passing its name to the `load_dataset()` function:"
      ],
      "metadata": {
        "id": "SjA06ZzfAbju"
      },
      "id": "SjA06ZzfAbju"
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"imdb\")"
      ],
      "metadata": {
        "id": "vzkZn9MmAi7B"
      },
      "id": "vzkZn9MmAi7B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "metadata": {
        "id": "CzWQjCFCAPMS"
      },
      "id": "CzWQjCFCAPMS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.DataFrame(ds['train'])\n",
        "\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "CO5SaYGxCE32"
      },
      "id": "CO5SaYGxCE32",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.DataFrame(ds['test'])\n",
        "\n",
        "test_df.head()"
      ],
      "metadata": {
        "id": "Z6ovqboyCkHg"
      },
      "id": "Z6ovqboyCkHg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.info()"
      ],
      "metadata": {
        "id": "VqHhbkn3CPi5"
      },
      "id": "VqHhbkn3CPi5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(np.unique(train_df['label'])) # what labels do we have?"
      ],
      "metadata": {
        "id": "hul7Oi_JC57M"
      },
      "id": "hul7Oi_JC57M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Tokenize The Data\n",
        "\n",
        "Let's tokenize the dataset. We will add padding of size `max_length` (the maximum lenght accpeted by the model) in order for our tokens to be of the same size - padding shorter phrases -  and set `truncation=True` for the same reason (truncating longer words to our max length). See [padding and truncation](https://huggingface.co/docs/transformers/pad_truncation#padding-and-truncation) for more details."
      ],
      "metadata": {
        "id": "ydSNt2eRDKnC"
      },
      "id": "ydSNt2eRDKnC"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the (bert-like) tokenizer (automatically retrieved by AutoTokenizer)\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "  return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_ds = ds.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "zgK_A16tEHEF"
      },
      "id": "zgK_A16tEHEF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above we use the `map` function. From its [documentation](https://huggingface.co/docs/datasets/process#map):\n",
        "\n",
        "\"*Some of the more powerful applications of ðŸ¤— Datasets come from using the `map()` function. The primary purpose of `map()` is to speed up processing functions. It allows you to apply a processing function to each example in a dataset, independently or in batches.*\""
      ],
      "metadata": {
        "id": "qCiQxzBzFt4h"
      },
      "id": "qCiQxzBzFt4h"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_ds"
      ],
      "metadata": {
        "id": "Q1kdCmjRGI0Z"
      },
      "id": "Q1kdCmjRGI0Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Set Up the Training Arguments\n",
        "\n",
        "We can almost start training; we just need to secify the hyperparameters and the training settings.\n",
        "\n",
        "We can do this with the help of Hugging face's [`TrainingArguments`](https://huggingface.co/docs/datasets/process#map):"
      ],
      "metadata": {
        "id": "O4r7b-yZGQ9I"
      },
      "id": "O4r7b-yZGQ9I"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\", # output dir\n",
        "    eval_strategy=\"epoch\",  # evaluate every epoch\n",
        "    learning_rate=2e-5, # lr\n",
        "    per_device_train_batch_size=16, #batch size for training\n",
        "    per_device_eval_batch_size=16,  # batch size for evaluation\n",
        "    num_train_epochs=1, # number of training epochs\n",
        "    weight_decay=0.01 # strength of weight decay\n",
        ")\n",
        "\n",
        "training_args"
      ],
      "metadata": {
        "id": "B3XeOCg2HBzO"
      },
      "id": "B3XeOCg2HBzO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Initialize the Model\n",
        "\n",
        "Now we can initialize our model, using the `Auto` class, and initialize its [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer#trainer)."
      ],
      "metadata": {
        "id": "FhvBlDAQHa_-"
      },
      "id": "FhvBlDAQHa_-"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, Trainer\n",
        "\n",
        "# Load pre-trained model\n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased',\n",
        "                                                           num_labels=2)  # we only have 2 labels (0,1)\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_ds['train'],\n",
        "    eval_dataset=tokenized_ds['test']\n",
        ")"
      ],
      "metadata": {
        "id": "2DjejE_qIL3q"
      },
      "id": "2DjejE_qIL3q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note:**\n",
        ">\n",
        "> In Hugging Face's `transformers` library, choosing the correct Auto Class is crucial depending on your task.\n",
        ">\n",
        "> `AutoModel` loads **only the base pre-trained transformer model** (like BERT or RoBERTa) without any task-specific layers. *It outputs embeddings but does not add the necessary heads for tasks like classification, generation, or question answering*.\n",
        ">\n",
        "> On the other hand, `AutoModelForSequenceClassification` automatically adds a classification head on top of the base model, specifically designed for classification tasks. This head typically consists of a dense layer that outputs logits for each class, enabling the model to compute classification loss functions like cross-entropy.\n",
        ">\n",
        "> Specifying the right Auto Class ensures that your model is not only correctly structured but also fully compatible with high-level APIs like Hugging Faceâ€™s `Trainer`, which expects models to output both logits and loss during training.\n",
        ">\n",
        "> In short, use `AutoModel` if you only need raw embeddings or plan to design your own head; use task-specific Auto Classes like `AutoModelForSequenceClassification` to quickly get a model ready for fine-tuning on your downstream task.\n"
      ],
      "metadata": {
        "id": "r845bhI8JsVD"
      },
      "id": "r845bhI8JsVD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5 Train the Model\n",
        "\n",
        "Now training is as simple as it could be:"
      ],
      "metadata": {
        "id": "wlABTaJiJLfz"
      },
      "id": "wlABTaJiJLfz"
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "NMQvAZZzJ_Yl"
      },
      "id": "NMQvAZZzJ_Yl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.6 Evaluate the Trained Model\n",
        "\n",
        "Evaluating the model is very simple as well:"
      ],
      "metadata": {
        "id": "g9wUZYa_K1OZ"
      },
      "id": "g9wUZYa_K1OZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "results = trainer.evaluate()\n",
        "print(results)"
      ],
      "metadata": {
        "id": "toevnd6XK9ot"
      },
      "id": "toevnd6XK9ot",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.7 Save the Fine-Tuned Model\n",
        "\n",
        "We can of course save the trained model for later use:"
      ],
      "metadata": {
        "id": "JdiVFfobLMJW"
      },
      "id": "JdiVFfobLMJW"
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model:\n",
        "model.save_pretrained('./fine-tuned-model')"
      ],
      "metadata": {
        "id": "vj1KFXVePtZd"
      },
      "id": "vj1KFXVePtZd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note:**\n",
        ">\n",
        "> When fine-tuning a transformer model, itâ€™s essential to save the tokenizer along with the model. The tokenizer defines how input text is split into tokens and mapped to IDs, and it must match the modelâ€™s pretraining configuration. `AutoTokenizer` relies on files like `tokenizer_config.json` and `vocab.txt` to load the correct tokenizer automatically. Without these files, the model would not know how to correctly interpret input text, leading to mismatches or errors. Saving both ensures that future users can seamlessly reload and use the fine-tuned model without manual adjustments.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iDEXAaGBQKc-"
      },
      "id": "iDEXAaGBQKc-"
    },
    {
      "cell_type": "code",
      "source": [
        "# Save tokenizer\n",
        "tokenizer.save_pretrained('./fine-tuned-model')"
      ],
      "metadata": {
        "id": "8M-gPf0uQbM-"
      },
      "id": "8M-gPf0uQbM-",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}