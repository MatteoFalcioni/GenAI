{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abef0716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020e4820",
   "metadata": {},
   "source": [
    "# LangChain notebook \n",
    "\n",
    "LangChain is a framework for developing applications powered by language models.\n",
    "\n",
    "- GitHub: https://github.com/hwchase17/langchain\n",
    "- Docs: https://python.langchain.com/v0.2/docs/introduction/\n",
    "\n",
    "### Overview:\n",
    "- Installation\n",
    "- LLMs\n",
    "- Prompt Templates\n",
    "- Chains\n",
    "- Agents and Tools\n",
    "- Memory\n",
    "- Document Loaders\n",
    "- Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a26a6553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain langchain_community -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26f77095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4db4ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "hf_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0161c96",
   "metadata": {},
   "source": [
    "## LLMs in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7303a0",
   "metadata": {},
   "source": [
    "The basic building block of LangChain is a Large Language Model which takes text as input and generates more text.\n",
    "\n",
    "Suppose we want to generate a company name based on the company description, so we will first initialize an OpenAI wrapper. In this case, since we want the output to be more random, we will intialize our model with high temprature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba11747",
   "metadata": {},
   "source": [
    "The temperature parameter adjusts the randomness of the output. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee6483a",
   "metadata": {},
   "source": [
    "temperature value--> how creative we want our model to be\n",
    "\n",
    "0 ---> temperature it means model is  very safe it is not taking any bets.\n",
    "\n",
    "1 --> it will take risk it might generate wrong output but it is very creative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6892727",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3728/166103413.py:2: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI(temperature=0.9)\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd63ffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"what would be a good company name for a company that make colorful socks?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11c2e6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo-instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3728/229049954.py:2: LangChainDeprecationWarning: The method `BaseLLM.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(llm.predict(text))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Rainbow Footwear Co.\"\n"
     ]
    }
   ],
   "source": [
    "print(llm.model_name)   # defaults to gpt 3.5 turbo instruct if none is selected\n",
    "print(llm.predict(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c50caf16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3728/4203956707.py:2: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(llm(text))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Rainbow Feet Co.\"\n"
     ]
    }
   ],
   "source": [
    "# equivalent\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f8b9f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Rainbow Socks Co.\n",
      "2. ColorCraze Socks\n",
      "3. Happy Feet Co.\n",
      "4. ChromaSock\n",
      "5. VibrantSteps\n",
      "6. Chromatic Sox\n",
      "7. BrightSocks Co.\n",
      "8. Kaleidosock Co.\n",
      "9. ColorPop Socks\n",
      "10. FunkyFoot Co.\n"
     ]
    }
   ],
   "source": [
    "# equivalent \n",
    "print(llm.invoke(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f7568c",
   "metadata": {},
   "source": [
    "## LangChain and HuggingFace\n",
    "\n",
    "Hugging Face allows us to use open source models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd57142d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet huggingface_hub langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c483cad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use a basic language model\n",
    "from langchain_huggingface.llms.huggingface_endpoint import HuggingFaceEndpoint\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"google/flan-t5-large\",\n",
    "    provider=\"sambanova\",\n",
    "    max_new_tokens=100,\n",
    "    do_sample=False,\n",
    "    huggingfacehub_api_token=hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f871cef6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Task 'text-generation' not supported for provider 'sambanova'. Available tasks: ['conversational', 'feature-extraction']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtranslate English to German: How old are you?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/openAI/lib/python3.10/site-packages/langchain_core/language_models/llms.py:389\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    386\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    387\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 389\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    400\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    401\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/openAI/lib/python3.10/site-packages/langchain_core/language_models/llms.py:766\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    764\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    765\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/openAI/lib/python3.10/site-packages/langchain_core/language_models/llms.py:973\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    959\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    960\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    961\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    971\u001b[0m         )\n\u001b[1;32m    972\u001b[0m     ]\n\u001b[0;32m--> 973\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    981\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    982\u001b[0m         callback_managers[idx]\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    983\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n\u001b[1;32m    991\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/openAI/lib/python3.10/site-packages/langchain_core/language_models/llms.py:792\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    783\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    789\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    791\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 792\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    796\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    800\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    801\u001b[0m         )\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    803\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/miniconda3/envs/openAI/lib/python3.10/site-packages/langchain_core/language_models/llms.py:1547\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1544\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m   1546\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1547\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1548\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m   1549\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1550\u001b[0m     )\n\u001b[1;32m   1551\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/miniconda3/envs/openAI/lib/python3.10/site-packages/langchain_huggingface/llms/huggingface_endpoint.py:312\u001b[0m, in \u001b[0;36mHuggingFaceEndpoint._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m completion\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m     response_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minvocation_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;66;03m# Maybe the generation has stopped at one of the stop sequences:\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;66;03m# then we remove this stop sequence from the end of the generated text\u001b[39;00m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stop_seq \u001b[38;5;129;01min\u001b[39;00m invocation_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/inference/_client.py:2296\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[0;34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[0m\n\u001b[1;32m   2290\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2291\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI endpoint/model for text-generation is not served via TGI. Cannot return output as a stream.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2292\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please pass `stream=False` as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2293\u001b[0m         )\n\u001b[1;32m   2295\u001b[0m model_id \u001b[38;5;241m=\u001b[39m model \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m-> 2296\u001b[0m provider_helper \u001b[38;5;241m=\u001b[39m \u001b[43mget_provider_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2297\u001b[0m request_parameters \u001b[38;5;241m=\u001b[39m provider_helper\u001b[38;5;241m.\u001b[39mprepare_request(\n\u001b[1;32m   2298\u001b[0m     inputs\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m   2299\u001b[0m     parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2303\u001b[0m     api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken,\n\u001b[1;32m   2304\u001b[0m )\n\u001b[1;32m   2306\u001b[0m \u001b[38;5;66;03m# Handle errors separately for more precise error messages\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/inference/_providers/__init__.py:202\u001b[0m, in \u001b[0;36mget_provider_helper\u001b[0;34m(provider, task, model)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvider \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprovider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not supported. Available values: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or any provider from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(PROVIDERS\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (default value) will automatically select the first provider available for the model, sorted \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby the user\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms order in https://hf.co/settings/inference-providers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m provider_tasks:\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not supported for provider \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprovider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Available tasks: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(provider_tasks\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    204\u001b[0m     )\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m provider_tasks[task]\n",
      "\u001b[0;31mValueError\u001b[0m: Task 'text-generation' not supported for provider 'sambanova'. Available tasks: ['conversational', 'feature-extraction']"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"translate English to German: How old are you?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57bc030",
   "metadata": {},
   "source": [
    "**Note:** there is a problem with this format in LangChain at the moment, I tried several other methods to use HF but they are not working right now (24/06/2025). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887954ec",
   "metadata": {},
   "source": [
    "## Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13dcf6f",
   "metadata": {},
   "source": [
    "Currently in the above applications we are writing an entire prompt, if you are creating a user directed application then this is not an ideal case\n",
    "\n",
    "LangChain faciliates prompt management and optimization.\n",
    "\n",
    "Normally when you use an LLM in an application, you are not sending user input directly to the LLM. Instead, you need to take the user input and construct a prompt, and only then send that to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff824f70",
   "metadata": {},
   "source": [
    "In many Large Language Model applications we donot pass the user input directly to the Large Language Model, we add the user input to a large piece of text called prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0bf7bdd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to open a restaurant for indian food. Suggest a fancy name for this.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],   # give input variables\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fancy name for this.\"\n",
    ")\n",
    "p = prompt_template_name.format(cuisine=\"indian\")\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa29c5e",
   "metadata": {},
   "source": [
    "Or we can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65389777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is a good name for a company that makes colorful socks'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}\")\n",
    "prompt.format(product=\"colorful socks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912a89f3",
   "metadata": {},
   "source": [
    "## Chains: Combining LLM & Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e771b165",
   "metadata": {},
   "source": [
    "Now as we have the  **model**:\n",
    "\n",
    "```python\n",
    "llm = OpenAI(temperature=0.9)\n",
    "```\n",
    "\n",
    "and the **Prompt Template**:\n",
    "\n",
    "```python\n",
    "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}\")\n",
    "prompt.format(product=\"colorful socks\")\n",
    "```\n",
    "\n",
    "Now using Chains we will link together model and the PromptTemplate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c6d143",
   "metadata": {},
   "source": [
    "In LangChain, **chains** are modular sequences of steps where each step involves calling a component such as a language model, a prompt template, a retriever, or a tool. Chains are designed to handle complex workflows by linking together multiple operations, enabling tasks like document summarization, question answering, or agentic reasoning. Instead of manually orchestrating each step, chains allow developers to define an end-to-end logic where inputs are processed through a series of transformations, making the application both *reusable* and *scalable*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e231c43e",
   "metadata": {},
   "source": [
    "### Single Chains "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6518dcb9",
   "metadata": {},
   "source": [
    "The simplest and most common type of Chain is LLMChain, which passes the input first to Prompt Template and then to Large Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06889156",
   "metadata": {},
   "source": [
    "[**LLMChain**](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html#llmchain) is responsible to execute the PromptTemplate, For every PromptTemplate we will specifically have an LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0468d337",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af1d5f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3728/2715051390.py:3: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt)\n",
      "/tmp/ipykernel_3728/2715051390.py:4: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response= chain.run(\"colorful socks\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Rainbow Threads\" or \"ChromaSocks\" \n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "response= chain.run(\"colorful socks\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8275be",
   "metadata": {},
   "source": [
    "Another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "16194fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "\"El Sabroso Cantina\" (\"The Delicious Cantina\") \n"
     ]
    }
   ],
   "source": [
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template_name)\n",
    "response=chain.run(\"Mexican\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4505b77",
   "metadata": {},
   "source": [
    "If we want to see how the chain is working internally we can pass the parameter `verbose=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0a32fce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mI want to open a restaurant for Mexican food. Suggest a fency name for this.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\"Taco Tres Chic\"\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt_template_name, verbose=True)\n",
    "response=chain.run(\"Mexican\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ae97a5",
   "metadata": {},
   "source": [
    "### Multiple Chains\n",
    "\n",
    "What if we want to pass the output of a chain to another chain? Then we should construct a [**Sequential Chain**](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.sequential.SequentialChain.html#sequentialchain).\n",
    "How can we do that? \n",
    "\n",
    "For a 'double' chain, for example, we need to construct two single chains and link them: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da5afbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.6)\n",
    "\n",
    "# first prompt template\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "\n",
    "# first chain\n",
    "name_chain =LLMChain(llm=llm, prompt=prompt_template_name)\n",
    "\n",
    "# second prompt template\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template=\"\"\"Suggest some menu items for {restaurant_name}\"\"\"\n",
    ")\n",
    "\n",
    "# second chain\n",
    "food_items_chain = LLMChain(llm=llm, prompt=prompt_template_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bfb260c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Spicy Chicken Tikka Masala \n",
      "2. Lamb Vindaloo \n",
      "3. Vegetable Samosas \n",
      "4. Tandoori Shrimp \n",
      "5. Chana Masala \n",
      "6. Butter Chicken \n",
      "7. Aloo Gobi \n",
      "8. Chicken Biryani \n",
      "9. Palak Paneer \n",
      "10. Naan Bread \n",
      "11. Mango Lassi \n",
      "12. Garlic Naan \n",
      "13. Vegetable Korma \n",
      "14. Chicken Tandoori Wrap \n",
      "15. Mango Chutney \n",
      "16. Dal Makhani \n",
      "17. Malai Kofta \n",
      "18. Tandoori Mixed Grill \n",
      "19. Saag Chicken \n",
      "20. Kulfi (Indian ice cream)\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "chain = SimpleSequentialChain(chains = [name_chain, food_items_chain])\n",
    "\n",
    "content = chain.run(\"indian\")\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d188c49b",
   "metadata": {},
   "source": [
    "The limit with `SimpleSequentialChain` is that it only shows the last output. To overcome this, we can use `SequentialChain`. For this one we have to specify the parameter `output_key` in our `LLMChain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c6320327",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "\n",
    "name_chain =LLMChain(llm=llm, prompt=prompt_template_name, output_key=\"restaurant_name\")\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template=\"Suggest some menu items for {restaurant_name}.\"\n",
    ")\n",
    "\n",
    "food_items_chain =LLMChain(llm=llm, prompt=prompt_template_items, output_key=\"menu_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "373e6c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3728/2442697741.py:9: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(chain({\"cuisine\": \"indian\"}))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cuisine': 'indian', 'restaurant_name': '\\n\\n\"Spice Palace\"', 'menu_items': ' \\n\\n1. Spicy Chicken Tikka Masala\\n2. Lamb Vindaloo\\n3. Vegetable Biryani\\n4. Aloo Gobi (potato and cauliflower curry)\\n5. Tandoori Shrimp\\n6. Saag Paneer (spinach and paneer cheese curry)\\n7. Chana Masala (chickpea curry)\\n8. Butter Chicken\\n9. Naan bread\\n10. Samosas\\n11. Papadum (crispy lentil crackers)\\n12. Mango Lassi (yogurt drink)\\n13. Gulab Jamun (fried milk dumplings in syrup)\\n14. Kheer (rice pudding)\\n15. Masala Chai (spiced tea)'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "chain = SequentialChain(\n",
    "    chains = [name_chain, food_items_chain],\n",
    "    input_variables = ['cuisine'],\n",
    "    output_variables = ['restaurant_name', \"menu_items\"]\n",
    ")\n",
    "\n",
    "print(chain({\"cuisine\": \"indian\"}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622529c8",
   "metadata": {},
   "source": [
    "## Tools in LangChain "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bdc31a",
   "metadata": {},
   "source": [
    "**Agents** in LangChain enable dynamic decision-making by allowing a language model to choose actions based on the current context, observe the results, and iteratively continue until a final answer is reached. This loop—**decide, act, observe, repeat**—makes agents particularly well-suited for handling complex or multi-step tasks that can't be solved with a single prompt.\n",
    "\n",
    "When used effectively, agents can be extremely powerful. To work with agents in LangChain, it's important to understand the following core components:\n",
    "\n",
    "- **Tool**: A callable function that performs a specific task, such as web search, database lookup, math calculation, or invoking another chain. Tools are how the agent interacts with the outside world.\n",
    "- **LLM (Language Model)**: The underlying model responsible for interpreting the task, selecting tools, and reasoning through the workflow.\n",
    "- **Agent**: The controller that uses the LLM to decide which tool to invoke, based on the current state of the task and previous observations.\n",
    "\n",
    "By combining these components, LangChain agents can reason, access external data, and adapt their behavior in real time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42deeddc",
   "metadata": {},
   "source": [
    "For example I have to travel from Dubai to Canada, I type this in ChatGPT\n",
    "\n",
    "\n",
    "\n",
    "---> Give me  two flight options from Dubai to Canada on September 1, 2024 | ChatGPT will not be able to answer because has knowledge till\n",
    "September 2021\n",
    "\n",
    "\n",
    "\n",
    "ChatGPT plus has Expedia Plugin, if we enable this plugin it will go to Expedia Plugin and will try to pull information about Flights & it will show the information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbdce27",
   "metadata": {},
   "source": [
    "#### Tool example: Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6b6b0ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22106c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1b6846e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define llm\n",
    "\n",
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "26a9c57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should always think about what to do\n",
      "Action: wikipedia\n",
      "Action Input: GDP of US in 2024\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: List of U.S. states and territories by GDP\n",
      "Summary: This is a list of U.S. states and territories by gross domestic product (GDP). This article presents the 50 U.S. states and the District of Columbia and their nominal GDP at current prices.\n",
      "The data source for the list is the Bureau of Economic Analysis (BEA) in 2024. The BEA defined GDP by state as \"the sum of value added from all industries in the state.\"\n",
      "Overall, in the calendar year 2024, the United States' Nominal GDP at Current Prices totaled at $29.184 trillion, as compared to $27.720 trillion in 2023.\n",
      "The three U.S. states with the highest GDPs were California ($4.103 trillion), Texas ($2.709 trillion), and New York ($2.297 trillion). The three U.S. states with the lowest GDPs were Vermont ($45.7 billion), Wyoming ($53.0 billion), and Alaska ($69.9 billion).\n",
      "GDP per capita also varied widely throughout the United States in 2024, with New York ($117,332), Massachusetts ($110,561), and Washington (state) ($108,468) recording the three highest GDP per capita figures in the U.S., while Mississippi ($53,061), Arkansas ($60,276), and West Virginia ($60,783) recorded the three lowest GDP per capita figures in the U.S. The District of Columbia, though, recorded a GDP per capita figure far higher than any U.S. state in 2024 at $263,220.\n",
      "\n",
      "Page: List of countries by GDP (nominal) per capita\n",
      "Summary: This is a list of countries by nominal GDP per capita. GDP per capita is the total value of a country's finished goods and services (gross domestic product) divided by its total population (per capita).\n",
      "Gross domestic product (GDP) per capita is often considered an indicator of a country's standard of living; however, this is inaccurate because GDP per capita is not a measure of personal income. Measures of personal income include average wage, real income, median income, disposable income and GNI per capita.\n",
      "Comparisons of GDP per capita are also frequently made on the basis of purchasing power parity (PPP), to adjust for differences in the cost of living in different countries, see List of countries by GDP (PPP) per capita. PPP largely removes the exchange rate problem but not others; it does not reflect the value of economic output in international trade, and it also requires more estimation than GDP per capita. On the whole, PPP per capita figures are more narrowly spread than nominal GDP per capita figures.\n",
      "The figures presented here do not take into account differences in the cost of living in different countries, and the results vary greatly from one year to another based on fluctuations in the exchange rates of the country's currency. Such fluctuations change a country's ranking from one year to the next, even though they often make little or no difference to the standard of living of its population.\n",
      "For change of GDP per capita over time as a measure of economic growth, see real GDP growth and real GDP per capita growth.\n",
      "Non-sovereign entities (the world, continents, and some dependent territories) and states with limited international recognition are included in the list in cases in which they appear in the sources. These economies are not ranked in the charts here (except Kosovo and Taiwan), but are listed in sequence by GDP for comparison. In addition, non-sovereign entities are marked in italics. Four UN members (Cuba, Liechtenstein, Monaco and North Korea) do not belong to the International Monetary Fund (IMF), hence their economies are not ranked below. Kosovo, despite not being a member of the United Nations, is a member of IMF. Taiwan is not a IMF member but it is still listed in the official IMF indices.\n",
      "Several leading GDP-per-capita (nominal) jurisdictions may be considered tax havens, and their GDP data subject to material distortion by tax-planning activities. Examples include Bermuda, the Cayman Islands, Ireland and Luxembourg.\n",
      "\n",
      "Page: List of countries by GDP (nominal)\n",
      "Summary: Gross domestic product (GDP) is the market value of all final goods and servic\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The GDP of the US in 2024 was $29.184 trillion.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The GDP of the US in 2024 was $29.184 trillion.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The tools we'll give the Agent access to. \n",
    "tools = load_tools([\"wikipedia\"])\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,    #https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent_types.AgentType.html#agenttype\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Let's test it out!\n",
    "\n",
    "\n",
    "agent.run(\"What was the GDP of US in 2024?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1cb5918d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should use the wikipedia tool to search for the GDP of US in 2024.\n",
      "Action: wikipedia\n",
      "Action Input: GDP of US in 2024\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: List of U.S. states and territories by GDP\n",
      "Summary: This is a list of U.S. states and territories by gross domestic product (GDP). This article presents the 50 U.S. states and the District of Columbia and their nominal GDP at current prices.\n",
      "The data source for the list is the Bureau of Economic Analysis (BEA) in 2024. The BEA defined GDP by state as \"the sum of value added from all industries in the state.\"\n",
      "Overall, in the calendar year 2024, the United States' Nominal GDP at Current Prices totaled at $29.184 trillion, as compared to $27.720 trillion in 2023.\n",
      "The three U.S. states with the highest GDPs were California ($4.103 trillion), Texas ($2.709 trillion), and New York ($2.297 trillion). The three U.S. states with the lowest GDPs were Vermont ($45.7 billion), Wyoming ($53.0 billion), and Alaska ($69.9 billion).\n",
      "GDP per capita also varied widely throughout the United States in 2024, with New York ($117,332), Massachusetts ($110,561), and Washington (state) ($108,468) recording the three highest GDP per capita figures in the U.S., while Mississippi ($53,061), Arkansas ($60,276), and West Virginia ($60,783) recorded the three lowest GDP per capita figures in the U.S. The District of Columbia, though, recorded a GDP per capita figure far higher than any U.S. state in 2024 at $263,220.\n",
      "\n",
      "Page: List of countries by GDP (nominal) per capita\n",
      "Summary: This is a list of countries by nominal GDP per capita. GDP per capita is the total value of a country's finished goods and services (gross domestic product) divided by its total population (per capita).\n",
      "Gross domestic product (GDP) per capita is often considered an indicator of a country's standard of living; however, this is inaccurate because GDP per capita is not a measure of personal income. Measures of personal income include average wage, real income, median income, disposable income and GNI per capita.\n",
      "Comparisons of GDP per capita are also frequently made on the basis of purchasing power parity (PPP), to adjust for differences in the cost of living in different countries, see List of countries by GDP (PPP) per capita. PPP largely removes the exchange rate problem but not others; it does not reflect the value of economic output in international trade, and it also requires more estimation than GDP per capita. On the whole, PPP per capita figures are more narrowly spread than nominal GDP per capita figures.\n",
      "The figures presented here do not take into account differences in the cost of living in different countries, and the results vary greatly from one year to another based on fluctuations in the exchange rates of the country's currency. Such fluctuations change a country's ranking from one year to the next, even though they often make little or no difference to the standard of living of its population.\n",
      "For change of GDP per capita over time as a measure of economic growth, see real GDP growth and real GDP per capita growth.\n",
      "Non-sovereign entities (the world, continents, and some dependent territories) and states with limited international recognition are included in the list in cases in which they appear in the sources. These economies are not ranked in the charts here (except Kosovo and Taiwan), but are listed in sequence by GDP for comparison. In addition, non-sovereign entities are marked in italics. Four UN members (Cuba, Liechtenstein, Monaco and North Korea) do not belong to the International Monetary Fund (IMF), hence their economies are not ranked below. Kosovo, despite not being a member of the United Nations, is a member of IMF. Taiwan is not a IMF member but it is still listed in the official IMF indices.\n",
      "Several leading GDP-per-capita (nominal) jurisdictions may be considered tax havens, and their GDP data subject to material distortion by tax-planning activities. Examples include Bermuda, the Cayman Islands, Ireland and Luxembourg.\n",
      "\n",
      "Page: List of countries by GDP (nominal)\n",
      "Summary: Gross domestic product (GDP) is the market value of all final goods and servic\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should use the Calculator tool to calculate the ratio between the 2024 GDP and 2023 GDP.\n",
      "Action: Calculator\n",
      "Action Input: (29.184 trillion / 27.720 trillion) * 100\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 105.28138528138528\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: The GDP of US in 2024 was $29.184 trillion and the ratio between the 2024 GDP and 2023 GDP was 105.28138528138528%.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The GDP of US in 2024 was $29.184 trillion and the ratio between the 2024 GDP and 2023 GDP was 105.28138528138528%.'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,    #https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent_types.AgentType.html#agenttype\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent.run(\"What was the GDP of US in 2024? Retrieve it using the wikipedia tool i gave you. And what is the ratio between the 20224 GDP and 2023 GDP?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb6510d",
   "metadata": {},
   "source": [
    "> **Note:** Prompts are very important. For example, without specifying that he should use the wikipedia tool, the agent will try to use Wikipedia and fail. That's why prompt engeneering is important. Also we usually compose a prompt of a hardcoded system prompt + the user prompt, in order to avoid mistakes like these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "71a2d47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should use the Calculator tool to perform mathematical operations.\n",
      "Action: Calculator\n",
      "Action Input: (2024 GDP - 2023 GDP) / 2023 GDP\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 0.0004943153732081067\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should use the Wikipedia tool to find the GDP of US in 2024 and 2023.\n",
      "Action: Wikipedia\n",
      "Action Input: \"GDP of US in 2024\" and \"GDP of US in 2023\"\u001b[0m\n",
      "Observation: Wikipedia is not a valid tool, try one of [wikipedia, Calculator].\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should use the Calculator tool to perform mathematical operations.\n",
      "Action: Calculator\n",
      "Action Input: 2024 GDP and 2023 GDP\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 4047\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The GDP of US in 2024 is 4047 and the ratio between the 2024 GDP and 2023 GDP is 0.0004943153732081067.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The GDP of US in 2024 is 4047 and the ratio between the 2024 GDP and 2023 GDP is 0.0004943153732081067.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,    #https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent_types.AgentType.html#agenttype\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent.run(\"What was the GDP of US in 2024? And what is the ratio between the 20224 GDP and 2023 GDP?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0bdcce",
   "metadata": {},
   "source": [
    "## Memory in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444be0b8",
   "metadata": {},
   "source": [
    "Chatbot application like ChatGPT, you will notice that it remember past information. Does a chain have memory by its own?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e352ed7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "\"El Sabroso Cantina\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm,prompt=prompt_template_name)\n",
    "name = chain.run(\"Mexican\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dd5aca3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Masala Majesty\"\n"
     ]
    }
   ],
   "source": [
    "name = chain.run(\"Indian\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58e55c8",
   "metadata": {},
   "source": [
    "Let's access `chain.memory`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "95a5ea5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chain.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ebe395",
   "metadata": {},
   "source": [
    "Ok, there's nothing there. \n",
    "\n",
    "This is because LangChain Chains don't automatically store memory if not told so. We can actually use 3 types of memory inside LagnChain:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d53bf83",
   "metadata": {},
   "source": [
    "### Conversation Buffer Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef6306c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3728/3208601181.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"La Cantina de México\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "chain = LLMChain(llm=llm, \n",
    "                 prompt=prompt_template_name, \n",
    "                 memory=memory) # pass the memory parameter\n",
    "name = chain.run(\"Mexican\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f32c0364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Mexican\n",
      "AI: \n",
      "\n",
      "\"La Cantina de México\"\n"
     ]
    }
   ],
   "source": [
    "print(chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b100429",
   "metadata": {},
   "source": [
    "It's remembering the past chat! Amazing.\n",
    "\n",
    "But there is an issue with `ConversationBufferMemory()`: it remembers all previous conversations, therefore is very heavy in memory. \n",
    "\n",
    "To overcome this problem we can use `ConversationChain()`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb76dc3b",
   "metadata": {},
   "source": [
    "### ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "514939f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3728/3006166724.py:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  convo = ConversationChain(llm=OpenAI(temperature=0.7))\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "convo = ConversationChain(llm=OpenAI(temperature=0.7))\n",
    "print(convo.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e79b6d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Hello Matteo, it's nice to meet you. I am an AI created by OpenAI and I am always happy to chat with humans. I have access to a vast amount of information and can provide you with specific details related to your research at the University of Bologna. What specifically are you interested in?\""
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"My name is Matteo and I am a researcher at the University of Bologna\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e3fc26db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Your name is Matteo, and according to my records, you are a researcher at the University of Bologna. Would you like me to provide more information about your research projects or any specific areas of interest you have?'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"What was my name again?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "555fd8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: My name is Matteo and I am a researcher at the University of Bologna\n",
      "AI:  Hello Matteo, it's nice to meet you. I am an AI created by OpenAI and I am always happy to chat with humans. I have access to a vast amount of information and can provide you with specific details related to your research at the University of Bologna. What specifically are you interested in?\n",
      "Human: What was my name again?\n",
      "AI:  Your name is Matteo, and according to my records, you are a researcher at the University of Bologna. Would you like me to provide more information about your research projects or any specific areas of interest you have?\n"
     ]
    }
   ],
   "source": [
    "print(convo.memory.buffer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
