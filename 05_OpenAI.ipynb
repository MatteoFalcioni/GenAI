{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "Ak7ZgyzAZTvR",
      "metadata": {
        "id": "Ak7ZgyzAZTvR"
      },
      "source": [
        "# **OpenAI**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ff27fe1",
      "metadata": {
        "id": "8ff27fe1"
      },
      "source": [
        "## OpenAI and Its APIs\n",
        "\n",
        "OpenAI is a leading artificial intelligence research and deployment company whose mission is to ensure that artificial general intelligence (AGI) benefits all of humanity. It is best known for developing state-of-the-art AI models such as GPT (Generative Pre-trained Transformer) for natural language processing, DALL·E for image generation, Whisper for speech recognition, and Codex for code generation.\n",
        "\n",
        "OpenAI provides access to these models via powerful, developer-friendly APIs that allow users to integrate advanced AI capabilities into their applications. These APIs support a wide range of use cases, such as:\n",
        "\n",
        "*  **Natural Language Understanding and Generation**: chatbots, summarization, translation, question answering\n",
        "*  **Image Generation**: text-to-image generation using models like DALL·E\n",
        "*  **Code Completion and Assistance**: intelligent code generation with Codex\n",
        "*  **Speech-to-Text**: transcription and audio processing with Whisper\n",
        "\n",
        "## OpenAI API Access\n",
        "\n",
        "Developers can access these services via the [OpenAI platform](https://platform.openai.com/), where they can obtain [API keys](https://platform.openai.com/api-keys), monitor usage, and manage billing. The APIs can be accessed using Python, JavaScript, or any HTTP client. OpenAI also supports **fine-tuning**, **embeddings**, and **function calling**, allowing for highly customized applications."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfbcc269",
      "metadata": {},
      "source": [
        "## 0. Set Up\n",
        "\n",
        "We need to install some packages, which you can find in the `openAi_requirements.txt` file.\n",
        "\n",
        "Next we need to import some libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "52eba9ab",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from dotenv import load_dotenv  # for the API key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "0f4c37d3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade openai -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "74e4a8d6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: openai\n",
            "Version: 1.86.0\n",
            "Summary: The official Python library for the openai API\n",
            "Home-page: https://github.com/openai/openai-python\n",
            "Author: \n",
            "Author-email: OpenAI <support@openai.com>\n",
            "License: Apache-2.0\n",
            "Location: /home/matteo/miniconda3/envs/openAI/lib/python3.10/site-packages\n",
            "Requires: anyio, distro, httpx, jiter, pydantic, sniffio, tqdm, typing-extensions\n",
            "Required-by: \n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip show openai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f0687d7",
      "metadata": {},
      "source": [
        "Next we need to [get our OpenAI key](https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key) and save it in our `.env` file, so we can load it with `load_dotenv()`. \n",
        "\n",
        "We should save it in the following format:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35e728eb",
      "metadata": {},
      "source": [
        "```\n",
        "OPENAI_API_KEY = \"your_key_here\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e5bf408c",
      "metadata": {},
      "outputs": [],
      "source": [
        "load_dotenv()\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")  # load key "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "3d5461a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)  # or rely on env variable OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "26c2d32f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SyncPage[Model](data=[Model(id='gpt-4-0613', created=1686588896, object='model', owned_by='openai'), Model(id='gpt-4', created=1687882411, object='model', owned_by='openai'), Model(id='gpt-3.5-turbo', created=1677610602, object='model', owned_by='openai'), Model(id='gpt-4o-audio-preview-2025-06-03', created=1748908498, object='model', owned_by='system'), Model(id='gpt-4.1-nano-2025-04-14', created=1744321025, object='model', owned_by='system'), Model(id='gpt-4.1-nano', created=1744321707, object='model', owned_by='system'), Model(id='gpt-image-1', created=1745517030, object='model', owned_by='system'), Model(id='gpt-4o-realtime-preview-2025-06-03', created=1748907838, object='model', owned_by='system'), Model(id='davinci-002', created=1692634301, object='model', owned_by='system'), Model(id='babbage-002', created=1692634615, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-instruct', created=1692901427, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-instruct-0914', created=1694122472, object='model', owned_by='system'), Model(id='dall-e-3', created=1698785189, object='model', owned_by='system'), Model(id='dall-e-2', created=1698798177, object='model', owned_by='system'), Model(id='gpt-4-1106-preview', created=1698957206, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-1106', created=1698959748, object='model', owned_by='system'), Model(id='tts-1-hd', created=1699046015, object='model', owned_by='system'), Model(id='tts-1-1106', created=1699053241, object='model', owned_by='system'), Model(id='tts-1-hd-1106', created=1699053533, object='model', owned_by='system'), Model(id='text-embedding-3-small', created=1705948997, object='model', owned_by='system'), Model(id='text-embedding-3-large', created=1705953180, object='model', owned_by='system'), Model(id='gpt-4-0125-preview', created=1706037612, object='model', owned_by='system'), Model(id='gpt-4-turbo-preview', created=1706037777, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-0125', created=1706048358, object='model', owned_by='system'), Model(id='gpt-4-turbo', created=1712361441, object='model', owned_by='system'), Model(id='gpt-4-turbo-2024-04-09', created=1712601677, object='model', owned_by='system'), Model(id='gpt-4o', created=1715367049, object='model', owned_by='system'), Model(id='gpt-4o-2024-05-13', created=1715368132, object='model', owned_by='system'), Model(id='gpt-4o-mini-2024-07-18', created=1721172717, object='model', owned_by='system'), Model(id='gpt-4o-mini', created=1721172741, object='model', owned_by='system'), Model(id='gpt-4o-2024-08-06', created=1722814719, object='model', owned_by='system'), Model(id='chatgpt-4o-latest', created=1723515131, object='model', owned_by='system'), Model(id='o1-preview-2024-09-12', created=1725648865, object='model', owned_by='system'), Model(id='o1-preview', created=1725648897, object='model', owned_by='system'), Model(id='o1-mini-2024-09-12', created=1725648979, object='model', owned_by='system'), Model(id='o1-mini', created=1725649008, object='model', owned_by='system'), Model(id='gpt-4o-realtime-preview-2024-10-01', created=1727131766, object='model', owned_by='system'), Model(id='gpt-4o-audio-preview-2024-10-01', created=1727389042, object='model', owned_by='system'), Model(id='gpt-4o-audio-preview', created=1727460443, object='model', owned_by='system'), Model(id='gpt-4o-realtime-preview', created=1727659998, object='model', owned_by='system'), Model(id='omni-moderation-latest', created=1731689265, object='model', owned_by='system'), Model(id='omni-moderation-2024-09-26', created=1732734466, object='model', owned_by='system'), Model(id='gpt-4o-realtime-preview-2024-12-17', created=1733945430, object='model', owned_by='system'), Model(id='gpt-4o-audio-preview-2024-12-17', created=1734034239, object='model', owned_by='system'), Model(id='gpt-4o-mini-realtime-preview-2024-12-17', created=1734112601, object='model', owned_by='system'), Model(id='gpt-4o-mini-audio-preview-2024-12-17', created=1734115920, object='model', owned_by='system'), Model(id='o1-2024-12-17', created=1734326976, object='model', owned_by='system'), Model(id='o1', created=1734375816, object='model', owned_by='system'), Model(id='gpt-4o-mini-realtime-preview', created=1734387380, object='model', owned_by='system'), Model(id='gpt-4o-mini-audio-preview', created=1734387424, object='model', owned_by='system'), Model(id='o3-mini', created=1737146383, object='model', owned_by='system'), Model(id='o3-mini-2025-01-31', created=1738010200, object='model', owned_by='system'), Model(id='gpt-4o-2024-11-20', created=1739331543, object='model', owned_by='system'), Model(id='gpt-4.5-preview', created=1740623059, object='model', owned_by='system'), Model(id='gpt-4.5-preview-2025-02-27', created=1740623304, object='model', owned_by='system'), Model(id='gpt-4o-search-preview-2025-03-11', created=1741388170, object='model', owned_by='system'), Model(id='gpt-4o-search-preview', created=1741388720, object='model', owned_by='system'), Model(id='gpt-4o-mini-search-preview-2025-03-11', created=1741390858, object='model', owned_by='system'), Model(id='gpt-4o-mini-search-preview', created=1741391161, object='model', owned_by='system'), Model(id='gpt-4o-transcribe', created=1742068463, object='model', owned_by='system'), Model(id='gpt-4o-mini-transcribe', created=1742068596, object='model', owned_by='system'), Model(id='o1-pro-2025-03-19', created=1742251504, object='model', owned_by='system'), Model(id='o1-pro', created=1742251791, object='model', owned_by='system'), Model(id='gpt-4o-mini-tts', created=1742403959, object='model', owned_by='system'), Model(id='gpt-4.1-2025-04-14', created=1744315746, object='model', owned_by='system'), Model(id='gpt-4.1', created=1744316542, object='model', owned_by='system'), Model(id='gpt-4.1-mini-2025-04-14', created=1744317547, object='model', owned_by='system'), Model(id='gpt-4.1-mini', created=1744318173, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-16k', created=1683758102, object='model', owned_by='openai-internal'), Model(id='tts-1', created=1681940951, object='model', owned_by='openai-internal'), Model(id='whisper-1', created=1677532384, object='model', owned_by='openai-internal'), Model(id='text-embedding-ada-002', created=1671217299, object='model', owned_by='openai-internal')], object='list')\n"
          ]
        }
      ],
      "source": [
        "models = client.models.list()\n",
        "print(models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "f28c1064",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                id     created object owned_by\n",
            "0                       gpt-4-0613  1686588896  model   openai\n",
            "1                            gpt-4  1687882411  model   openai\n",
            "2                    gpt-3.5-turbo  1677610602  model   openai\n",
            "3  gpt-4o-audio-preview-2025-06-03  1748908498  model   system\n",
            "4          gpt-4.1-nano-2025-04-14  1744321025  model   system\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert model objects to dictionaries\n",
        "model_data = [m.model_dump() for m in models.data]  # For SDK v1\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(model_data)\n",
        "\n",
        "# Show the first few rows\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34d1e3b2",
      "metadata": {},
      "source": [
        "## 1. Hands on OpenAI - ChatCompletion API and Completion API\n",
        "\n",
        "We have two kinds of API in OpenAI: \n",
        "- **Chat Completion APIs**\n",
        "- **Completion APIs**\n",
        "\n",
        "Both are very easy to use. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "381b1f59",
      "metadata": {},
      "source": [
        "### 1.1 ChatCompletion API\n",
        "\n",
        "Let's say we want to use GPT-Trubo 3.5 for chat completion: we can use the `ChatCompletion` class methods, but we need to follow a specific format. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "ed66a6b0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatCompletion(id='chatcmpl-BhZYHWkUZdtUwS30E8XztVTAee76X', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello! I'm here and ready to assist you. How can I help you today?\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1749724465, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=18, prompt_tokens=23, total_tokens=41, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Hello, How are you?\"\n",
        "\n",
        "result = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",  # first we choose the model\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4da1f118",
      "metadata": {},
      "source": [
        "To interact with OpenAI's Chat API using the `openai.ChatCompletion.create()` method, you need to structure your request properly. First, specify the model you want to use (e.g., `\"gpt-3.5-turbo\"`). Then, define the conversation history through the `messages` parameter — a list of dictionaries where each message includes a `role` (`\"system\"`, `\"user\"`, or `\"assistant\"`) and its corresponding `content`. The `\"system\"` message sets the behavior or context of the assistant, while the `\"user\"` message contains the actual prompt or question. This structured format allows the model to maintain context and respond coherently in a conversational style.\n",
        "\n",
        "We can see that the result is not actually parsed, but in a dictionary form. If we want the actual response, we can select it by doing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "a25e4b10",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! I'm here and ready to assist you. How can I help you today?\n"
          ]
        }
      ],
      "source": [
        "print(result.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cd53a0c",
      "metadata": {},
      "source": [
        "#### 1.1.2 Prompting With More Context\n",
        "\n",
        "In a chat completion model, we can actually pass multiple prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "53036330",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm sorry, but I cannot provide personal information about you as I don't have access to that data. How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "prompt1 = \"Hello How are you?\"\n",
        "prompt2 = \"I am 25 years old & I am an AI Researcher\"\n",
        "prompt3 = \"Tell me about me\"\n",
        "\n",
        "result = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt1},\n",
        "        {\"role\": \"user\", \"content\": prompt2},\n",
        "        {\"role\": \"user\", \"content\": prompt3},\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(result.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d679dd9",
      "metadata": {},
      "source": [
        "Well, it looks like our model hasn't captured what we told him... why is that? To fix this we need to look into the `\"assistant\"` role.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9203bfa",
      "metadata": {},
      "source": [
        "#### 1.1.3 The `assistant` Role\n",
        "\n",
        "The `\"assistant\"` role in the `ChatCompletion.create()` method represents the model's responses in the conversation. Including assistant messages helps maintain the flow of dialogue and provides the model with a history of what it has already said. This is crucial for generating coherent and contextually accurate replies. By alternating between `\"user\"` and `\"assistant\"` messages, you allow the model to build a logical understanding of the conversation and respond appropriately based on prior interactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "32fce451",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on what you've shared, you're a 25-year-old AI researcher. You must have a strong interest in technology and innovation. Is there anything specific you would like to know or discuss further?\n"
          ]
        }
      ],
      "source": [
        "prompt1 = \"Hello How are you?\"\n",
        "prompt2 = \"I am 25 years old & I am an AI Researcher\"\n",
        "prompt3 = \"Tell me about me\"\n",
        "\n",
        "result = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"I'm great! How can I help you today?\"},\n",
        "        {\"role\": \"user\", \"content\": \"I am 25 years old & I am an AI Researcher.\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"That's impressive! Working in AI is very exciting.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Tell me about me.\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(result.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d1a2978",
      "metadata": {},
      "source": [
        "> **Note:** When using the `ChatCompletion.create()` method, it's important to structure your messages as a real conversation by [alternating between `\"user\"` and `\"assistant\"` roles](https://community.make.com/t/what-is-the-difference-between-system-user-and-assistant-roles-in-chatgpt/36160). If you stack multiple `\"user\"` messages without including the assistant's responses, the model won't be able to process the context properly. This can result in generic answers, such as stating it doesn't have personal information, even if you provided it earlier. To maintain coherent dialogue and context awareness, always simulate a back-and-forth interaction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71528935",
      "metadata": {},
      "source": [
        "Although it might seem redundant to \"tell the assistant\" something and then ask about it, this structure is essential for real-world applications of the `ChatCompletion.create()` method. The key is not to think of it as quizzing the assistant on what you just said — instead, you're building up context that the assistant can use to reason, generate, or respond meaningfully. \n",
        "\n",
        "For example, in a chatbot or virtual assistant, the user might first provide travel details like \"I'm looking for flights from Rome to Paris next week,\" and the assistant will follow up with a relevant question. Later, when more information is given, the assistant can suggest results or take action. \n",
        "\n",
        "Another use case is when you provide background information (e.g., your age, job, preferences) and then ask the assistant to generate a bio or personalized message — it's not about repeating facts, but about synthesizing them into something new. \n",
        "\n",
        "In reasoning tasks, you can guide the assistant step-by-step (e.g., assigning values to variables), and finally ask it to compute or deduce something based on prior steps. Including both `\"user\"` and `\"assistant\"` messages creates a history that allows the model to maintain context and behave intelligently, even in multi-turn conversations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8e848d4",
      "metadata": {},
      "source": [
        "#### 1.1.4 Tweaking Parameters\n",
        "\n",
        "* `max_tokens`: number of maximum tokens the model will generate (does not include input tokens)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "d0fb9762",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on what you've shared, you're a 25-year-old AI researcher. You must have a strong interest in technology and innovation. Is there anything specific you would like to know or discuss further?\n"
          ]
        }
      ],
      "source": [
        "prompt = \"What is Python?\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model = \"gpt-3.5-turbo\",\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ],\n",
        "    max_tokens = 25\n",
        ")\n",
        "\n",
        "print(result.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3de82d65",
      "metadata": {},
      "source": [
        "* `temperature`: parameter that allows the model to weight more less probable completion characters, meaning that it will have more \"freedom of choice\" in generating the next token, as it will not always choose the one with maximum probability.\n",
        "\n",
        "We call it temperature because it resembles the thermodinamic temperature in the Softmax function, $\\text{softmax}(x)_i = \\exp(\\frac{y_i}{T}) / \\exp(\\frac{\\sum_j y_j}{T}))$. So if $T$ is very big, the model choose aribitrarily between the generated next completion tokens, while if $T=0$ it only chooses the one with maximum probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "097cc435",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on what you've shared, you're a 25-year-old AI researcher. You must have a strong interest in technology and innovation. Is there anything specific you would like to know or discuss further?\n"
          ]
        }
      ],
      "source": [
        "prompt = \"What is Python?\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ],\n",
        "    temperature = 0.6\n",
        ")\n",
        "\n",
        "print(result.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "50b7ebb2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on what you've shared, you're a 25-year-old AI researcher. You must have a strong interest in technology and innovation. Is there anything specific you would like to know or discuss further?\n"
          ]
        }
      ],
      "source": [
        "prompt = \"What is Python?\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ],\n",
        "    temperature = 1.6\n",
        ")\n",
        "\n",
        "print(result.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "446f278e",
      "metadata": {},
      "source": [
        "* `n`: number of responses the model outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "1db7c616",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python is a high-level programming language known for its simplicity and readability. It is widely used in various fields such as web development, data science, artificial intelligence, and automation. Python supports multiple programming paradigms and has a large standard library that makes it versatile for different types of projects.\n",
            "Python is a high-level, interpreted programming language known for its simplicity and readability. It is widely used for web development, data analysis, artificial intelligence, scientific computing, and more. Python supports multiple programming paradigms, including procedural, object-oriented, and functional programming. It has a large standard library and a vibrant community that contributes to various libraries and frameworks, making it a versatile and popular choice among developers.\n",
            "Python is a high-level, interpreted programming language known for its simplicity and readability. It is widely used in various fields such as web development, data science, artificial intelligence, and automation. Python supports multiple programming paradigms and has a large standard library that provides useful modules and packages for different tasks.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"What is Python?\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ],\n",
        "    temperature = 0.6,\n",
        "    n = 3\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n",
        "print(response.choices[1].message.content)\n",
        "print(response.choices[2].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9018403b",
      "metadata": {},
      "source": [
        "Apart from just text generation, we can do any LLM task, for example sentiment analysis, code writing and so on:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "64887d1f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Certainly! Here is a simple Python code to add two numbers:\n",
            "\n",
            "```python\n",
            "num1 = 5\n",
            "num2 = 3\n",
            "\n",
            "sum = num1 + num2\n",
            "\n",
            "print(\"The sum of\", num1, \"and\", num2, \"is:\", sum)\n",
            "```\n",
            "\n",
            "You can replace the values of `num1` and `num2` with any numbers you want to add.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Give me a Python code to add 2 numbers\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ],\n",
        "    temperature = 0.6,\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbbb73a5",
      "metadata": {},
      "source": [
        "### 1.2 Completion APIs\n",
        "\n",
        "OpenAI provides two main APIs for text generation: `Completion` and `ChatCompletion`. While both can generate text, they differ in structure and use cases. `Completion` is used with older models like GPT-3 and expects a single plain-text prompt. On the other hand, `ChatCompletion` is designed for newer models like GPT-3.5 and GPT-4, and supports multi-turn conversations with structured messages that include roles like `system`, `user`, and `assistant`. For building chatbots or conversational agents, `ChatCompletion` is the recommended approach.\n",
        "\n",
        "| Feature                 | `Completion`                              | `ChatCompletion`                                           |\n",
        "| ----------------------- | ----------------------------------------- | ---------------------------------------------------------- |\n",
        "| **Supported Models**    | GPT-3 (`text-davinci-003`, `curie`, etc.) | GPT-3.5, GPT-4 (`gpt-3.5-turbo`, `gpt-4`, etc.)            |\n",
        "| **Input Format**        | Plain prompt (`prompt=\"...\"`)             | Structured messages (`messages=[...]`)                     |\n",
        "| **Interaction Style**   | Single-shot completions                   | Multi-turn chat with roles (`system`, `user`, `assistant`) |\n",
        "| **Context Handling**    | No role or turn distinction               | Tracks full conversation history                           |\n",
        "| **Best Use Cases**      | Simple completions or prompts             | Conversational agents, chatbots, assistants                |\n",
        "| **System Instructions** | Not supported                             | Supported via `system` role                                |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "c237c537",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " in Compose TextBalloon.\n",
            "I already know how to add numbers with @key. Can someone please help? I know that I can add them with a single formula, but I want to list them in the Compose, for multiple items to respond to individual user inputs. Thank you in advance.\n",
            "```\n",
            "class EnumContainer(models.Model):\n",
            "    class Meta:\n",
            "        verbose_name_plural = \"Items\"\n",
            "\n",
            "    model = models.CharField(max_length=25, unique=True, required=False, db_index=True\n"
          ]
        }
      ],
      "source": [
        "response = client.completions.create(\n",
        "    model=\"babbage-002\",  # or \"text-davinci-003\"\n",
        "    prompt=prompt,\n",
        "    max_tokens=100\n",
        ")\n",
        "print(response.choices[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34e01f34",
      "metadata": {},
      "source": [
        "## 2. Function Calling in OpenAI\n",
        "\n",
        "[Function calling](https://platform.openai.com/docs/guides/function-calling?api-mode=responses) provides a powerful and flexible way for OpenAI models to interface with your code or external services.\n",
        "\n",
        "More specifically, function calling enables models like gpt-4 or gpt-3.5-turbo to return structured data (e.g., JSON) that can be used to trigger predefined functions in your code. Instead of just responding in plain text, the model can suggest calling a specific function with specific arguments.\n",
        "\n",
        "If you are familiar with *tool calling*, we can say that function calling is the \"older\" and less general version of tool calling, since the model is allowed to interact only with functions. In a sense, they overlap, but tools have a broader scope than just function calling.\n",
        "\n",
        "### boh\n",
        "\n",
        "Let's see an example of function calling at work. I accessed the [RapidAPI](https://rapidapi.com/MeteosourceWeather/api/ai-weather-by-meteosource/playground/apiendpoint_051aea00-95fb-437c-944c-06dd8d4049d8) in order to get the free plan of the [AI weather](https://rapidapi.com/MeteosourceWeather/api/ai-weather-by-meteosource) API by meteosource.\n",
        "\n",
        "On the page we can find the following code (below code snippet - python):"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8700fa8",
      "metadata": {},
      "source": [
        "```python\n",
        "import http.client\n",
        "\n",
        "conn = http.client.HTTPSConnection(\"ai-weather-by-meteosource.p.rapidapi.com\")\n",
        "\n",
        "headers = {\n",
        "    'x-rapidapi-key': \"06d2d29013msh6c0e9656309d413p150efdjsn294c8f9639bb\",\n",
        "    'x-rapidapi-host': \"ai-weather-by-meteosource.p.rapidapi.com\"\n",
        "}\n",
        "\n",
        "conn.request(\"GET\", \"/time_machine?lat=37.81021&lon=-122.42282&date=2021-08-24&units=auto\", headers=headers)\n",
        "\n",
        "res = conn.getresponse()\n",
        "data = res.read()\n",
        "\n",
        "print(data.decode(\"utf-8\"))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3eda671",
      "metadata": {},
      "source": [
        "With the help of this we can write a function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e76114c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example dummy function hard coded to return the same weather\n",
        "# In production, this could be your backend API or an external API\n",
        "\n",
        "import requests\n",
        "def get_current_weather(location):\n",
        "    \"\"\"Get the current weather in a given location\"\"\"\n",
        "\n",
        "    url = \"https://ai-weather-by-meteosource.p.rapidapi.com/find_places\"\n",
        "\n",
        "    querystring = {\"text\":location}\n",
        "\n",
        "    headers = {\n",
        "      'x-rapidapi-key': \"156877d1e1msh6806b57dfec44b9p1bd2c8jsn796f2ff15790\",\n",
        "      'x-rapidapi-host': \"ai-weather-by-meteosource.p.rapidapi.com\"\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, headers=headers, params=querystring)\n",
        "\n",
        "    print(response.json())\n",
        "  \n",
        "    return response.json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ffabc0b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'name': 'Bologna', 'place_id': 'bologna', 'adm_area1': 'Emilia-Romagna', 'adm_area2': 'Bologna', 'country': 'Italy', 'lat': '44.49381N', 'lon': '11.33875E', 'timezone': 'Europe/Rome', 'type': 'settlement'}, {'name': 'Bologna', 'place_id': 'bologna-8971140', 'adm_area1': 'Piedmont', 'adm_area2': 'Provincia di Asti', 'country': 'Italy', 'lat': '44.80098N', 'lon': '8.2688E', 'timezone': 'Europe/Rome', 'type': 'settlement'}, {'name': 'Bologna', 'place_id': 'bologna-8977649', 'adm_area1': 'Emilia-Romagna', 'adm_area2': 'Provincia di Ferrara', 'country': 'Italy', 'lat': '44.89688N', 'lon': '11.98523E', 'timezone': 'Europe/Rome', 'type': 'settlement'}, {'name': 'Bologna', 'place_id': 'bologna-377261', 'adm_area1': 'Central Equatoria', 'adm_area2': None, 'country': 'Republic of South Sudan', 'lat': '5.59644N', 'lon': '31.45129E', 'timezone': 'Africa/Juba', 'type': 'settlement'}, {'name': 'Imola', 'place_id': 'imola', 'adm_area1': 'Emilia-Romagna', 'adm_area2': 'Bologna', 'country': 'Italy', 'lat': '44.35916N', 'lon': '11.7132E', 'timezone': 'Europe/Rome', 'type': 'settlement'}, {'name': 'Casalecchio di Reno', 'place_id': 'casalecchio-di-reno', 'adm_area1': 'Emilia-Romagna', 'adm_area2': 'Bologna', 'country': 'Italy', 'lat': '44.47563N', 'lon': '11.27495E', 'timezone': 'Europe/Rome', 'type': 'settlement'}, {'name': 'Savena', 'place_id': 'savena', 'adm_area1': 'Emilia-Romagna', 'adm_area2': 'Bologna', 'country': 'Italy', 'lat': '44.46933N', 'lon': '11.37864E', 'timezone': 'Europe/Rome', 'type': 'administrative_area'}, {'name': 'Navile', 'place_id': 'navile', 'adm_area1': 'Emilia-Romagna', 'adm_area2': 'Bologna', 'country': 'Italy', 'lat': '44.53166N', 'lon': '11.34647E', 'timezone': 'Europe/Rome', 'type': 'administrative_area'}, {'name': 'San Lazzaro di Savena', 'place_id': 'san-lazzaro-di-savena', 'adm_area1': 'Emilia-Romagna', 'adm_area2': 'Bologna', 'country': 'Italy', 'lat': '44.46777N', 'lon': '11.41401E', 'timezone': 'Europe/Rome', 'type': 'administrative_area'}]\n"
          ]
        }
      ],
      "source": [
        "# we can call this function as usual...\n",
        "response = get_current_weather(\"Bologna\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f9917f3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'name': 'Bologna',\n",
              "  'place_id': 'bologna',\n",
              "  'adm_area1': 'Emilia-Romagna',\n",
              "  'adm_area2': 'Bologna',\n",
              "  'country': 'Italy',\n",
              "  'lat': '44.49381N',\n",
              "  'lon': '11.33875E',\n",
              "  'timezone': 'Europe/Rome',\n",
              "  'type': 'settlement'},\n",
              " {'name': 'Bologna',\n",
              "  'place_id': 'bologna-8971140',\n",
              "  'adm_area1': 'Piedmont',\n",
              "  'adm_area2': 'Provincia di Asti',\n",
              "  'country': 'Italy',\n",
              "  'lat': '44.80098N',\n",
              "  'lon': '8.2688E',\n",
              "  'timezone': 'Europe/Rome',\n",
              "  'type': 'settlement'},\n",
              " {'name': 'Bologna',\n",
              "  'place_id': 'bologna-8977649',\n",
              "  'adm_area1': 'Emilia-Romagna',\n",
              "  'adm_area2': 'Provincia di Ferrara',\n",
              "  'country': 'Italy',\n",
              "  'lat': '44.89688N',\n",
              "  'lon': '11.98523E',\n",
              "  'timezone': 'Europe/Rome',\n",
              "  'type': 'settlement'},\n",
              " {'name': 'Bologna',\n",
              "  'place_id': 'bologna-377261',\n",
              "  'adm_area1': 'Central Equatoria',\n",
              "  'adm_area2': None,\n",
              "  'country': 'Republic of South Sudan',\n",
              "  'lat': '5.59644N',\n",
              "  'lon': '31.45129E',\n",
              "  'timezone': 'Africa/Juba',\n",
              "  'type': 'settlement'},\n",
              " {'name': 'Imola',\n",
              "  'place_id': 'imola',\n",
              "  'adm_area1': 'Emilia-Romagna',\n",
              "  'adm_area2': 'Bologna',\n",
              "  'country': 'Italy',\n",
              "  'lat': '44.35916N',\n",
              "  'lon': '11.7132E',\n",
              "  'timezone': 'Europe/Rome',\n",
              "  'type': 'settlement'},\n",
              " {'name': 'Casalecchio di Reno',\n",
              "  'place_id': 'casalecchio-di-reno',\n",
              "  'adm_area1': 'Emilia-Romagna',\n",
              "  'adm_area2': 'Bologna',\n",
              "  'country': 'Italy',\n",
              "  'lat': '44.47563N',\n",
              "  'lon': '11.27495E',\n",
              "  'timezone': 'Europe/Rome',\n",
              "  'type': 'settlement'},\n",
              " {'name': 'Savena',\n",
              "  'place_id': 'savena',\n",
              "  'adm_area1': 'Emilia-Romagna',\n",
              "  'adm_area2': 'Bologna',\n",
              "  'country': 'Italy',\n",
              "  'lat': '44.46933N',\n",
              "  'lon': '11.37864E',\n",
              "  'timezone': 'Europe/Rome',\n",
              "  'type': 'administrative_area'},\n",
              " {'name': 'Navile',\n",
              "  'place_id': 'navile',\n",
              "  'adm_area1': 'Emilia-Romagna',\n",
              "  'adm_area2': 'Bologna',\n",
              "  'country': 'Italy',\n",
              "  'lat': '44.53166N',\n",
              "  'lon': '11.34647E',\n",
              "  'timezone': 'Europe/Rome',\n",
              "  'type': 'administrative_area'},\n",
              " {'name': 'San Lazzaro di Savena',\n",
              "  'place_id': 'san-lazzaro-di-savena',\n",
              "  'adm_area1': 'Emilia-Romagna',\n",
              "  'adm_area2': 'Bologna',\n",
              "  'country': 'Italy',\n",
              "  'lat': '44.46777N',\n",
              "  'lon': '11.41401E',\n",
              "  'timezone': 'Europe/Rome',\n",
              "  'type': 'administrative_area'}]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2495d22c",
      "metadata": {},
      "source": [
        "The interesting thing is that our LLM can use it! \n",
        "\n",
        "We need to follow a [specific function format](https://platform.openai.com/docs/guides/function-calling#defining-functions) to pass functions to our model - be careful, in the past we would have constructed a `function` dictionary directly, but now this format is deprecated -> we must use the `tools` syntax and specify `function` as the `type`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "5e9e7e46",
      "metadata": {},
      "outputs": [],
      "source": [
        "tools = [{\n",
        "    \"type\": \"function\",  # This should always be \"function\" when defining a function tool\n",
        "    \"function\": {\n",
        "    \"name\": \"get_current_weather\",  # Unique name of the function to be called by the model\n",
        "    \"description\": \"Get current weather in a given location\",  # Description helps the model decide when to use this tool\n",
        "    \"parameters\": {  # JSON Schema defining the structure of the input the function expects\n",
        "        \"type\": \"object\",  # The top-level input is a JSON object (i.e., a dictionary)\n",
        "        \"properties\": {  # Defines the fields that this object can have\n",
        "            \"location\": {\n",
        "                \"type\": \"string\",  # The value for \"location\" must be a string\n",
        "                \"description\": \"The city and state, e.g. San Francisco, CA\",  # Helps the model understand what to pass\n",
        "            },\n",
        "        },\n",
        "        \"required\": [\"location\"],  # \"location\" is a mandatory parameter; the model must provide it\n",
        "    }\n",
        "    },\n",
        "    \"strict\": True  # If True, only allows exactly the parameters in the schema; False allows extra args\n",
        "}]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0796d5d5",
      "metadata": {},
      "source": [
        "Now, our model can use this function tool if needed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "b5879b62",
      "metadata": {},
      "outputs": [],
      "source": [
        "input_messages = [{\"role\": \"user\", \"content\": \"What's the weather like in Paris today?\"}]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages= input_messages,\n",
        "    tools=tools    \n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "4fdda765",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-BhZjOctOhjQm9BGuz425gMRGqke3C', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_6P9dRiz7YgGvI64CWrN193pW', function=Function(arguments='{\"location\":\"Paris\"}', name='get_current_weather'), type='function')]))], created=1749725154, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=15, prompt_tokens=71, total_tokens=86, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "d633cd05",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_6P9dRiz7YgGvI64CWrN193pW', function=Function(arguments='{\"location\":\"Paris\"}', name='get_current_weather'), type='function')])"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the assistant's message\n",
        "message = response.choices[0].message\n",
        "message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "f2e4f6a4",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[ChatCompletionMessageToolCall(id='call_6P9dRiz7YgGvI64CWrN193pW', function=Function(arguments='{\"location\":\"Paris\"}', name='get_current_weather'), type='function')]"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "message.tool_calls"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06270afc",
      "metadata": {},
      "source": [
        "We can see that the response is a **tool call**, not an actual chat message!\n",
        "\n",
        "This means we are using the LLM within an **AI agent framework**. In this setup, the LLM acts as the \"brain\" of the agent. First, it performs a **thought step**, where it reasons about the task. Then it takes an **action step** by choosing to call a function — in this case, our `get_current_weather` tool.\n",
        "\n",
        "However, the workflow doesn't end there. After the function is called and the tool returns a result, the agent must process this new information and generate a final response. This is known as the **observation step**.\n",
        "\n",
        "So yes — at this point, you need to pass the **tool's result back to the model** as a new message (from a `tool` role), so it can continue the reasoning and generate the final answer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "1d1212ac",
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'ChatCompletion' object has no attribute 'output'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[52], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m tool_call \u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m args \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(tool_call\u001b[38;5;241m.\u001b[39marguments)\n\u001b[1;32m      6\u001b[0m result \u001b[38;5;241m=\u001b[39m get_current_weather(args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocation\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
            "File \u001b[0;32m~/miniconda3/envs/openAI/lib/python3.10/site-packages/pydantic/main.py:991\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[0;32m--> 991\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ChatCompletion' object has no attribute 'output'"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "tool_call = response.output[0]\n",
        "args = json.loads(tool_call.arguments)\n",
        "\n",
        "result = get_current_weather(args[\"location\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55c5f4c9",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "openAI",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
